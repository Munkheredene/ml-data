{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9805f93-02b9-4fe3-83aa-96315585bf04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1efab9c1-f135-4f95-95ea-01f6c1e4e0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GetMiniBatch:\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : 次の形のndarray, shape (n_samples, n_features)\n",
    "      訓練データ\n",
    "    y : 次の形のndarray, shape (n_samples, 1)\n",
    "      正解値\n",
    "    batch_size : int\n",
    "      バッチサイズ\n",
    "    seed : int\n",
    "      NumPyの乱数のシード\n",
    "    \"\"\"\n",
    "    def __init__(self, X, y, batch_size = 20, seed=0):\n",
    "        self.batch_size = batch_size\n",
    "        np.random.seed(seed)\n",
    "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
    "        self._X = X[shuffle_index]\n",
    "        self._y = y[shuffle_index]\n",
    "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(int)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._stop\n",
    "\n",
    "    def __getitem__(self,item):\n",
    "        p0 = item*self.batch_size\n",
    "        p1 = item*self.batch_size + self.batch_size\n",
    "        return self._X[p0:p1], self._y[p0:p1]\n",
    "\n",
    "    def __iter__(self):\n",
    "        self._counter = 0\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        if self._counter >= self._stop:\n",
    "            raise StopIteration()\n",
    "        p0 = self._counter*self.batch_size\n",
    "        p1 = self._counter*self.batch_size + self.batch_size\n",
    "        self._counter += 1\n",
    "        return self._X[p0:p1], self._y[p0:p1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f3e93b3-faca-4ce8-9035-0b57d9ada9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FC:\n",
    "    \"\"\"\n",
    "    ノード数n_nodes1からn_nodes2への全結合層\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_nodes1 : int\n",
    "      前の層のノード数\n",
    "    n_nodes2 : int\n",
    "      後の層のノード数\n",
    "    initializer : 初期化方法のインスタンス\n",
    "    optimizer : 最適化手法のインスタンス\n",
    "    \"\"\"\n",
    "    def __init__(self, n_nodes1, n_nodes2, initializer, optimizer):\n",
    "        \n",
    "        self.optimizer = optimizer\n",
    "        self.W, self.B = initializer.W(n_nodes1, n_nodes2), initializer.B(n_nodes2)\n",
    "        \n",
    "        \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        フォワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            入力\n",
    "        Returns\n",
    "        ----------\n",
    "        A : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            出力\n",
    "        \"\"\"\n",
    "        A = X @ self.W + self.B\n",
    "        \n",
    "        return A\n",
    "        \n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "        バックワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        dA : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            後ろから流れてきた勾配\n",
    "        Returns\n",
    "        ----------\n",
    "        dZ : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            前に流す勾配\n",
    "        \"\"\"\n",
    "        dZ = dA @ self.W.T\n",
    "        dB = np.sum(dA, axis=0)\n",
    "        self.W, self.B = self.optimizer.update(self.W, self.B, dZ, dB)\n",
    "        \n",
    "        return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b02eaac-8f86-426d-8ad8-20d999585ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializer\n",
    "\n",
    "class SimpleInitializer:\n",
    "    \"\"\"\n",
    "    ガウス分布によるシンプルな初期化\n",
    "    Parameters\n",
    "    ----------\n",
    "    sigma : float\n",
    "      ガウス分布の標準偏差\n",
    "    \"\"\"\n",
    "    def __init__(self, sigma):\n",
    "        self.sigma = sigma\n",
    "        \n",
    "    def W(self, n_nodes1, n_nodes2):\n",
    "        \"\"\"\n",
    "        重みの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes1 : int\n",
    "          前の層のノード数\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        W :\n",
    "        \"\"\"\n",
    "        W = self.sigma * np.random.randn(n_nodes1, n_nodes2)\n",
    "        return W\n",
    "        \n",
    "    def B(self, n_nodes2):\n",
    "        \"\"\"\n",
    "        バイアスの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        B :\n",
    "        \"\"\"\n",
    "        B = self.sigma * np.random.randn(1, n_nodes2)\n",
    "        \n",
    "        return B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3018a097-e75c-4c8c-90c6-5f07e6bc330f",
   "metadata": {},
   "source": [
    "Optimizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f9871eb-c8cb-4f85-bd9b-c76d51f3f76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    \"\"\"\n",
    "    確率的勾配降下法\n",
    "    Parameters\n",
    "    ----------\n",
    "    lr : 学習率\n",
    "    \"\"\"\n",
    "    def __init__(self, lr):\n",
    "        self.lr = lr\n",
    "    def update(self, W, B, dZ, dB):\n",
    "        \"\"\"\n",
    "        ある層の重みやバイアスの更新\n",
    "        Parameters\n",
    "        ----------\n",
    "        layer : 更新前の層のインスタンス\n",
    "        \"\"\"\n",
    "        W -= self.lr * dZ\n",
    "        B -= self.lr * dB\n",
    "        return W, B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc35ade3-debf-45e2-aa36-1c61c4ed54e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class activation_func:\n",
    "\n",
    "    def __init__(self, function_type):\n",
    "        \n",
    "        self.function_type=function_type\n",
    "        \n",
    "    def forward(self, X):\n",
    "        \n",
    "        if self.function_type==\"tanh\":\n",
    "            A = self.tanh_function(X)\n",
    "        elif self.function_type==\"softmax\":\n",
    "            A = self.softmax(X)\n",
    "\n",
    "        return A\n",
    "        \n",
    "    def backward(self, X, y=None):\n",
    "\n",
    "        if self.function_type==\"tanh\":\n",
    "            A = self.grad_tanh(X)\n",
    "        elif self.function_type==\"softmax\":\n",
    "            A = self.grad_softmax(X, y)\n",
    "        return A\n",
    "\n",
    "    def tanh_function(self, A):\n",
    "        return (np.exp(A)-np.exp(-A))/(np.exp(A)+np.exp(-A))\n",
    "    \n",
    "    def softmax(self, A):\n",
    "        return np.exp(A)/np.sum(np.exp(A), axis=0)\n",
    "\n",
    "    def grad_tanh(self, A):\n",
    "         return (1 - self.tanh_function(self.A)**2)\n",
    "\n",
    "    def grad_softmax(self, Z3, y):\n",
    "        return (Z3 - y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9b0550d1-4f4e-40a4-a577-0cb5ed0237ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScratchSimpleDeepNeuralNetrowkClassifier():\n",
    "    def __init__(self,\n",
    "                 batch_size = 20, #number of data\n",
    "                 n_features = 784, #input size\n",
    "                 n_nodes1 = 400, # hidden size of first layer \n",
    "                 n_nodes2 = 200, # hidden size of second layer \n",
    "                 n_output = 10, # 0-9 total 10 numbers\n",
    "                 sigma = 0.02, # parameter initialize \n",
    "                 lr = 0.01, #learning rate \n",
    "                 epoch = 10, #iteration \n",
    "                 verbose=True):\n",
    "        \n",
    "        self.verbose = verbose\n",
    "        self.batch_size = batch_size\n",
    "        self.n_features = n_features\n",
    "        self.n_nodes1 = n_nodes1\n",
    "        self.n_nodes2 = n_nodes2 \n",
    "        self.n_output = n_output\n",
    "        self.sigma = sigma\n",
    "        self.lr = lr\n",
    "        self.epoch = epoch\n",
    "        self.loss_train = []\n",
    "        self.loss_val = []\n",
    "\n",
    "        #neural net initilize\n",
    "        optimizer = SGD(self.lr)\n",
    "        self.FC1 = FC(self.n_features, self.n_nodes1, SimpleInitializer(self.sigma), optimizer)\n",
    "        self.activation1 = activation_func(function_type='tanh')\n",
    "        self.FC2 = FC(self.n_nodes1, self.n_nodes2, SimpleInitializer(self.sigma), optimizer)\n",
    "        self.activation2 = activation_func(function_type='tanh')\n",
    "        self.FC3 = FC(self.n_nodes2, self.n_output, SimpleInitializer(self.sigma), optimizer)\n",
    "        self.activation3 = activation_func(function_type='softmax')\n",
    "\n",
    "    def fit(self, X, y, X_val=None, y_val=None):\n",
    "        \n",
    "        for e in range(self.epoch):\n",
    "            get_mini_batch = GetMiniBatch(X, y, batch_size=self.batch_size)\n",
    "            for mini_X_train, mini_y_train in get_mini_batch:\n",
    "                Z3 = self.forward(mini_X_train)\n",
    "                self.backward(Z3, mini_y_train)\n",
    "                \n",
    "            self.forward(X)\n",
    "            self.loss_train.append(self.cross_entropy_error(y, self.Z3))\n",
    "            \n",
    "            if X_val is not None:\n",
    "                self.forward(X_val)\n",
    "                self.loss_val.append(self.cross_entropy_error(y_val, self.Z3))\n",
    "                \n",
    "            if self.verbose:\n",
    "                if X_val is None:\n",
    "                    print(self.loss_train)\n",
    "                else:\n",
    "                    print(f\"Epoch {e}:\", \"Train loss:\", self.loss_train[-1], \"Val loss:\", self.loss_val[-1])\n",
    "\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \n",
    "        A1 = self.FC1.forward(X)\n",
    "        Z1 = self.activation1.forward(A1)\n",
    "        A2 = self.FC2.forward(Z1)\n",
    "        Z2 = self.activation2.forward(A2)\n",
    "        A3 = self.FC3.forward(Z2)\n",
    "        Z3 = self.activation3.forward(A3)\n",
    "        return Z3\n",
    "\n",
    "    def backward(self, Z3, Y):\n",
    "\n",
    "        dA3 = self.activation3.backward(Z3, Y)\n",
    "        dZ2 = self.FC3.backward(dA3)\n",
    "        dA2 = self.activation2.backward(dZ2)\n",
    "        dZ1 = self.FC2.backward(dA2)\n",
    "        dA1 = self.activation1.backward(dZ1)\n",
    "        dZ0 = self.FC1.backward(dA1) #\n",
    "\n",
    "    \n",
    "    def cross_entropy_error(self, y, Z):\n",
    "        L = - np.sum(y * np.log(Z+1e-7)) / len(y)\n",
    "        return L\n",
    "        \n",
    "    def predict(self, X):\n",
    "        self.forward(X)\n",
    "        return np.argmax(self.Z3, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af393e8-1ae8-4dd2-82a4-44e2585b0135",
   "metadata": {},
   "source": [
    "Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "68632705-747c-4867-b4d3-7011059755f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "44363a98-61bf-4a3f-b05f-1a2f146f379e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 0us/step\n"
     ]
    }
   ],
   "source": [
    "(X_train, y_train), (X_test, y_test) = keras.datasets.mnist.load_data()\n",
    "X_train = X_train.reshape(-1, 784)\n",
    "X_test = X_test.reshape(-1, 784)\n",
    "\n",
    "X_train = X_train / 255.0\n",
    "X_test = X_test / 255.0\n",
    "\n",
    "X_train = X_train[:500, :] #because of computation\n",
    "X_test = X_test[:100, :]\n",
    "y_train = y_train[:500]\n",
    "y_test = y_test[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b7b5b75c-3bb9-406e-b486-0ea13bccab88",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = OneHotEncoder(handle_unknown='ignore')\n",
    "y_train_one_hot = enc.fit_transform(y_train[:, np.newaxis])\n",
    "y_test_one_hot = enc.transform(y_test[:, np.newaxis])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "25d53411-4826-43a2-9134-f46629963eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_nn = ScratchSimpleDeepNeuralNetrowkClassifier(batch_size=8, epoch=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "46126638-41d9-40ff-9a65-1a7fe503ad7a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (200,10) (8,200) (200,10) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmodel_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_one_hot\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test_one_hot\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[7], line 40\u001b[0m, in \u001b[0;36mScratchSimpleDeepNeuralNetrowkClassifier.fit\u001b[1;34m(self, X, y, X_val, y_val)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m mini_X_train, mini_y_train \u001b[38;5;129;01min\u001b[39;00m get_mini_batch:\n\u001b[0;32m     39\u001b[0m     Z3 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(mini_X_train)\n\u001b[1;32m---> 40\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mZ3\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmini_y_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(X)\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_train\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcross_entropy_error(y, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mZ3))\n",
      "Cell \u001b[1;32mIn[7], line 69\u001b[0m, in \u001b[0;36mScratchSimpleDeepNeuralNetrowkClassifier.backward\u001b[1;34m(self, Z3, Y)\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbackward\u001b[39m(\u001b[38;5;28mself\u001b[39m, Z3, Y):\n\u001b[0;32m     68\u001b[0m     dA3 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation3\u001b[38;5;241m.\u001b[39mbackward(Z3, Y)\n\u001b[1;32m---> 69\u001b[0m     dZ2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFC3\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdA3\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     70\u001b[0m     dA2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation2\u001b[38;5;241m.\u001b[39mbackward(dZ2)\n\u001b[0;32m     71\u001b[0m     dZ1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mFC2\u001b[38;5;241m.\u001b[39mbackward(dA2)\n",
      "Cell \u001b[1;32mIn[3], line 49\u001b[0m, in \u001b[0;36mFC.backward\u001b[1;34m(self, dA)\u001b[0m\n\u001b[0;32m     47\u001b[0m dZ \u001b[38;5;241m=\u001b[39m dA \u001b[38;5;241m@\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mW\u001b[38;5;241m.\u001b[39mT\n\u001b[0;32m     48\u001b[0m dB \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(dA, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m---> 49\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mW, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mB \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mW\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mB\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdZ\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdB\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dZ\n",
      "Cell \u001b[1;32mIn[5], line 17\u001b[0m, in \u001b[0;36mSGD.update\u001b[1;34m(self, W, B, dZ, dB)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mupdate\u001b[39m(\u001b[38;5;28mself\u001b[39m, W, B, dZ, dB):\n\u001b[0;32m     11\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;124;03m    ある層の重みやバイアスの更新\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;124;03m    ----------\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m    layer : 更新前の層のインスタンス\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m     W \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlr \u001b[38;5;241m*\u001b[39m dZ\n\u001b[0;32m     18\u001b[0m     B \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlr \u001b[38;5;241m*\u001b[39m dB\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m W, B\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (200,10) (8,200) (200,10) "
     ]
    }
   ],
   "source": [
    "model_nn.fit(X_train, y_train_one_hot.toarray(), X_test, y_test_one_hot.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4aa5a98-e11b-4546-adf7-8ee244639f8f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
