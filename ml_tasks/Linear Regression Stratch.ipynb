{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb2ffba1-557b-472e-825b-d001eca4030d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "71e56549-005a-4771-a66e-78a38c2ce11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScratchLinearRegression():\n",
    "    \n",
    "    \"\"\"\n",
    "   Scratch implementation of linear regression\n",
    "    \n",
    "     Parameters\n",
    "     ----------\n",
    "     num_iter : int\n",
    "       number of iterations\n",
    "     lr: float\n",
    "       learning rate\n",
    "     no_bias : bool\n",
    "       True if no bias term is included\n",
    "     verbose : bool\n",
    "       True to output the learning process\n",
    "    \n",
    "     Attributes\n",
    "     ----------\n",
    "     self.coef_ : ndarray of the following shape, shape (n_features,)\n",
    "       parameters\n",
    "     self.loss : ndarray of the following shape, shape (self.iter,)\n",
    "       Recording losses on training data\n",
    "     self.val_loss : ndarray of the following shape, shape (self.iter,)\n",
    "       Recording losses against validation data\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_iter, lr, no_bias, verbose): \n",
    "        \n",
    "        self.iter = num_iter\n",
    "        self.lr = lr\n",
    "        self.bias = no_bias\n",
    "        self.verbose = verbose\n",
    "     \n",
    "        self.loss = np.zeros(self.iter)\n",
    "        self.val_loss = np.zeros(self.iter)\n",
    "      \n",
    "\n",
    "    # Problem 6 (learning and estimation)\n",
    "    def fit(self, X, y, X_val=None, y_val=None):\n",
    "        \"\"\"\n",
    "        Learn linear regression. If validation data is input, the loss and accuracy for it are also calculated for each iteration.\n",
    "         Parameters\n",
    "         ----------\n",
    "         X : ndarray of the following shape, shape (n_samples, n_features)\n",
    "             Training data features\n",
    "         y : ndarray of the following shape, shape (n_samples, )\n",
    "             Ground truth value of training data\n",
    "         X_val : ndarray of the following shape, shape (n_samples, n_features)\n",
    "             Features of validation data\n",
    "         y_val : ndarray of the following shape, shape (n_samples, )\n",
    "             Correct value of validation data\n",
    "        \"\"\"\n",
    "        \n",
    "        if self.bias == True:\n",
    "            bias = np.ones((X.shape[0], 1))\n",
    "            X = np.hstack((bias, X))\n",
    "            if X_val is not None:\n",
    "                bias = np.ones((X_val.shape[0], 1))\n",
    "                X_val = np.hstack((bias, X_val))\n",
    "            self.coef_ = np.random.rand(X.shape[1])\n",
    "            self.coef_ = self.coef_.reshape(X.shape[1], 1)\n",
    "    \n",
    "\n",
    "        for epoch in range(self.iter):\n",
    "            y_pred = self._linear_hypothesis(X)\n",
    "            self.loss[epoch] = np.mean((y-y_pred)**2)\n",
    "            \n",
    "            if X_val is not None:\n",
    "                pred_val = self._linear_hypothesis(X_val)\n",
    "                self.val_loss[epoch] = np.mean((y_val-pred_val)**2)\n",
    "                \n",
    "            self.coef_ = self._gradient_descent(X, (y_pred-y))\n",
    "           \n",
    "            if self.verbose == True:\n",
    "                print('{}-th epoch train loss {}'.format(epoch, self.loss[epoch]))\n",
    "                if X_val is not None:\n",
    "                    print('{}-th epoch val loss {}'.format(epoch, self.val_loss[epoch] ))\n",
    "\n",
    "\n",
    "    # Question 1\n",
    "    def _linear_hypothesis(self, X):\n",
    "        \"\"\"\n",
    "       Compute the output of the hypothesis function\n",
    "         Parameters\n",
    "         ----------\n",
    "         X : ndarray of the following shape, shape (n_samples, n_features)\n",
    "           training data\n",
    "         Returns\n",
    "         -------\n",
    "         ndarray of the following shape, shape (n_samples, 1)\n",
    "         Estimation results using linear assumption function\n",
    "        \"\"\"\n",
    "        pred = X @ self.coef_\n",
    "        \n",
    "        return pred\n",
    "\n",
    "    # Problem 2\n",
    "    def _gradient_descent(self, X, error):\n",
    "\n",
    "        for i in range(X.shape[1]):\n",
    "            gradient = error*X[:, i]\n",
    "            self.coef_[i, :] = self.coef_[i, :] - self.lr * np.mean(gradient)\n",
    "\n",
    "        return self.coef_\n",
    "        \n",
    "\n",
    "    # Problem 3\n",
    "    def predict(self, X):\n",
    "        if self.bias == True:\n",
    "            bias = np.ones(X.shape[0]).reshape(X.shape[0], 1)\n",
    "            X = np.hstack([bias, X])\n",
    "        pred_y = self._linear_hypothesis(X)\n",
    "        return pred_y\n",
    "\n",
    "    # Question 4\n",
    "    def _mse(self, y_pred, y):\n",
    "        \"\"\"\n",
    "        Calculating mean squared error\n",
    "        \"\"\"\n",
    "        mse = np.mean((y-y_pred)**2)\n",
    "        \n",
    "        return mse\n",
    "\n",
    "    # Question 5\n",
    "    def _loss_func(self, pred, y):\n",
    "        \"\"\"\n",
    "       loss function\n",
    "        \"\"\"\n",
    "        loss = self._mse(pred, y)/2\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a32a0b7-b9f3-439c-a417-f028b4605e1d",
   "metadata": {},
   "source": [
    "Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "da63794b-ccf9-4e18-923a-4baf71ebe077",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "dataset = pd.read_csv(\"../data/train.csv\")\n",
    "X = dataset.loc[:, ['GrLivArea', 'YearBuilt']]\n",
    "y = dataset.loc[:, ['SalePrice']]\n",
    "X = X.values\n",
    "X = MinMaxScaler().fit_transform(X)\n",
    "y = np.log(y.values)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "19eea5ce-4eea-4b86-85c8-f0a6c51d3335",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0-th epoch train loss 116.10735018470682\n",
      "0-th epoch val loss 116.4537993592473\n",
      "1-th epoch train loss 112.49530510458892\n",
      "1-th epoch val loss 112.83534099034716\n",
      "2-th epoch train loss 108.99608232703666\n",
      "2-th epoch val loss 109.32989203219927\n",
      "3-th epoch train loss 105.60616575759407\n",
      "3-th epoch val loss 105.93393076522473\n",
      "4-th epoch train loss 102.32214875706667\n",
      "4-th epoch val loss 102.64404509699301\n",
      "5-th epoch train loss 99.1407307361326\n",
      "5-th epoch val loss 99.45692915153576\n",
      "6-th epoch train loss 96.05871385587243\n",
      "6-th epoch val loss 96.36937996474322\n",
      "7-th epoch train loss 93.07299983092396\n",
      "7-th epoch val loss 93.37829428254457\n",
      "8-th epoch train loss 90.18058683207008\n",
      "8-th epoch val loss 90.48066545867518\n",
      "9-th epoch train loss 87.3785664851679\n",
      "9-th epoch val loss 87.6735804489346\n",
      "10-th epoch train loss 84.66412096342263\n",
      "10-th epoch val loss 84.95421689893348\n",
      "11-th epoch train loss 82.03452017010373\n",
      "11-th epoch val loss 82.319840322423\n",
      "12-th epoch train loss 79.48711900888985\n",
      "12-th epoch val loss 79.76780136738903\n",
      "13-th epoch train loss 77.01935473911844\n",
      "13-th epoch val loss 77.29553316718207\n",
      "14-th epoch train loss 74.62874441329865\n",
      "14-th epoch val loss 74.90054877403819\n",
      "15-th epoch train loss 72.31288239432959\n",
      "15-th epoch val loss 72.58043867242874\n",
      "16-th epoch train loss 70.06943794994514\n",
      "16-th epoch val loss 70.33286836975631\n",
      "17-th epoch train loss 67.89615292198351\n",
      "17-th epoch val loss 68.15557606199131\n",
      "18-th epoch train loss 65.79083946815446\n",
      "18-th epoch val loss 66.04637037191861\n",
      "19-th epoch train loss 63.751377874049794\n",
      "19-th epoch val loss 64.0031281577363\n",
      "20-th epoch train loss 61.77571443321221\n",
      "20-th epoch val loss 62.02379238981832\n",
      "21-th epoch train loss 59.86185939314623\n",
      "21-th epoch val loss 60.10637009352136\n",
      "22-th epoch train loss 58.007884965220086\n",
      "22-th epoch val loss 58.24893035598178\n",
      "23-th epoch train loss 56.21192339647192\n",
      "23-th epoch val loss 56.44960239491279\n",
      "24-th epoch train loss 54.47216510139458\n",
      "24-th epoch val loss 54.70657368747341\n",
      "25-th epoch train loss 52.78685685183428\n",
      "25-th epoch val loss 53.01808815734102\n",
      "26-th epoch train loss 51.15430002319508\n",
      "26-th epoch val loss 51.382444418177556\n",
      "27-th epoch train loss 49.572848895198604\n",
      "27-th epoch val loss 49.797994071735125\n",
      "28-th epoch train loss 48.040909005501994\n",
      "28-th epoch val loss 48.263140058902124\n",
      "29-th epoch train loss 46.55693555453016\n",
      "29-th epoch val loss 46.77633506204309\n",
      "30-th epoch train loss 45.11943185992985\n",
      "30-th epoch val loss 45.33607995703713\n",
      "31-th epoch train loss 43.72694785910169\n",
      "31-th epoch val loss 43.94092231346927\n",
      "32-th epoch train loss 42.378078658315566\n",
      "32-th epoch val loss 42.58945494147716\n",
      "33-th epoch train loss 41.07146312695998\n",
      "33-th epoch val loss 41.28031448380199\n",
      "34-th epoch train loss 39.80578253552206\n",
      "34-th epoch val loss 40.01218005163782\n",
      "35-th epoch train loss 38.57975923593795\n",
      "35-th epoch val loss 38.78377190291715\n",
      "36-th epoch train loss 37.3921553829958\n",
      "36-th epoch val loss 37.59385016171286\n",
      "37-th epoch train loss 36.241771695514714\n",
      "37-th epoch val loss 36.441213577477654\n",
      "38-th epoch train loss 35.12744625606227\n",
      "38-th epoch val loss 35.32469832288232\n",
      "39-th epoch train loss 34.04805334801237\n",
      "39-th epoch val loss 34.243176829051926\n",
      "40-th epoch train loss 33.0025023287816\n",
      "40-th epoch val loss 33.195556657036995\n",
      "41-th epoch train loss 31.98973653811945\n",
      "41-th epoch val loss 32.18077940439287\n",
      "42-th epoch train loss 31.008732240361503\n",
      "42-th epoch val loss 31.197819645774945\n",
      "43-th epoch train loss 30.05849759958989\n",
      "43-th epoch val loss 30.24568390649204\n",
      "44-th epoch train loss 29.138071686676888\n",
      "44-th epoch val loss 29.323409667992912\n",
      "45-th epoch train loss 28.2465235172207\n",
      "45-th epoch val loss 28.43006440429251\n",
      "46-th epoch train loss 27.382951119411917\n",
      "46-th epoch val loss 27.564744648375772\n",
      "47-th epoch train loss 26.546480630900234\n",
      "47-th epoch val loss 26.72657508764664\n",
      "48-th epoch train loss 25.736265423759175\n",
      "48-th epoch val loss 25.914707687518767\n",
      "49-th epoch train loss 24.95148525667493\n",
      "49-th epoch val loss 25.128320842272757\n",
      "50-th epoch train loss 24.191345453512604\n",
      "50-th epoch val loss 24.366618552331825\n",
      "51-th epoch train loss 23.455076107439446\n",
      "51-th epoch val loss 23.628829627134134\n",
      "52-th epoch train loss 22.74193130981004\n",
      "52-th epoch val loss 22.91420691280574\n",
      "53-th epoch train loss 22.05118840304341\n",
      "53-th epoch val loss 22.222026543862718\n",
      "54-th epoch train loss 21.382147256745714\n",
      "54-th epoch val loss 21.551587218195095\n",
      "55-th epoch train loss 20.734129566355517\n",
      "55-th epoch val loss 20.90220949460841\n",
      "56-th epoch train loss 20.10647817361114\n",
      "56-th epoch val loss 20.273235112221364\n",
      "57-th epoch train loss 19.498556408161207\n",
      "57-th epoch val loss 19.66402633103954\n",
      "58-th epoch train loss 18.909747449660912\n",
      "58-th epoch val loss 19.07396529304682\n",
      "59-th epoch train loss 18.339453709716654\n",
      "59-th epoch val loss 18.502453403176066\n",
      "60-th epoch train loss 17.787096233061746\n",
      "60-th epoch val loss 17.94891072954081\n",
      "61-th epoch train loss 17.252114117364993\n",
      "61-th epoch val loss 17.412775422328956\n",
      "62-th epoch train loss 16.733963951092573\n",
      "62-th epoch val loss 16.893503150777775\n",
      "63-th epoch train loss 16.232119268861638\n",
      "63-th epoch val loss 16.39056655766804\n",
      "64-th epoch train loss 15.746070023741561\n",
      "64-th epoch val loss 15.903454730792145\n",
      "65-th epoch train loss 15.275322075975728\n",
      "65-th epoch val loss 15.431672690868416\n",
      "66-th epoch train loss 14.819396697612952\n",
      "66-th epoch val loss 14.974740895389873\n",
      "67-th epoch train loss 14.377830092553793\n",
      "67-th epoch val loss 14.532194757911908\n",
      "68-th epoch train loss 13.950172931532146\n",
      "68-th epoch val loss 14.103584182298594\n",
      "69-th epoch train loss 13.535989901567612\n",
      "69-th epoch val loss 13.688473111462338\n",
      "70-th epoch train loss 13.13485926943849\n",
      "70-th epoch val loss 13.286439090146041\n",
      "71-th epoch train loss 12.746372458739184\n",
      "71-th epoch val loss 12.897072841310907\n",
      "72-th epoch train loss 12.37013364009958\n",
      "72-th epoch val loss 12.519977855706761\n",
      "73-th epoch train loss 12.005759334156858\n",
      "73-th epoch val loss 12.15476999421475\n",
      "74-th epoch train loss 11.652878026883089\n",
      "74-th epoch val loss 11.801077102565117\n",
      "75-th epoch train loss 11.311129796884257\n",
      "75-th epoch val loss 11.45853863804512\n",
      "76-th epoch train loss 10.98016595429831\n",
      "76-th epoch val loss 11.126805307824108\n",
      "77-th epoch train loss 10.659648690931359\n",
      "77-th epoch val loss 10.80553871853434\n",
      "78-th epoch train loss 10.349250741282535\n",
      "78-th epoch val loss 10.494411036757455\n",
      "79-th epoch train loss 10.048655054118642\n",
      "79-th epoch val loss 10.193104660077305\n",
      "80-th epoch train loss 9.757554474270558\n",
      "80-th epoch val loss 9.901311898370448\n",
      "81-th epoch train loss 9.475651434333258\n",
      "81-th epoch val loss 9.61873466501585\n",
      "82-th epoch train loss 9.202657655961483\n",
      "82-th epoch val loss 9.345084177715199\n",
      "83-th epoch train loss 8.938293860462455\n",
      "83-th epoch val loss 9.08008066862485\n",
      "84-th epoch train loss 8.68228948839647\n",
      "84-th epoch val loss 8.823453103509754\n",
      "85-th epoch train loss 8.434382427905092\n",
      "85-th epoch val loss 8.574938909638673\n",
      "86-th epoch train loss 8.19431875149547\n",
      "86-th epoch val loss 8.334283712148736\n",
      "87-th epoch train loss 7.961852461017655\n",
      "87-th epoch val loss 8.101241078615905\n",
      "88-th epoch train loss 7.736745240580105\n",
      "88-th epoch val loss 7.875572271576001\n",
      "89-th epoch train loss 7.5187662171563225\n",
      "89-th epoch val loss 7.657046008748989\n",
      "90-th epoch train loss 7.307691728643412\n",
      "90-th epoch val loss 7.445438230726847\n",
      "91-th epoch train loss 7.103305099140704\n",
      "91-th epoch val loss 7.240531875892831\n",
      "92-th epoch train loss 6.905396421223789\n",
      "92-th epoch val loss 7.042116662347152\n",
      "93-th epoch train loss 6.713762344996349\n",
      "93-th epoch val loss 6.849988876621081\n",
      "94-th epoch train loss 6.528205873708922\n",
      "94-th epoch val loss 6.663951168968296\n",
      "95-th epoch train loss 6.348536165740237\n",
      "95-th epoch val loss 6.483812355028821\n",
      "96-th epoch train loss 6.174568342743228\n",
      "96-th epoch val loss 6.309387223667318\n",
      "97-th epoch train loss 6.006123303763874\n",
      "97-th epoch val loss 6.140496350793594\n",
      "98-th epoch train loss 5.843027545147051\n",
      "98-th epoch val loss 5.976965918979244\n",
      "99-th epoch train loss 5.6851129860493455\n",
      "99-th epoch val loss 5.8186275426900425\n",
      "100-th epoch train loss 5.532216799384353\n",
      "100-th epoch val loss 5.665318098959413\n",
      "101-th epoch train loss 5.384181248031464\n",
      "101-th epoch val loss 5.516879563333636\n",
      "102-th epoch train loss 5.24085352614433\n",
      "102-th epoch val loss 5.373158850924794\n",
      "103-th epoch train loss 5.102085605400372\n",
      "103-th epoch val loss 5.234007662412521\n",
      "104-th epoch train loss 4.9677340860375585\n",
      "104-th epoch val loss 5.099282334840599\n",
      "105-th epoch train loss 4.837660052529508\n",
      "105-th epoch val loss 4.968843697059158\n",
      "106-th epoch train loss 4.711728933754582\n",
      "106-th epoch val loss 4.842556929667984\n",
      "107-th epoch train loss 4.589810367519158\n",
      "107-th epoch val loss 4.72029142932085\n",
      "108-th epoch train loss 4.471778069299542\n",
      "108-th epoch val loss 4.6019206772551735\n",
      "109-th epoch train loss 4.357509705071318\n",
      "109-th epoch val loss 4.487322111915548\n",
      "110-th epoch train loss 4.246886768098879\n",
      "110-th epoch val loss 4.376377005543711\n",
      "111-th epoch train loss 4.139794459561947\n",
      "111-th epoch val loss 4.268970344611582\n",
      "112-th epoch train loss 4.0361215728996775\n",
      "112-th epoch val loss 4.164990713977735\n",
      "113-th epoch train loss 3.9357603817566584\n",
      "113-th epoch val loss 4.064330184651485\n",
      "114-th epoch train loss 3.838606531418691\n",
      "114-th epoch val loss 3.966884205052279\n",
      "115-th epoch train loss 3.744558933629806\n",
      "115-th epoch val loss 3.8725514956556553\n",
      "116-th epoch train loss 3.653519664685252\n",
      "116-th epoch val loss 3.781233946920377\n",
      "117-th epoch train loss 3.565393866698514\n",
      "117-th epoch val loss 3.6928365203946263\n",
      "118-th epoch train loss 3.4800896519436018\n",
      "118-th epoch val loss 3.6072671529023266\n",
      "119-th epoch train loss 3.397518010176874\n",
      "119-th epoch val loss 3.5244366637137547\n",
      "120-th epoch train loss 3.317592718845704\n",
      "120-th epoch val loss 3.4442586646075455\n",
      "121-th epoch train loss 3.2402302560941174\n",
      "121-th epoch val loss 3.366649472734112\n",
      "122-th epoch train loss 3.165349716478365\n",
      "122-th epoch val loss 3.2915280261933275\n",
      "123-th epoch train loss 3.0928727293080915\n",
      "123-th epoch val loss 3.2188158022419437\n",
      "124-th epoch train loss 3.0227233795313833\n",
      "124-th epoch val loss 3.1484367380489524\n",
      "125-th epoch train loss 2.9548281310845064\n",
      "125-th epoch val loss 3.080317153919548\n",
      "126-th epoch train loss 2.8891157526296465\n",
      "126-th epoch val loss 3.0143856789108927\n",
      "127-th epoch train loss 2.825517245606317\n",
      "127-th epoch val loss 2.950573178765234\n",
      "128-th epoch train loss 2.763965774524407\n",
      "128-th epoch val loss 2.8888126860882295\n",
      "129-th epoch train loss 2.70439659942912\n",
      "129-th epoch val loss 2.8290393327026297\n",
      "130-th epoch train loss 2.6467470104701887\n",
      "130-th epoch val loss 2.7711902841095792\n",
      "131-th epoch train loss 2.590956264509866\n",
      "131-th epoch val loss 2.7152046759919624\n",
      "132-th epoch train loss 2.53696552370624\n",
      "132-th epoch val loss 2.661023552696215\n",
      "133-th epoch train loss 2.484717796010377\n",
      "133-th epoch val loss 2.6085898076310183\n",
      "134-th epoch train loss 2.43415787751772\n",
      "134-th epoch val loss 2.5578481255232237\n",
      "135-th epoch train loss 2.385232296616037\n",
      "135-th epoch val loss 2.5087449264731845\n",
      "136-th epoch train loss 2.337889259873962\n",
      "136-th epoch val loss 2.461228311753476\n",
      "137-th epoch train loss 2.292078599615984\n",
      "137-th epoch val loss 2.415248011296757\n",
      "138-th epoch train loss 2.2477517231313535\n",
      "138-th epoch val loss 2.370755332820165\n",
      "139-th epoch train loss 2.2048615634660513\n",
      "139-th epoch val loss 2.327703112535321\n",
      "140-th epoch train loss 2.163362531748551\n",
      "140-th epoch val loss 2.286045667394569\n",
      "141-th epoch train loss 2.1232104710015967\n",
      "141-th epoch val loss 2.245738748825633\n",
      "142-th epoch train loss 2.084362611393758\n",
      "142-th epoch val loss 2.2067394979083526\n",
      "143-th epoch train loss 2.04677752688592\n",
      "143-th epoch val loss 2.169006401948596\n",
      "144-th epoch train loss 2.010415093229294\n",
      "144-th epoch val loss 2.1324992524058737\n",
      "145-th epoch train loss 1.9752364472728563\n",
      "145-th epoch val loss 2.097179104132473\n",
      "146-th epoch train loss 1.94120394753945\n",
      "146-th epoch val loss 2.063008235883312\n",
      "147-th epoch train loss 1.908281136031055\n",
      "147-th epoch val loss 2.0299501120569343\n",
      "148-th epoch train loss 1.87643270122495\n",
      "148-th epoch val loss 1.997969345629316\n",
      "149-th epoch train loss 1.8456244422236787\n",
      "149-th epoch val loss 1.9670316622433388\n",
      "150-th epoch train loss 1.8158232340229061\n",
      "150-th epoch val loss 1.9371038654179615\n",
      "151-th epoch train loss 1.7869969938623507\n",
      "151-th epoch val loss 1.9081538028422094\n",
      "152-th epoch train loss 1.759114648626059\n",
      "152-th epoch val loss 1.880150333720209\n",
      "153-th epoch train loss 1.7321461032593544\n",
      "153-th epoch val loss 1.8530632971345407\n",
      "154-th epoch train loss 1.7060622101708014\n",
      "154-th epoch val loss 1.8268634813961997\n",
      "155-th epoch train loss 1.680834739588502\n",
      "155-th epoch val loss 1.801522594350436\n",
      "156-th epoch train loss 1.656436350841014\n",
      "156-th epoch val loss 1.7770132346087197\n",
      "157-th epoch train loss 1.6328405645340913\n",
      "157-th epoch val loss 1.7533088636779743\n",
      "158-th epoch train loss 1.6100217355953406\n",
      "158-th epoch val loss 1.7303837789591456\n",
      "159-th epoch train loss 1.5879550271597809\n",
      "159-th epoch val loss 1.7082130875880317\n",
      "160-th epoch train loss 1.5666163852700947\n",
      "160-th epoch val loss 1.6867726810921366\n",
      "161-th epoch train loss 1.545982514366214\n",
      "161-th epoch val loss 1.6660392108381412\n",
      "162-th epoch train loss 1.526030853539644\n",
      "162-th epoch val loss 1.6459900642453573\n",
      "163-th epoch train loss 1.5067395535287134\n",
      "163-th epoch val loss 1.6266033417413202\n",
      "164-th epoch train loss 1.4880874544316642\n",
      "164-th epoch val loss 1.6078578344363896\n",
      "165-th epoch train loss 1.4700540641152255\n",
      "165-th epoch val loss 1.589733002494971\n",
      "166-th epoch train loss 1.4526195372970099\n",
      "166-th epoch val loss 1.5722089541816666\n",
      "167-th epoch train loss 1.4357646552807322\n",
      "167-th epoch val loss 1.5552664255613082\n",
      "168-th epoch train loss 1.419470806323925\n",
      "168-th epoch val loss 1.538886760832536\n",
      "169-th epoch train loss 1.4037199666184388\n",
      "169-th epoch val loss 1.5230518932751538\n",
      "170-th epoch train loss 1.3884946818646375\n",
      "170-th epoch val loss 1.5077443267921626\n",
      "171-th epoch train loss 1.373778049420799\n",
      "171-th epoch val loss 1.4929471180279437\n",
      "172-th epoch train loss 1.3595537010097867\n",
      "172-th epoch val loss 1.4786438590446278\n",
      "173-th epoch train loss 1.3458057859656456\n",
      "173-th epoch val loss 1.4648186605392768\n",
      "174-th epoch train loss 1.332518955003286\n",
      "174-th epoch val loss 1.4514561355850197\n",
      "175-th epoch train loss 1.3196783444949647\n",
      "175-th epoch val loss 1.4385413838798176\n",
      "176-th epoch train loss 1.3072695612377656\n",
      "176-th epoch val loss 1.4260599764870423\n",
      "177-th epoch train loss 1.2952786676967862\n",
      "177-th epoch val loss 1.4139979410525458\n",
      "178-th epoch train loss 1.2836921677092006\n",
      "178-th epoch val loss 1.4023417474833724\n",
      "179-th epoch train loss 1.2724969926348404\n",
      "179-th epoch val loss 1.3910782940737219\n",
      "180-th epoch train loss 1.2616804879393706\n",
      "180-th epoch val loss 1.3801948940642352\n",
      "181-th epoch train loss 1.251230400196595\n",
      "181-th epoch val loss 1.3696792626210967\n",
      "182-th epoch train loss 1.241134864496807\n",
      "182-th epoch val loss 1.3595195042218626\n",
      "183-th epoch train loss 1.2313823922485474\n",
      "183-th epoch val loss 1.3497041004353516\n",
      "184-th epoch train loss 1.2219618593614985\n",
      "184-th epoch val loss 1.3402218980833012\n",
      "185-th epoch train loss 1.212862494798633\n",
      "185-th epoch val loss 1.3310620977719043\n",
      "186-th epoch train loss 1.2040738694861206\n",
      "186-th epoch val loss 1.322214242781693\n",
      "187-th epoch train loss 1.19558588556982\n",
      "187-th epoch val loss 1.3136682083045939\n",
      "188-th epoch train loss 1.18738876600757\n",
      "188-th epoch val loss 1.305414191017339\n",
      "189-th epoch train loss 1.179473044486798\n",
      "189-th epoch val loss 1.2974426989807475\n",
      "190-th epoch train loss 1.171829555657318\n",
      "190-th epoch val loss 1.2897445418547178\n",
      "191-th epoch train loss 1.1644494256694697\n",
      "191-th epoch val loss 1.2823108214190824\n",
      "192-th epoch train loss 1.157324063008107\n",
      "192-th epoch val loss 1.275132922390803\n",
      "193-th epoch train loss 1.1504451496131816\n",
      "193-th epoch val loss 1.2682025035282507\n",
      "194-th epoch train loss 1.1438046322780073\n",
      "194-th epoch val loss 1.2615114890136296\n",
      "195-th epoch train loss 1.1373947143165366\n",
      "195-th epoch val loss 1.2550520601048756\n",
      "196-th epoch train loss 1.1312078474912626\n",
      "196-th epoch val loss 1.2488166470486108\n",
      "197-th epoch train loss 1.1252367241936136\n",
      "197-th epoch val loss 1.24279792124603\n",
      "198-th epoch train loss 1.1194742698689761\n",
      "198-th epoch val loss 1.2369887876638246\n",
      "199-th epoch train loss 1.1139136356787058\n",
      "199-th epoch val loss 1.2313823774824995\n",
      "200-th epoch train loss 1.1085481913917414\n",
      "200-th epoch val loss 1.2259720409746828\n",
      "201-th epoch train loss 1.103371518498655\n",
      "201-th epoch val loss 1.2207513406062551\n",
      "202-th epoch train loss 1.09837740354121\n",
      "202-th epoch val loss 1.2157140443533483\n",
      "203-th epoch train loss 1.093559831650687\n",
      "203-th epoch val loss 1.2108541192284772\n",
      "204-th epoch train loss 1.0889129802884814\n",
      "204-th epoch val loss 1.2061657250092828\n",
      "205-th epoch train loss 1.0844312131826526\n",
      "205-th epoch val loss 1.2016432081635702\n",
      "206-th epoch train loss 1.0801090744543134\n",
      "206-th epoch val loss 1.197281095964506\n",
      "207-th epoch train loss 1.0759412829279396\n",
      "207-th epoch val loss 1.193074090790059\n",
      "208-th epoch train loss 1.0719227266198534\n",
      "208-th epoch val loss 1.189017064600919\n",
      "209-th epoch train loss 1.0680484573993299\n",
      "209-th epoch val loss 1.1851050535913379\n",
      "210-th epoch train loss 1.06431368581693\n",
      "210-th epoch val loss 1.1813332530074865\n",
      "211-th epoch train loss 1.0607137760948497\n",
      "211-th epoch val loss 1.1776970121281098\n",
      "212-th epoch train loss 1.0572442412742238\n",
      "212-th epoch val loss 1.1741918294024067\n",
      "213-th epoch train loss 1.0539007385144834\n",
      "213-th epoch val loss 1.1708133477402334\n",
      "214-th epoch train loss 1.0506790645400306\n",
      "214-th epoch val loss 1.1675573499498748\n",
      "215-th epoch train loss 1.0475751512296172\n",
      "215-th epoch val loss 1.164419754318774\n",
      "216-th epoch train loss 1.044585061343986\n",
      "216-th epoch val loss 1.1613966103327642\n",
      "217-th epoch train loss 1.0417049843874489\n",
      "217-th epoch val loss 1.1584840945294725\n",
      "218-th epoch train loss 1.038931232599226\n",
      "218-th epoch val loss 1.1556785064817152\n",
      "219-th epoch train loss 1.0362602370704905\n",
      "219-th epoch val loss 1.152976264906815\n",
      "220-th epoch train loss 1.033688543983191\n",
      "220-th epoch val loss 1.1503739038979166\n",
      "221-th epoch train loss 1.0312128109668617\n",
      "221-th epoch val loss 1.1478680692734955\n",
      "222-th epoch train loss 1.0288298035697145\n",
      "222-th epoch val loss 1.1454555150413506\n",
      "223-th epoch train loss 1.0265363918404649\n",
      "223-th epoch val loss 1.1431330999735294\n",
      "224-th epoch train loss 1.0243295470174165\n",
      "224-th epoch val loss 1.140897784288703\n",
      "225-th epoch train loss 1.0222063383214666\n",
      "225-th epoch val loss 1.1387466264386423\n",
      "226-th epoch train loss 1.0201639298497753\n",
      "226-th epoch val loss 1.1366767799955457\n",
      "227-th epoch train loss 1.0181995775669588\n",
      "227-th epoch val loss 1.1346854906370596\n",
      "228-th epoch train loss 1.0163106263907624\n",
      "228-th epoch val loss 1.1327700932259492\n",
      "229-th epoch train loss 1.0144945073692548\n",
      "229-th epoch val loss 1.1309280089814588\n",
      "230-th epoch train loss 1.012748734946687\n",
      "230-th epoch val loss 1.129156742739492\n",
      "231-th epoch train loss 1.0110709043152464\n",
      "231-th epoch val loss 1.127453880298844\n",
      "232-th epoch train loss 1.0094586888500166\n",
      "232-th epoch val loss 1.125817085850793\n",
      "233-th epoch train loss 1.0079098376245488\n",
      "233-th epoch val loss 1.1242440994894454\n",
      "234-th epoch train loss 1.0064221730045173\n",
      "234-th epoch val loss 1.1227327348003104\n",
      "235-th epoch train loss 1.0049935883170262\n",
      "235-th epoch val loss 1.1212808765246611\n",
      "236-th epoch train loss 1.0036220455931955\n",
      "236-th epoch val loss 1.1198864782973137\n",
      "237-th epoch train loss 1.0023055733817376\n",
      "237-th epoch val loss 1.1185475604555217\n",
      "238-th epoch train loss 1.001042264631305\n",
      "238-th epoch val loss 1.117262207916771\n",
      "239-th epoch train loss 0.99983027463946\n",
      "239-th epoch val loss 1.1160285681233202\n",
      "240-th epoch train loss 0.9986678190661755\n",
      "240-th epoch val loss 1.1148448490513907\n",
      "241-th epoch train loss 0.9975531720098597\n",
      "241-th epoch val loss 1.1137093172829946\n",
      "242-th epoch train loss 0.9964846641439387\n",
      "242-th epoch val loss 1.112620296138437\n",
      "243-th epoch train loss 0.9954606809121073\n",
      "243-th epoch val loss 1.1115761638675923\n",
      "244-th epoch train loss 0.9944796607804124\n",
      "244-th epoch val loss 1.1105753518981214\n",
      "245-th epoch train loss 0.9935400935443901\n",
      "245-th epoch val loss 1.1096163431388446\n",
      "246-th epoch train loss 0.9926405186895307\n",
      "246-th epoch val loss 1.1086976703365434\n",
      "247-th epoch train loss 0.9917795238034087\n",
      "247-th epoch val loss 1.1078179144845266\n",
      "248-th epoch train loss 0.9909557430378548\n",
      "248-th epoch val loss 1.1069757032813277\n",
      "249-th epoch train loss 0.9901678556196065\n",
      "249-th epoch val loss 1.1061697096379777\n",
      "250-th epoch train loss 0.9894145844079181\n",
      "250-th epoch val loss 1.1053986502323265\n",
      "251-th epoch train loss 0.9886946944976617\n",
      "251-th epoch val loss 1.1046612841089376\n",
      "252-th epoch train loss 0.9880069918664922\n",
      "252-th epoch val loss 1.1039564113231384\n",
      "253-th epoch train loss 0.9873503220646974\n",
      "253-th epoch val loss 1.1032828716278285\n",
      "254-th epoch train loss 0.9867235689463968\n",
      "254-th epoch val loss 1.102639543201726\n",
      "255-th epoch train loss 0.9861256534407947\n",
      "255-th epoch val loss 1.1020253414177372\n",
      "256-th epoch train loss 0.9855555323622267\n",
      "256-th epoch val loss 1.1014392176502017\n",
      "257-th epoch train loss 0.9850121972577955\n",
      "257-th epoch val loss 1.1008801581197938\n",
      "258-th epoch train loss 0.9844946732914043\n",
      "258-th epoch val loss 1.100347182774898\n",
      "259-th epoch train loss 0.9840020181630608\n",
      "259-th epoch val loss 1.0998393442083199\n",
      "260-th epoch train loss 0.9835333210623335\n",
      "260-th epoch val loss 1.0993557266082201\n",
      "261-th epoch train loss 0.9830877016548989\n",
      "261-th epoch val loss 1.0988954447422001\n",
      "262-th epoch train loss 0.9826643091011397\n",
      "262-th epoch val loss 1.098457642973509\n",
      "263-th epoch train loss 0.9822623211057853\n",
      "263-th epoch val loss 1.0980414943083443\n",
      "264-th epoch train loss 0.9818809429976263\n",
      "264-th epoch val loss 1.097646199473299\n",
      "265-th epoch train loss 0.9815194068383561\n",
      "265-th epoch val loss 1.0972709860219831\n",
      "266-th epoch train loss 0.9811769705596276\n",
      "266-th epoch val loss 1.0969151074699237\n",
      "267-th epoch train loss 0.9808529171274389\n",
      "267-th epoch val loss 1.096577842456847\n",
      "268-th epoch train loss 0.980546553732993\n",
      "268-th epoch val loss 1.0962584939354851\n",
      "269-th epoch train loss 0.980257211009193\n",
      "269-th epoch val loss 1.095956388386077\n",
      "270-th epoch train loss 0.9799842422719814\n",
      "270-th epoch val loss 1.095670875055758\n",
      "271-th epoch train loss 0.9797270227857273\n",
      "271-th epoch val loss 1.0954013252220494\n",
      "272-th epoch train loss 0.9794849490519234\n",
      "272-th epoch val loss 1.095147131479705\n",
      "273-th epoch train loss 0.9792574381204451\n",
      "273-th epoch val loss 1.0949077070501667\n",
      "274-th epoch train loss 0.9790439269226746\n",
      "274-th epoch val loss 1.0946824851129309\n",
      "275-th epoch train loss 0.9788438716257973\n",
      "275-th epoch val loss 1.0944709181581294\n",
      "276-th epoch train loss 0.9786567470076076\n",
      "276-th epoch val loss 1.0942724773596644\n",
      "277-th epoch train loss 0.9784820458511779\n",
      "277-th epoch val loss 1.0940866519682484\n",
      "278-th epoch train loss 0.9783192783587674\n",
      "278-th epoch val loss 1.093912948723722\n",
      "279-th epoch train loss 0.9781679715843676\n",
      "279-th epoch val loss 1.0937508912860527\n",
      "280-th epoch train loss 0.9780276688842966\n",
      "280-th epoch val loss 1.093600019684414\n",
      "281-th epoch train loss 0.9778979293852713\n",
      "281-th epoch val loss 1.0934598897837882\n",
      "282-th epoch train loss 0.9777783274694185\n",
      "282-th epoch val loss 1.0933300727685356\n",
      "283-th epoch train loss 0.977668452275678\n",
      "283-th epoch val loss 1.0932101546423982\n",
      "284-th epoch train loss 0.9775679072170941\n",
      "284-th epoch val loss 1.0930997357444225\n",
      "285-th epoch train loss 0.9774763095134872\n",
      "285-th epoch val loss 1.0929984302802962\n",
      "286-th epoch train loss 0.977393289739027\n",
      "286-th epoch val loss 1.09290586586862\n",
      "287-th epoch train loss 0.9773184913842331\n",
      "287-th epoch val loss 1.0928216831016389\n",
      "288-th epoch train loss 0.9772515704319545\n",
      "288-th epoch val loss 1.0927455351199822\n",
      "289-th epoch train loss 0.9771921949468838\n",
      "289-th epoch val loss 1.092677087200966\n",
      "290-th epoch train loss 0.9771400446781774\n",
      "290-th epoch val loss 1.0926160163600376\n",
      "291-th epoch train loss 0.9770948106747769\n",
      "291-th epoch val loss 1.092562010964944\n",
      "292-th epoch train loss 0.9770561949130242\n",
      "292-th epoch val loss 1.09251477036222\n",
      "293-th epoch train loss 0.9770239099361835\n",
      "293-th epoch val loss 1.0924740045156185\n",
      "294-th epoch train loss 0.9769976785055\n",
      "294-th epoch val loss 1.0924394336560945\n",
      "295-th epoch train loss 0.9769772332624251\n",
      "295-th epoch val loss 1.0924107879429878\n",
      "296-th epoch train loss 0.9769623164016595\n",
      "296-th epoch val loss 1.092387807136045\n",
      "297-th epoch train loss 0.9769526793546721\n",
      "297-th epoch val loss 1.0923702402779432\n",
      "298-th epoch train loss 0.9769480824833654\n",
      "298-th epoch val loss 1.092357845386981\n",
      "299-th epoch train loss 0.9769482947835633\n",
      "299-th epoch val loss 1.0923503891596198\n",
      "300-th epoch train loss 0.9769530935980152\n",
      "300-th epoch val loss 1.0923476466825597\n",
      "301-th epoch train loss 0.9769622643386152\n",
      "301-th epoch val loss 1.0923494011540518\n",
      "302-th epoch train loss 0.9769756002175427\n",
      "302-th epoch val loss 1.0923554436141591\n",
      "303-th epoch train loss 0.9769929019870472\n",
      "303-th epoch val loss 1.0923655726836734\n",
      "304-th epoch train loss 0.9770139776875947\n",
      "304-th epoch val loss 1.0923795943114223\n",
      "305-th epoch train loss 0.977038642404128\n",
      "305-th epoch val loss 1.0923973215297036\n",
      "306-th epoch train loss 0.9770667180301649\n",
      "306-th epoch val loss 1.09241857421758\n",
      "307-th epoch train loss 0.9770980330395023\n",
      "307-th epoch val loss 1.0924431788717968\n",
      "308-th epoch train loss 0.9771324222652724\n",
      "308-th epoch val loss 1.0924709683850717\n",
      "309-th epoch train loss 0.9771697266861328\n",
      "309-th epoch val loss 1.0925017818315348\n",
      "310-th epoch train loss 0.9772097932193501\n",
      "310-th epoch val loss 1.0925354642590772\n",
      "311-th epoch train loss 0.9772524745205691\n",
      "311-th epoch val loss 1.0925718664884077\n",
      "312-th epoch train loss 0.9772976287900499\n",
      "312-th epoch val loss 1.0926108449185874\n",
      "313-th epoch train loss 0.977345119585172\n",
      "313-th epoch val loss 1.0926522613388516\n",
      "314-th epoch train loss 0.9773948156390045\n",
      "314-th epoch val loss 1.0926959827465117\n",
      "315-th epoch train loss 0.9774465906847483\n",
      "315-th epoch val loss 1.0927418811707437\n",
      "316-th epoch train loss 0.9775003232858693\n",
      "316-th epoch val loss 1.092789833502084\n",
      "317-th epoch train loss 0.977555896671739\n",
      "317-th epoch val loss 1.0928397213274461\n",
      "318-th epoch train loss 0.9776131985786035\n",
      "318-th epoch val loss 1.092891430770479\n",
      "319-th epoch train loss 0.9776721210957214\n",
      "319-th epoch val loss 1.092944852337109\n",
      "320-th epoch train loss 0.9777325605164976\n",
      "320-th epoch val loss 1.0929998807660921\n",
      "321-th epoch train loss 0.9777944171944597\n",
      "321-th epoch val loss 1.0930564148844193\n",
      "322-th epoch train loss 0.9778575954039171\n",
      "322-th epoch val loss 1.093114357467418\n",
      "323-th epoch train loss 0.9779220032051668\n",
      "323-th epoch val loss 1.0931736151034144\n",
      "324-th epoch train loss 0.9779875523140793\n",
      "324-th epoch val loss 1.093234098062786\n",
      "325-th epoch train loss 0.9780541579759526\n",
      "325-th epoch val loss 1.0932957201712943\n",
      "326-th epoch train loss 0.9781217388434718\n",
      "326-th epoch val loss 1.0933583986875357\n",
      "327-th epoch train loss 0.9781902168586656\n",
      "327-th epoch val loss 1.0934220541843986\n",
      "328-th epoch train loss 0.9782595171387167\n",
      "328-th epoch val loss 1.093486610434387\n",
      "329-th epoch train loss 0.9783295678655125\n",
      "329-th epoch val loss 1.0935519942986938\n",
      "330-th epoch train loss 0.9784003001788125\n",
      "330-th epoch val loss 1.093618135619903\n",
      "331-th epoch train loss 0.9784716480729184\n",
      "331-th epoch val loss 1.0936849671182025\n",
      "332-th epoch train loss 0.9785435482967374\n",
      "332-th epoch val loss 1.0937524242910004\n",
      "333-th epoch train loss 0.9786159402571234\n",
      "333-th epoch val loss 1.0938204453158313\n",
      "334-th epoch train loss 0.9786887659254007\n",
      "334-th epoch val loss 1.0938889709564512\n",
      "335-th epoch train loss 0.978761969746963\n",
      "335-th epoch val loss 1.0939579444720178\n",
      "336-th epoch train loss 0.9788354985538478\n",
      "336-th epoch val loss 1.0940273115292567\n",
      "337-th epoch train loss 0.9789093014801967\n",
      "337-th epoch val loss 1.094097020117522\n",
      "338-th epoch train loss 0.9789833298805056\n",
      "338-th epoch val loss 1.094167020466652\n",
      "339-th epoch train loss 0.9790575372505738\n",
      "339-th epoch val loss 1.0942372649675376\n",
      "340-th epoch train loss 0.9791318791510729\n",
      "340-th epoch val loss 1.0943077080953134\n",
      "341-th epoch train loss 0.9792063131336411\n",
      "341-th epoch val loss 1.094378306335085\n",
      "342-th epoch train loss 0.979280798669431\n",
      "342-th epoch val loss 1.0944490181101174\n",
      "343-th epoch train loss 0.9793552970800282\n",
      "343-th epoch val loss 1.0945198037124022\n",
      "344-th epoch train loss 0.9794297714706632\n",
      "344-th epoch val loss 1.0945906252355224\n",
      "345-th epoch train loss 0.9795041866656496\n",
      "345-th epoch val loss 1.0946614465097546\n",
      "346-th epoch train loss 0.9795785091459656\n",
      "346-th epoch val loss 1.09473223303932\n",
      "347-th epoch train loss 0.9796527069889229\n",
      "347-th epoch val loss 1.094802951941729\n",
      "348-th epoch train loss 0.9797267498098473\n",
      "348-th epoch val loss 1.0948735718891436\n",
      "349-th epoch train loss 0.9798006087057078\n",
      "349-th epoch val loss 1.0949440630516987\n",
      "350-th epoch train loss 0.9798742562006352\n",
      "350-th epoch val loss 1.0950143970427153\n",
      "351-th epoch train loss 0.9799476661932627\n",
      "351-th epoch val loss 1.0950845468657422\n",
      "352-th epoch train loss 0.9800208139058366\n",
      "352-th epoch val loss 1.0951544868633774\n",
      "353-th epoch train loss 0.9800936758350388\n",
      "353-th epoch val loss 1.0952241926678032\n",
      "354-th epoch train loss 0.980166229704458\n",
      "354-th epoch val loss 1.0952936411529777\n",
      "355-th epoch train loss 0.980238454418668\n",
      "355-th epoch val loss 1.0953628103884399\n",
      "356-th epoch train loss 0.9803103300188545\n",
      "356-th epoch val loss 1.095431679594667\n",
      "357-th epoch train loss 0.9803818376399396\n",
      "357-th epoch val loss 1.0955002290999367\n",
      "358-th epoch train loss 0.9804529594691606\n",
      "358-th epoch val loss 1.0955684402986476\n",
      "359-th epoch train loss 0.980523678706049\n",
      "359-th epoch val loss 1.0956362956110448\n",
      "360-th epoch train loss 0.980593979523773\n",
      "360-th epoch val loss 1.0957037784443169\n",
      "361-th epoch train loss 0.9806638470317895\n",
      "361-th epoch val loss 1.0957708731550042\n",
      "362-th epoch train loss 0.9807332672397739\n",
      "362-th epoch val loss 1.09583756501269\n",
      "363-th epoch train loss 0.9808022270227752\n",
      "363-th epoch val loss 1.0959038401649246\n",
      "364-th epoch train loss 0.980870714087564\n",
      "364-th epoch val loss 1.0959696856033447\n",
      "365-th epoch train loss 0.9809387169401349\n",
      "365-th epoch val loss 1.096035089130951\n",
      "366-th epoch train loss 0.9810062248543189\n",
      "366-th epoch val loss 1.0961000393305014\n",
      "367-th epoch train loss 0.9810732278414783\n",
      "367-th epoch val loss 1.096164525533992\n",
      "368-th epoch train loss 0.9811397166212417\n",
      "368-th epoch val loss 1.0962285377931815\n",
      "369-th epoch train loss 0.9812056825932496\n",
      "369-th epoch val loss 1.096292066851128\n",
      "370-th epoch train loss 0.9812711178098774\n",
      "370-th epoch val loss 1.0963551041147126\n",
      "371-th epoch train loss 0.9813360149498997\n",
      "371-th epoch val loss 1.0964176416281015\n",
      "372-th epoch train loss 0.9814003672930692\n",
      "372-th epoch val loss 1.0964796720471344\n",
      "373-th epoch train loss 0.9814641686955823\n",
      "373-th epoch val loss 1.0965411886145964\n",
      "374-th epoch train loss 0.9815274135663928\n",
      "374-th epoch val loss 1.0966021851363454\n",
      "375-th epoch train loss 0.98159009684436\n",
      "375-th epoch val loss 1.0966626559582764\n",
      "376-th epoch train loss 0.9816522139761896\n",
      "376-th epoch val loss 1.0967225959440834\n",
      "377-th epoch train loss 0.981713760895152\n",
      "377-th epoch val loss 1.096782000453799\n",
      "378-th epoch train loss 0.9817747340005473\n",
      "378-th epoch val loss 1.0968408653230897\n",
      "379-th epoch train loss 0.9818351301378913\n",
      "379-th epoch val loss 1.0968991868432694\n",
      "380-th epoch train loss 0.9818949465798056\n",
      "380-th epoch val loss 1.0969569617420232\n",
      "381-th epoch train loss 0.9819541810075793\n",
      "381-th epoch val loss 1.097014187164808\n",
      "382-th epoch train loss 0.9820128314933878\n",
      "382-th epoch val loss 1.097070860656909\n",
      "383-th epoch train loss 0.9820708964831444\n",
      "383-th epoch val loss 1.0971269801461336\n",
      "384-th epoch train loss 0.9821283747799592\n",
      "384-th epoch val loss 1.0971825439261202\n",
      "385-th epoch train loss 0.9821852655281935\n",
      "385-th epoch val loss 1.0972375506402363\n",
      "386-th epoch train loss 0.9822415681980865\n",
      "386-th epoch val loss 1.0972919992660597\n",
      "387-th epoch train loss 0.9822972825709302\n",
      "387-th epoch val loss 1.097345889100408\n",
      "388-th epoch train loss 0.9823524087247826\n",
      "388-th epoch val loss 1.0973992197449078\n",
      "389-th epoch train loss 0.9824069470206955\n",
      "389-th epoch val loss 1.0974519910920832\n",
      "390-th epoch train loss 0.9824608980894445\n",
      "390-th epoch val loss 1.097504203311948\n",
      "391-th epoch train loss 0.9825142628187366\n",
      "391-th epoch val loss 1.0975558568390786\n",
      "392-th epoch train loss 0.9825670423408902\n",
      "392-th epoch val loss 1.097606952360162\n",
      "393-th epoch train loss 0.9826192380209616\n",
      "393-th epoch val loss 1.0976574908019914\n",
      "394-th epoch train loss 0.9826708514453126\n",
      "394-th epoch val loss 1.0977074733199068\n",
      "395-th epoch train loss 0.9827218844105914\n",
      "395-th epoch val loss 1.097756901286651\n",
      "396-th epoch train loss 0.98277233891313\n",
      "396-th epoch val loss 1.0978057762816456\n",
      "397-th epoch train loss 0.9828222171387289\n",
      "397-th epoch val loss 1.0978541000806517\n",
      "398-th epoch train loss 0.982871521452825\n",
      "398-th epoch val loss 1.0979018746458225\n",
      "399-th epoch train loss 0.9829202543910236\n",
      "399-th epoch val loss 1.0979491021161178\n",
      "400-th epoch train loss 0.9829684186499917\n",
      "400-th epoch val loss 1.097995784798085\n",
      "401-th epoch train loss 0.9830160170786878\n",
      "401-th epoch val loss 1.0980419251569746\n",
      "402-th epoch train loss 0.9830630526699268\n",
      "402-th epoch val loss 1.0980875258081972\n",
      "403-th epoch train loss 0.9831095285522661\n",
      "403-th epoch val loss 1.0981325895091014\n",
      "404-th epoch train loss 0.9831554479821969\n",
      "404-th epoch val loss 1.0981771191510579\n",
      "405-th epoch train loss 0.9832008143366393\n",
      "405-th epoch val loss 1.0982211177518533\n",
      "406-th epoch train loss 0.9832456311057226\n",
      "406-th epoch val loss 1.098264588448365\n",
      "407-th epoch train loss 0.9832899018858465\n",
      "407-th epoch val loss 1.0983075344895268\n",
      "408-th epoch train loss 0.9833336303730102\n",
      "408-th epoch val loss 1.0983499592295547\n",
      "409-th epoch train loss 0.983376820356403\n",
      "409-th epoch val loss 1.0983918661214445\n",
      "410-th epoch train loss 0.9834194757122442\n",
      "410-th epoch val loss 1.0984332587107155\n",
      "411-th epoch train loss 0.9834616003978699\n",
      "411-th epoch val loss 1.0984741406294043\n",
      "412-th epoch train loss 0.9835031984460471\n",
      "412-th epoch val loss 1.098514515590287\n",
      "413-th epoch train loss 0.9835442739595214\n",
      "413-th epoch val loss 1.0985543873813386\n",
      "414-th epoch train loss 0.9835848311057772\n",
      "414-th epoch val loss 1.0985937598604036\n",
      "415-th epoch train loss 0.9836248741120123\n",
      "415-th epoch val loss 1.0986326369500845\n",
      "416-th epoch train loss 0.9836644072603158\n",
      "416-th epoch val loss 1.0986710226328347\n",
      "417-th epoch train loss 0.9837034348830388\n",
      "417-th epoch val loss 1.0987089209462468\n",
      "418-th epoch train loss 0.9837419613583597\n",
      "418-th epoch val loss 1.0987463359785319\n",
      "419-th epoch train loss 0.9837799911060299\n",
      "419-th epoch val loss 1.098783271864191\n",
      "420-th epoch train loss 0.9838175285832959\n",
      "420-th epoch val loss 1.098819732779852\n",
      "421-th epoch train loss 0.9838545782809922\n",
      "421-th epoch val loss 1.0988557229402887\n",
      "422-th epoch train loss 0.9838911447197998\n",
      "422-th epoch val loss 1.0988912465946017\n",
      "423-th epoch train loss 0.9839272324466614\n",
      "423-th epoch val loss 1.0989263080225569\n",
      "424-th epoch train loss 0.9839628460313515\n",
      "424-th epoch val loss 1.0989609115310857\n",
      "425-th epoch train loss 0.9839979900631921\n",
      "425-th epoch val loss 1.0989950614509247\n",
      "426-th epoch train loss 0.9840326691479121\n",
      "426-th epoch val loss 1.0990287621334067\n",
      "427-th epoch train loss 0.9840668879046449\n",
      "427-th epoch val loss 1.099062017947386\n",
      "428-th epoch train loss 0.9841006509630582\n",
      "428-th epoch val loss 1.0990948332763009\n",
      "429-th epoch train loss 0.9841339629606065\n",
      "429-th epoch val loss 1.09912721251536\n",
      "430-th epoch train loss 0.9841668285399167\n",
      "430-th epoch val loss 1.099159160068858\n",
      "431-th epoch train loss 0.9841992523462822\n",
      "431-th epoch val loss 1.0991906803476095\n",
      "432-th epoch train loss 0.9842312390252798\n",
      "432-th epoch val loss 1.0992217777664963\n",
      "433-th epoch train loss 0.9842627932204889\n",
      "433-th epoch val loss 1.099252456742131\n",
      "434-th epoch train loss 0.984293919571326\n",
      "434-th epoch val loss 1.0992827216906205\n",
      "435-th epoch train loss 0.9843246227109721\n",
      "435-th epoch val loss 1.099312577025442\n",
      "436-th epoch train loss 0.9843549072644082\n",
      "436-th epoch val loss 1.0993420271554122\n",
      "437-th epoch train loss 0.9843847778465388\n",
      "437-th epoch val loss 1.099371076482756\n",
      "438-th epoch train loss 0.9844142390604083\n",
      "438-th epoch val loss 1.0993997294012636\n",
      "439-th epoch train loss 0.9844432954955116\n",
      "439-th epoch val loss 1.0994279902945445\n",
      "440-th epoch train loss 0.9844719517261797\n",
      "440-th epoch val loss 1.099455863534359\n",
      "441-th epoch train loss 0.9845002123100567\n",
      "441-th epoch val loss 1.0994833534790376\n",
      "442-th epoch train loss 0.9845280817866477\n",
      "442-th epoch val loss 1.0995104644719798\n",
      "443-th epoch train loss 0.9845555646759511\n",
      "443-th epoch val loss 1.0995372008402282\n",
      "444-th epoch train loss 0.9845826654771525\n",
      "444-th epoch val loss 1.099563566893116\n",
      "445-th epoch train loss 0.9846093886674037\n",
      "445-th epoch val loss 1.0995895669209885\n",
      "446-th epoch train loss 0.9846357387006531\n",
      "446-th epoch val loss 1.0996152051939916\n",
      "447-th epoch train loss 0.9846617200065538\n",
      "447-th epoch val loss 1.0996404859609221\n",
      "448-th epoch train loss 0.9846873369894287\n",
      "448-th epoch val loss 1.0996654134481514\n",
      "449-th epoch train loss 0.9847125940272962\n",
      "449-th epoch val loss 1.0996899918585998\n",
      "450-th epoch train loss 0.9847374954709548\n",
      "450-th epoch val loss 1.0997142253707746\n",
      "451-th epoch train loss 0.9847620456431234\n",
      "451-th epoch val loss 1.0997381181378656\n",
      "452-th epoch train loss 0.9847862488376335\n",
      "452-th epoch val loss 1.0997616742868916\n",
      "453-th epoch train loss 0.9848101093186733\n",
      "453-th epoch val loss 1.0997848979178992\n",
      "454-th epoch train loss 0.9848336313200833\n",
      "454-th epoch val loss 1.0998077931032173\n",
      "455-th epoch train loss 0.9848568190446976\n",
      "455-th epoch val loss 1.0998303638867521\n",
      "456-th epoch train loss 0.9848796766637303\n",
      "456-th epoch val loss 1.099852614283334\n",
      "457-th epoch train loss 0.984902208316207\n",
      "457-th epoch val loss 1.0998745482781056\n",
      "458-th epoch train loss 0.9849244181084357\n",
      "458-th epoch val loss 1.0998961698259528\n",
      "459-th epoch train loss 0.9849463101135241\n",
      "459-th epoch val loss 1.0999174828509837\n",
      "460-th epoch train loss 0.984967888370927\n",
      "460-th epoch val loss 1.0999384912460322\n",
      "461-th epoch train loss 0.9849891568860383\n",
      "461-th epoch val loss 1.0999591988722155\n",
      "462-th epoch train loss 0.9850101196298149\n",
      "462-th epoch val loss 1.0999796095585181\n",
      "463-th epoch train loss 0.9850307805384357\n",
      "463-th epoch val loss 1.099999727101413\n",
      "464-th epoch train loss 0.9850511435129921\n",
      "464-th epoch val loss 1.1000195552645162\n",
      "465-th epoch train loss 0.9850712124192114\n",
      "465-th epoch val loss 1.100039097778273\n",
      "466-th epoch train loss 0.9850909910872112\n",
      "466-th epoch val loss 1.1000583583396784\n",
      "467-th epoch train loss 0.9851104833112799\n",
      "467-th epoch val loss 1.1000773406120203\n",
      "468-th epoch train loss 0.9851296928496842\n",
      "468-th epoch val loss 1.100096048224655\n",
      "469-th epoch train loss 0.9851486234245098\n",
      "469-th epoch val loss 1.1001144847728106\n",
      "470-th epoch train loss 0.9851672787215162\n",
      "470-th epoch val loss 1.1001326538174137\n",
      "471-th epoch train loss 0.9851856623900269\n",
      "471-th epoch val loss 1.1001505588849418\n",
      "472-th epoch train loss 0.9852037780428351\n",
      "472-th epoch val loss 1.1001682034672995\n",
      "473-th epoch train loss 0.9852216292561334\n",
      "473-th epoch val loss 1.100185591021716\n",
      "474-th epoch train loss 0.9852392195694658\n",
      "474-th epoch val loss 1.1002027249706656\n",
      "475-th epoch train loss 0.9852565524857007\n",
      "475-th epoch val loss 1.1002196087018108\n",
      "476-th epoch train loss 0.9852736314710179\n",
      "476-th epoch val loss 1.1002362455679588\n",
      "477-th epoch train loss 0.9852904599549204\n",
      "477-th epoch val loss 1.1002526388870442\n",
      "478-th epoch train loss 0.9853070413302599\n",
      "478-th epoch val loss 1.1002687919421228\n",
      "479-th epoch train loss 0.9853233789532789\n",
      "479-th epoch val loss 1.10028470798139\n",
      "480-th epoch train loss 0.9853394761436715\n",
      "480-th epoch val loss 1.1003003902182082\n",
      "481-th epoch train loss 0.9853553361846544\n",
      "481-th epoch val loss 1.1003158418311547\n",
      "482-th epoch train loss 0.9853709623230587\n",
      "482-th epoch val loss 1.1003310659640821\n",
      "483-th epoch train loss 0.9853863577694274\n",
      "483-th epoch val loss 1.1003460657261925\n",
      "484-th epoch train loss 0.9854015256981354\n",
      "484-th epoch val loss 1.100360844192129\n",
      "485-th epoch train loss 0.985416469247513\n",
      "485-th epoch val loss 1.1003754044020746\n",
      "486-th epoch train loss 0.9854311915199852\n",
      "486-th epoch val loss 1.1003897493618642\n",
      "487-th epoch train loss 0.9854456955822224\n",
      "487-th epoch val loss 1.1004038820431128\n",
      "488-th epoch train loss 0.9854599844653018\n",
      "488-th epoch val loss 1.1004178053833498\n",
      "489-th epoch train loss 0.9854740611648749\n",
      "489-th epoch val loss 1.100431522286164\n",
      "490-th epoch train loss 0.9854879286413504\n",
      "490-th epoch val loss 1.100445035621362\n",
      "491-th epoch train loss 0.9855015898200798\n",
      "491-th epoch val loss 1.1004583482251311\n",
      "492-th epoch train loss 0.9855150475915557\n",
      "492-th epoch val loss 1.1004714629002121\n",
      "493-th epoch train loss 0.9855283048116188\n",
      "493-th epoch val loss 1.1004843824160864\n",
      "494-th epoch train loss 0.9855413643016647\n",
      "494-th epoch val loss 1.10049710950916\n",
      "495-th epoch train loss 0.9855542288488695\n",
      "495-th epoch val loss 1.1005096468829652\n",
      "496-th epoch train loss 0.9855669012064094\n",
      "496-th epoch val loss 1.1005219972083606\n",
      "497-th epoch train loss 0.985579384093697\n",
      "497-th epoch val loss 1.1005341631237466\n",
      "498-th epoch train loss 0.9855916801966164\n",
      "498-th epoch val loss 1.1005461472352764\n",
      "499-th epoch train loss 0.985603792167768\n",
      "499-th epoch val loss 1.100557952117084\n",
      "500-th epoch train loss 0.9856157226267137\n",
      "500-th epoch val loss 1.1005695803115079\n",
      "501-th epoch train loss 0.9856274741602317\n",
      "501-th epoch val loss 1.1005810343293243\n",
      "502-th epoch train loss 0.9856390493225725\n",
      "502-th epoch val loss 1.1005923166499867\n",
      "503-th epoch train loss 0.9856504506357198\n",
      "503-th epoch val loss 1.1006034297218639\n",
      "504-th epoch train loss 0.9856616805896534\n",
      "504-th epoch val loss 1.100614375962487\n",
      "505-th epoch train loss 0.9856727416426203\n",
      "505-th epoch val loss 1.1006251577588004\n",
      "506-th epoch train loss 0.9856836362214019\n",
      "506-th epoch val loss 1.100635777467412\n",
      "507-th epoch train loss 0.9856943667215903\n",
      "507-th epoch val loss 1.1006462374148496\n",
      "508-th epoch train loss 0.985704935507862\n",
      "508-th epoch val loss 1.1006565398978183\n",
      "509-th epoch train loss 0.9857153449142593\n",
      "509-th epoch val loss 1.1006666871834634\n",
      "510-th epoch train loss 0.9857255972444674\n",
      "510-th epoch val loss 1.1006766815096312\n",
      "511-th epoch train loss 0.9857356947720999\n",
      "511-th epoch val loss 1.1006865250851383\n",
      "512-th epoch train loss 0.9857456397409814\n",
      "512-th epoch val loss 1.1006962200900352\n",
      "513-th epoch train loss 0.985755434365431\n",
      "513-th epoch val loss 1.1007057686758757\n",
      "514-th epoch train loss 0.9857650808305517\n",
      "514-th epoch val loss 1.10071517296599\n",
      "515-th epoch train loss 0.9857745812925169\n",
      "515-th epoch val loss 1.1007244350557537\n",
      "516-th epoch train loss 0.985783937878857\n",
      "516-th epoch val loss 1.1007335570128605\n",
      "517-th epoch train loss 0.9857931526887489\n",
      "517-th epoch val loss 1.1007425408775966\n",
      "518-th epoch train loss 0.9858022277933077\n",
      "518-th epoch val loss 1.100751388663115\n",
      "519-th epoch train loss 0.985811165235873\n",
      "519-th epoch val loss 1.1007601023557112\n",
      "520-th epoch train loss 0.9858199670323007\n",
      "520-th epoch val loss 1.1007686839150945\n",
      "521-th epoch train loss 0.9858286351712505\n",
      "521-th epoch val loss 1.1007771352746685\n",
      "522-th epoch train loss 0.9858371716144787\n",
      "522-th epoch val loss 1.1007854583418024\n",
      "523-th epoch train loss 0.9858455782971246\n",
      "523-th epoch val loss 1.100793654998109\n",
      "524-th epoch train loss 0.9858538571279998\n",
      "524-th epoch val loss 1.1008017270997175\n",
      "525-th epoch train loss 0.9858620099898786\n",
      "525-th epoch val loss 1.100809676477551\n",
      "526-th epoch train loss 0.9858700387397823\n",
      "526-th epoch val loss 1.1008175049375992\n",
      "527-th epoch train loss 0.9858779452092689\n",
      "527-th epoch val loss 1.1008252142611925\n",
      "528-th epoch train loss 0.9858857312047178\n",
      "528-th epoch val loss 1.1008328062052757\n",
      "529-th epoch train loss 0.9858933985076143\n",
      "529-th epoch val loss 1.100840282502679\n",
      "530-th epoch train loss 0.9859009488748357\n",
      "530-th epoch val loss 1.1008476448623925\n",
      "531-th epoch train loss 0.9859083840389306\n",
      "531-th epoch val loss 1.1008548949698334\n",
      "532-th epoch train loss 0.9859157057084048\n",
      "532-th epoch val loss 1.1008620344871183\n",
      "533-th epoch train loss 0.9859229155679979\n",
      "533-th epoch val loss 1.1008690650533302\n",
      "534-th epoch train loss 0.9859300152789648\n",
      "534-th epoch val loss 1.1008759882847863\n",
      "535-th epoch train loss 0.9859370064793506\n",
      "535-th epoch val loss 1.1008828057753033\n",
      "536-th epoch train loss 0.9859438907842688\n",
      "536-th epoch val loss 1.1008895190964632\n",
      "537-th epoch train loss 0.985950669786174\n",
      "537-th epoch val loss 1.100896129797876\n",
      "538-th epoch train loss 0.9859573450551362\n",
      "538-th epoch val loss 1.100902639407441\n",
      "539-th epoch train loss 0.9859639181391099\n",
      "539-th epoch val loss 1.100909049431608\n",
      "540-th epoch train loss 0.9859703905642038\n",
      "540-th epoch val loss 1.1009153613556355\n",
      "541-th epoch train loss 0.9859767638349496\n",
      "541-th epoch val loss 1.100921576643846\n",
      "542-th epoch train loss 0.9859830394345653\n",
      "542-th epoch val loss 1.1009276967398849\n",
      "543-th epoch train loss 0.9859892188252197\n",
      "543-th epoch val loss 1.1009337230669705\n",
      "544-th epoch train loss 0.985995303448295\n",
      "544-th epoch val loss 1.100939657028148\n",
      "545-th epoch train loss 0.9860012947246451\n",
      "545-th epoch val loss 1.1009455000065387\n",
      "546-th epoch train loss 0.986007194054854\n",
      "546-th epoch val loss 1.100951253365588\n",
      "547-th epoch train loss 0.9860130028194914\n",
      "547-th epoch val loss 1.1009569184493126\n",
      "548-th epoch train loss 0.9860187223793653\n",
      "548-th epoch val loss 1.100962496582542\n",
      "549-th epoch train loss 0.9860243540757755\n",
      "549-th epoch val loss 1.1009679890711646\n",
      "550-th epoch train loss 0.98602989923076\n",
      "550-th epoch val loss 1.1009733972023654\n",
      "551-th epoch train loss 0.9860353591473452\n",
      "551-th epoch val loss 1.1009787222448661\n",
      "552-th epoch train loss 0.9860407351097882\n",
      "552-th epoch val loss 1.1009839654491576\n",
      "553-th epoch train loss 0.9860460283838215\n",
      "553-th epoch val loss 1.1009891280477404\n",
      "554-th epoch train loss 0.9860512402168925\n",
      "554-th epoch val loss 1.1009942112553512\n",
      "555-th epoch train loss 0.9860563718384027\n",
      "555-th epoch val loss 1.1009992162691966\n",
      "556-th epoch train loss 0.9860614244599428\n",
      "556-th epoch val loss 1.101004144269179\n",
      "557-th epoch train loss 0.9860663992755271\n",
      "557-th epoch val loss 1.1010089964181218\n",
      "558-th epoch train loss 0.9860712974618262\n",
      "558-th epoch val loss 1.1010137738619976\n",
      "559-th epoch train loss 0.986076120178393\n",
      "559-th epoch val loss 1.1010184777301437\n",
      "560-th epoch train loss 0.9860808685678943\n",
      "560-th epoch val loss 1.1010231091354863\n",
      "561-th epoch train loss 0.9860855437563318\n",
      "561-th epoch val loss 1.101027669174754\n",
      "562-th epoch train loss 0.9860901468532668\n",
      "562-th epoch val loss 1.1010321589286967\n",
      "563-th epoch train loss 0.9860946789520391\n",
      "563-th epoch val loss 1.1010365794622963\n",
      "564-th epoch train loss 0.9860991411299866\n",
      "564-th epoch val loss 1.1010409318249772\n",
      "565-th epoch train loss 0.9861035344486587\n",
      "565-th epoch val loss 1.1010452170508152\n",
      "566-th epoch train loss 0.9861078599540315\n",
      "566-th epoch val loss 1.101049436158746\n",
      "567-th epoch train loss 0.9861121186767178\n",
      "567-th epoch val loss 1.101053590152766\n",
      "568-th epoch train loss 0.9861163116321767\n",
      "568-th epoch val loss 1.101057680022137\n",
      "569-th epoch train loss 0.9861204398209191\n",
      "569-th epoch val loss 1.1010617067415847\n",
      "570-th epoch train loss 0.9861245042287136\n",
      "570-th epoch val loss 1.101065671271497\n",
      "571-th epoch train loss 0.9861285058267859\n",
      "571-th epoch val loss 1.1010695745581203\n",
      "572-th epoch train loss 0.986132445572022\n",
      "572-th epoch val loss 1.101073417533751\n",
      "573-th epoch train loss 0.9861363244071609\n",
      "573-th epoch val loss 1.1010772011169287\n",
      "574-th epoch train loss 0.9861401432609954\n",
      "574-th epoch val loss 1.1010809262126233\n",
      "575-th epoch train loss 0.9861439030485601\n",
      "575-th epoch val loss 1.1010845937124238\n",
      "576-th epoch train loss 0.9861476046713263\n",
      "576-th epoch val loss 1.101088204494724\n",
      "577-th epoch train loss 0.9861512490173862\n",
      "577-th epoch val loss 1.1010917594249008\n",
      "578-th epoch train loss 0.986154836961643\n",
      "578-th epoch val loss 1.1010952593554995\n",
      "579-th epoch train loss 0.9861583693659932\n",
      "579-th epoch val loss 1.1010987051264103\n",
      "580-th epoch train loss 0.9861618470795096\n",
      "580-th epoch val loss 1.1011020975650452\n",
      "581-th epoch train loss 0.9861652709386186\n",
      "581-th epoch val loss 1.1011054374865104\n",
      "582-th epoch train loss 0.9861686417672815\n",
      "582-th epoch val loss 1.101108725693782\n",
      "583-th epoch train loss 0.9861719603771668\n",
      "583-th epoch val loss 1.1011119629778732\n",
      "584-th epoch train loss 0.9861752275678247\n",
      "584-th epoch val loss 1.1011151501180036\n",
      "585-th epoch train loss 0.9861784441268596\n",
      "585-th epoch val loss 1.101118287881767\n",
      "586-th epoch train loss 0.9861816108300967\n",
      "586-th epoch val loss 1.1011213770252906\n",
      "587-th epoch train loss 0.9861847284417505\n",
      "587-th epoch val loss 1.1011244182934021\n",
      "588-th epoch train loss 0.9861877977145908\n",
      "588-th epoch val loss 1.1011274124197885\n",
      "589-th epoch train loss 0.986190819390103\n",
      "589-th epoch val loss 1.1011303601271525\n",
      "590-th epoch train loss 0.9861937941986505\n",
      "590-th epoch val loss 1.1011332621273697\n",
      "591-th epoch train loss 0.9861967228596341\n",
      "591-th epoch val loss 1.1011361191216433\n",
      "592-th epoch train loss 0.9861996060816477\n",
      "592-th epoch val loss 1.1011389318006566\n",
      "593-th epoch train loss 0.9862024445626333\n",
      "593-th epoch val loss 1.1011417008447195\n",
      "594-th epoch train loss 0.9862052389900349\n",
      "594-th epoch val loss 1.1011444269239248\n",
      "595-th epoch train loss 0.9862079900409474\n",
      "595-th epoch val loss 1.1011471106982862\n",
      "596-th epoch train loss 0.9862106983822679\n",
      "596-th epoch val loss 1.1011497528178877\n",
      "597-th epoch train loss 0.9862133646708406\n",
      "597-th epoch val loss 1.1011523539230264\n",
      "598-th epoch train loss 0.9862159895536037\n",
      "598-th epoch val loss 1.1011549146443516\n",
      "599-th epoch train loss 0.9862185736677312\n",
      "599-th epoch val loss 1.1011574356030043\n",
      "600-th epoch train loss 0.9862211176407751\n",
      "600-th epoch val loss 1.1011599174107567\n",
      "601-th epoch train loss 0.9862236220908053\n",
      "601-th epoch val loss 1.101162360670145\n",
      "602-th epoch train loss 0.9862260876265467\n",
      "602-th epoch val loss 1.1011647659746056\n",
      "603-th epoch train loss 0.9862285148475137\n",
      "603-th epoch val loss 1.101167133908604\n",
      "604-th epoch train loss 0.9862309043441472\n",
      "604-th epoch val loss 1.101169465047768\n",
      "605-th epoch train loss 0.986233256697944\n",
      "605-th epoch val loss 1.1011717599590145\n",
      "606-th epoch train loss 0.9862355724815901\n",
      "606-th epoch val loss 1.1011740192006785\n",
      "607-th epoch train loss 0.9862378522590869\n",
      "607-th epoch val loss 1.1011762433226358\n",
      "608-th epoch train loss 0.9862400965858796\n",
      "608-th epoch val loss 1.1011784328664276\n",
      "609-th epoch train loss 0.9862423060089821\n",
      "609-th epoch val loss 1.1011805883653816\n",
      "610-th epoch train loss 0.9862444810671013\n",
      "610-th epoch val loss 1.1011827103447345\n",
      "611-th epoch train loss 0.986246622290758\n",
      "611-th epoch val loss 1.1011847993217476\n",
      "612-th epoch train loss 0.9862487302024079\n",
      "612-th epoch val loss 1.101186855805825\n",
      "613-th epoch train loss 0.9862508053165607\n",
      "613-th epoch val loss 1.1011888802986307\n",
      "614-th epoch train loss 0.9862528481398964\n",
      "614-th epoch val loss 1.1011908732941995\n",
      "615-th epoch train loss 0.9862548591713792\n",
      "615-th epoch val loss 1.1011928352790497\n",
      "616-th epoch train loss 0.986256838902376\n",
      "616-th epoch val loss 1.101194766732298\n",
      "617-th epoch train loss 0.9862587878167639\n",
      "617-th epoch val loss 1.101196668125762\n",
      "618-th epoch train loss 0.986260706391043\n",
      "618-th epoch val loss 1.1011985399240727\n",
      "619-th epoch train loss 0.9862625950944461\n",
      "619-th epoch val loss 1.10120038258478\n",
      "620-th epoch train loss 0.986264454389046\n",
      "620-th epoch val loss 1.1012021965584573\n",
      "621-th epoch train loss 0.9862662847298599\n",
      "621-th epoch val loss 1.1012039822888033\n",
      "622-th epoch train loss 0.9862680865649578\n",
      "622-th epoch val loss 1.1012057402127458\n",
      "623-th epoch train loss 0.9862698603355615\n",
      "623-th epoch val loss 1.1012074707605404\n",
      "624-th epoch train loss 0.986271606476151\n",
      "624-th epoch val loss 1.1012091743558723\n",
      "625-th epoch train loss 0.986273325414561\n",
      "625-th epoch val loss 1.101210851415951\n",
      "626-th epoch train loss 0.9862750175720814\n",
      "626-th epoch val loss 1.1012125023516086\n",
      "627-th epoch train loss 0.9862766833635553\n",
      "627-th epoch val loss 1.101214127567394\n",
      "628-th epoch train loss 0.986278323197475\n",
      "628-th epoch val loss 1.1012157274616665\n",
      "629-th epoch train loss 0.9862799374760765\n",
      "629-th epoch val loss 1.1012173024266882\n",
      "630-th epoch train loss 0.9862815265954331\n",
      "630-th epoch val loss 1.1012188528487157\n",
      "631-th epoch train loss 0.986283090945548\n",
      "631-th epoch val loss 1.1012203791080888\n",
      "632-th epoch train loss 0.9862846309104448\n",
      "632-th epoch val loss 1.1012218815793196\n",
      "633-th epoch train loss 0.9862861468682576\n",
      "633-th epoch val loss 1.10122336063118\n",
      "634-th epoch train loss 0.9862876391913181\n",
      "634-th epoch val loss 1.1012248166267866\n",
      "635-th epoch train loss 0.9862891082462446\n",
      "635-th epoch val loss 1.101226249923687\n",
      "636-th epoch train loss 0.9862905543940252\n",
      "636-th epoch val loss 1.1012276608739402\n",
      "637-th epoch train loss 0.9862919779901057\n",
      "637-th epoch val loss 1.1012290498242037\n",
      "638-th epoch train loss 0.9862933793844697\n",
      "638-th epoch val loss 1.1012304171158112\n",
      "639-th epoch train loss 0.9862947589217232\n",
      "639-th epoch val loss 1.1012317630848536\n",
      "640-th epoch train loss 0.9862961169411746\n",
      "640-th epoch val loss 1.1012330880622578\n",
      "641-th epoch train loss 0.9862974537769144\n",
      "641-th epoch val loss 1.1012343923738648\n",
      "642-th epoch train loss 0.9862987697578934\n",
      "642-th epoch val loss 1.101235676340506\n",
      "643-th epoch train loss 0.9863000652080021\n",
      "643-th epoch val loss 1.1012369402780775\n",
      "644-th epoch train loss 0.9863013404461451\n",
      "644-th epoch val loss 1.1012381844976171\n",
      "645-th epoch train loss 0.9863025957863176\n",
      "645-th epoch val loss 1.1012394093053761\n",
      "646-th epoch train loss 0.986303831537679\n",
      "646-th epoch val loss 1.1012406150028913\n",
      "647-th epoch train loss 0.9863050480046264\n",
      "647-th epoch val loss 1.1012418018870556\n",
      "648-th epoch train loss 0.9863062454868665\n",
      "648-th epoch val loss 1.1012429702501922\n",
      "649-th epoch train loss 0.986307424279487\n",
      "649-th epoch val loss 1.1012441203801182\n",
      "650-th epoch train loss 0.9863085846730255\n",
      "650-th epoch val loss 1.1012452525602157\n",
      "651-th epoch train loss 0.9863097269535408\n",
      "651-th epoch val loss 1.1012463670694999\n",
      "652-th epoch train loss 0.9863108514026773\n",
      "652-th epoch val loss 1.1012474641826837\n",
      "653-th epoch train loss 0.9863119582977353\n",
      "653-th epoch val loss 1.1012485441702426\n",
      "654-th epoch train loss 0.9863130479117346\n",
      "654-th epoch val loss 1.101249607298479\n",
      "655-th epoch train loss 0.986314120513482\n",
      "655-th epoch val loss 1.1012506538295892\n",
      "656-th epoch train loss 0.9863151763676331\n",
      "656-th epoch val loss 1.1012516840217195\n",
      "657-th epoch train loss 0.9863162157347554\n",
      "657-th epoch val loss 1.101252698129033\n",
      "658-th epoch train loss 0.9863172388713929\n",
      "658-th epoch val loss 1.101253696401766\n",
      "659-th epoch train loss 0.9863182460301245\n",
      "659-th epoch val loss 1.1012546790862943\n",
      "660-th epoch train loss 0.9863192374596266\n",
      "660-th epoch val loss 1.1012556464251817\n",
      "661-th epoch train loss 0.9863202134047317\n",
      "661-th epoch val loss 1.1012565986572498\n",
      "662-th epoch train loss 0.9863211741064856\n",
      "662-th epoch val loss 1.1012575360176242\n",
      "663-th epoch train loss 0.9863221198022072\n",
      "663-th epoch val loss 1.1012584587377976\n",
      "664-th epoch train loss 0.9863230507255446\n",
      "664-th epoch val loss 1.1012593670456834\n",
      "665-th epoch train loss 0.9863239671065315\n",
      "665-th epoch val loss 1.101260261165671\n",
      "666-th epoch train loss 0.9863248691716421\n",
      "666-th epoch val loss 1.1012611413186773\n",
      "667-th epoch train loss 0.9863257571438441\n",
      "667-th epoch val loss 1.1012620077222008\n",
      "668-th epoch train loss 0.986326631242655\n",
      "668-th epoch val loss 1.101262860590376\n",
      "669-th epoch train loss 0.9863274916841936\n",
      "669-th epoch val loss 1.1012637001340226\n",
      "670-th epoch train loss 0.9863283386812304\n",
      "670-th epoch val loss 1.101264526560695\n",
      "671-th epoch train loss 0.9863291724432416\n",
      "671-th epoch val loss 1.101265340074735\n",
      "672-th epoch train loss 0.9863299931764573\n",
      "672-th epoch val loss 1.1012661408773197\n",
      "673-th epoch train loss 0.9863308010839125\n",
      "673-th epoch val loss 1.1012669291665087\n",
      "674-th epoch train loss 0.9863315963654962\n",
      "674-th epoch val loss 1.101267705137295\n",
      "675-th epoch train loss 0.9863323792179975\n",
      "675-th epoch val loss 1.1012684689816485\n",
      "676-th epoch train loss 0.9863331498351564\n",
      "676-th epoch val loss 1.1012692208885644\n",
      "677-th epoch train loss 0.986333908407708\n",
      "677-th epoch val loss 1.101269961044109\n",
      "678-th epoch train loss 0.9863346551234292\n",
      "678-th epoch val loss 1.1012706896314621\n",
      "679-th epoch train loss 0.9863353901671853\n",
      "679-th epoch val loss 1.1012714068309646\n",
      "680-th epoch train loss 0.986336113720972\n",
      "680-th epoch val loss 1.1012721128201592\n",
      "681-th epoch train loss 0.9863368259639633\n",
      "681-th epoch val loss 1.1012728077738354\n",
      "682-th epoch train loss 0.9863375270725512\n",
      "682-th epoch val loss 1.1012734918640719\n",
      "683-th epoch train loss 0.986338217220391\n",
      "683-th epoch val loss 1.101274165260275\n",
      "684-th epoch train loss 0.9863388965784409\n",
      "684-th epoch val loss 1.1012748281292235\n",
      "685-th epoch train loss 0.9863395653150064\n",
      "685-th epoch val loss 1.1012754806351073\n",
      "686-th epoch train loss 0.9863402235957789\n",
      "686-th epoch val loss 1.1012761229395673\n",
      "687-th epoch train loss 0.9863408715838768\n",
      "687-th epoch val loss 1.1012767552017348\n",
      "688-th epoch train loss 0.9863415094398853\n",
      "688-th epoch val loss 1.1012773775782705\n",
      "689-th epoch train loss 0.986342137321894\n",
      "689-th epoch val loss 1.1012779902234016\n",
      "690-th epoch train loss 0.9863427553855373\n",
      "690-th epoch val loss 1.1012785932889586\n",
      "691-th epoch train loss 0.9863433637840312\n",
      "691-th epoch val loss 1.1012791869244158\n",
      "692-th epoch train loss 0.9863439626682099\n",
      "692-th epoch val loss 1.101279771276923\n",
      "693-th epoch train loss 0.9863445521865639\n",
      "693-th epoch val loss 1.1012803464913443\n",
      "694-th epoch train loss 0.9863451324852757\n",
      "694-th epoch val loss 1.1012809127102923\n",
      "695-th epoch train loss 0.9863457037082538\n",
      "695-th epoch val loss 1.1012814700741629\n",
      "696-th epoch train loss 0.9863462659971712\n",
      "696-th epoch val loss 1.1012820187211694\n",
      "697-th epoch train loss 0.9863468194914958\n",
      "697-th epoch val loss 1.1012825587873762\n",
      "698-th epoch train loss 0.9863473643285277\n",
      "698-th epoch val loss 1.1012830904067326\n",
      "699-th epoch train loss 0.9863479006434313\n",
      "699-th epoch val loss 1.1012836137111037\n",
      "700-th epoch train loss 0.986348428569267\n",
      "700-th epoch val loss 1.1012841288303046\n",
      "701-th epoch train loss 0.9863489482370276\n",
      "701-th epoch val loss 1.101284635892131\n",
      "702-th epoch train loss 0.986349459775666\n",
      "702-th epoch val loss 1.1012851350223907\n",
      "703-th epoch train loss 0.9863499633121279\n",
      "703-th epoch val loss 1.1012856263449335\n",
      "704-th epoch train loss 0.986350458971385\n",
      "704-th epoch val loss 1.1012861099816829\n",
      "705-th epoch train loss 0.9863509468764624\n",
      "705-th epoch val loss 1.1012865860526633\n",
      "706-th epoch train loss 0.9863514271484707\n",
      "706-th epoch val loss 1.1012870546760332\n",
      "707-th epoch train loss 0.9863518999066354\n",
      "707-th epoch val loss 1.101287515968111\n",
      "708-th epoch train loss 0.986352365268325\n",
      "708-th epoch val loss 1.1012879700434046\n",
      "709-th epoch train loss 0.9863528233490809\n",
      "709-th epoch val loss 1.1012884170146384\n",
      "710-th epoch train loss 0.9863532742626453\n",
      "710-th epoch val loss 1.1012888569927832\n",
      "711-th epoch train loss 0.9863537181209889\n",
      "711-th epoch val loss 1.10128929008708\n",
      "712-th epoch train loss 0.9863541550343378\n",
      "712-th epoch val loss 1.1012897164050695\n",
      "713-th epoch train loss 0.9863545851112008\n",
      "713-th epoch val loss 1.1012901360526173\n",
      "714-th epoch train loss 0.9863550084583977\n",
      "714-th epoch val loss 1.10129054913394\n",
      "715-th epoch train loss 0.9863554251810838\n",
      "715-th epoch val loss 1.1012909557516315\n",
      "716-th epoch train loss 0.9863558353827738\n",
      "716-th epoch val loss 1.101291356006685\n",
      "717-th epoch train loss 0.9863562391653719\n",
      "717-th epoch val loss 1.1012917499985235\n",
      "718-th epoch train loss 0.9863566366291935\n",
      "718-th epoch val loss 1.101292137825019\n",
      "719-th epoch train loss 0.9863570278729891\n",
      "719-th epoch val loss 1.101292519582517\n",
      "720-th epoch train loss 0.9863574129939722\n",
      "720-th epoch val loss 1.1012928953658636\n",
      "721-th epoch train loss 0.986357792087839\n",
      "721-th epoch val loss 1.1012932652684266\n",
      "722-th epoch train loss 0.9863581652487945\n",
      "722-th epoch val loss 1.101293629382116\n",
      "723-th epoch train loss 0.9863585325695755\n",
      "723-th epoch val loss 1.1012939877974124\n",
      "724-th epoch train loss 0.9863588941414719\n",
      "724-th epoch val loss 1.101294340603383\n",
      "725-th epoch train loss 0.9863592500543511\n",
      "725-th epoch val loss 1.1012946878877083\n",
      "726-th epoch train loss 0.9863596003966778\n",
      "726-th epoch val loss 1.1012950297367001\n",
      "727-th epoch train loss 0.9863599452555379\n",
      "727-th epoch val loss 1.1012953662353244\n",
      "728-th epoch train loss 0.9863602847166592\n",
      "728-th epoch val loss 1.1012956974672239\n",
      "729-th epoch train loss 0.9863606188644316\n",
      "729-th epoch val loss 1.1012960235147353\n",
      "730-th epoch train loss 0.9863609477819295\n",
      "730-th epoch val loss 1.1012963444589106\n",
      "731-th epoch train loss 0.9863612715509298\n",
      "731-th epoch val loss 1.101296660379539\n",
      "732-th epoch train loss 0.9863615902519355\n",
      "732-th epoch val loss 1.1012969713551644\n",
      "733-th epoch train loss 0.9863619039641923\n",
      "733-th epoch val loss 1.1012972774631047\n",
      "734-th epoch train loss 0.9863622127657088\n",
      "734-th epoch val loss 1.1012975787794719\n",
      "735-th epoch train loss 0.9863625167332786\n",
      "735-th epoch val loss 1.1012978753791915\n",
      "736-th epoch train loss 0.9863628159424942\n",
      "736-th epoch val loss 1.1012981673360178\n",
      "737-th epoch train loss 0.9863631104677701\n",
      "737-th epoch val loss 1.1012984547225562\n",
      "738-th epoch train loss 0.9863634003823594\n",
      "738-th epoch val loss 1.101298737610278\n",
      "739-th epoch train loss 0.9863636857583703\n",
      "739-th epoch val loss 1.1012990160695386\n",
      "740-th epoch train loss 0.9863639666667867\n",
      "740-th epoch val loss 1.1012992901695964\n",
      "741-th epoch train loss 0.9863642431774844\n",
      "741-th epoch val loss 1.1012995599786282\n",
      "742-th epoch train loss 0.9863645153592477\n",
      "742-th epoch val loss 1.101299825563746\n",
      "743-th epoch train loss 0.9863647832797878\n",
      "743-th epoch val loss 1.1013000869910152\n",
      "744-th epoch train loss 0.9863650470057577\n",
      "744-th epoch val loss 1.1013003443254692\n",
      "745-th epoch train loss 0.9863653066027702\n",
      "745-th epoch val loss 1.1013005976311259\n",
      "746-th epoch train loss 0.986365562135414\n",
      "746-th epoch val loss 1.1013008469710044\n",
      "747-th epoch train loss 0.9863658136672686\n",
      "747-th epoch val loss 1.1013010924071385\n",
      "748-th epoch train loss 0.9863660612609209\n",
      "748-th epoch val loss 1.101301334000595\n",
      "749-th epoch train loss 0.98636630497798\n",
      "749-th epoch val loss 1.101301571811486\n",
      "750-th epoch train loss 0.986366544879093\n",
      "750-th epoch val loss 1.1013018058989847\n",
      "751-th epoch train loss 0.9863667810239595\n",
      "751-th epoch val loss 1.1013020363213404\n",
      "752-th epoch train loss 0.9863670134713473\n",
      "752-th epoch val loss 1.101302263135893\n",
      "753-th epoch train loss 0.9863672422791051\n",
      "753-th epoch val loss 1.1013024863990857\n",
      "754-th epoch train loss 0.9863674675041791\n",
      "754-th epoch val loss 1.1013027061664822\n",
      "755-th epoch train loss 0.9863676892026246\n",
      "755-th epoch val loss 1.1013029224927753\n",
      "756-th epoch train loss 0.9863679074296225\n",
      "756-th epoch val loss 1.1013031354318057\n",
      "757-th epoch train loss 0.9863681222394901\n",
      "757-th epoch val loss 1.101303345036572\n",
      "758-th epoch train loss 0.9863683336856974\n",
      "758-th epoch val loss 1.101303551359245\n",
      "759-th epoch train loss 0.9863685418208782\n",
      "759-th epoch val loss 1.1013037544511803\n",
      "760-th epoch train loss 0.9863687466968437\n",
      "760-th epoch val loss 1.101303954362931\n",
      "761-th epoch train loss 0.9863689483645962\n",
      "761-th epoch val loss 1.1013041511442612\n",
      "762-th epoch train loss 0.9863691468743395\n",
      "762-th epoch val loss 1.1013043448441548\n",
      "763-th epoch train loss 0.9863693422754947\n",
      "763-th epoch val loss 1.1013045355108337\n",
      "764-th epoch train loss 0.9863695346167091\n",
      "764-th epoch val loss 1.1013047231917639\n",
      "765-th epoch train loss 0.9863697239458704\n",
      "765-th epoch val loss 1.1013049079336696\n",
      "766-th epoch train loss 0.9863699103101179\n",
      "766-th epoch val loss 1.1013050897825467\n",
      "767-th epoch train loss 0.9863700937558533\n",
      "767-th epoch val loss 1.10130526878367\n",
      "768-th epoch train loss 0.9863702743287533\n",
      "768-th epoch val loss 1.1013054449816067\n",
      "769-th epoch train loss 0.9863704520737818\n",
      "769-th epoch val loss 1.1013056184202303\n",
      "770-th epoch train loss 0.9863706270351972\n",
      "770-th epoch val loss 1.1013057891427245\n",
      "771-th epoch train loss 0.986370799256569\n",
      "771-th epoch val loss 1.101305957191601\n",
      "772-th epoch train loss 0.9863709687807831\n",
      "772-th epoch val loss 1.1013061226087044\n",
      "773-th epoch train loss 0.9863711356500569\n",
      "773-th epoch val loss 1.101306285435228\n",
      "774-th epoch train loss 0.9863712999059459\n",
      "774-th epoch val loss 1.1013064457117185\n",
      "775-th epoch train loss 0.9863714615893575\n",
      "775-th epoch val loss 1.101306603478091\n",
      "776-th epoch train loss 0.986371620740558\n",
      "776-th epoch val loss 1.1013067587736338\n",
      "777-th epoch train loss 0.9863717773991857\n",
      "777-th epoch val loss 1.1013069116370238\n",
      "778-th epoch train loss 0.9863719316042571\n",
      "778-th epoch val loss 1.1013070621063308\n",
      "779-th epoch train loss 0.9863720833941807\n",
      "779-th epoch val loss 1.1013072102190304\n",
      "780-th epoch train loss 0.9863722328067622\n",
      "780-th epoch val loss 1.101307356012012\n",
      "781-th epoch train loss 0.9863723798792181\n",
      "781-th epoch val loss 1.1013074995215881\n",
      "782-th epoch train loss 0.9863725246481824\n",
      "782-th epoch val loss 1.1013076407835047\n",
      "783-th epoch train loss 0.9863726671497147\n",
      "783-th epoch val loss 1.1013077798329463\n",
      "784-th epoch train loss 0.9863728074193122\n",
      "784-th epoch val loss 1.1013079167045488\n",
      "785-th epoch train loss 0.9863729454919169\n",
      "785-th epoch val loss 1.1013080514324056\n",
      "786-th epoch train loss 0.9863730814019228\n",
      "786-th epoch val loss 1.1013081840500776\n",
      "787-th epoch train loss 0.9863732151831879\n",
      "787-th epoch val loss 1.1013083145906006\n",
      "788-th epoch train loss 0.9863733468690384\n",
      "788-th epoch val loss 1.1013084430864928\n",
      "789-th epoch train loss 0.986373476492282\n",
      "789-th epoch val loss 1.1013085695697657\n",
      "790-th epoch train loss 0.9863736040852104\n",
      "790-th epoch val loss 1.1013086940719279\n",
      "791-th epoch train loss 0.9863737296796118\n",
      "791-th epoch val loss 1.1013088166239966\n",
      "792-th epoch train loss 0.9863738533067773\n",
      "792-th epoch val loss 1.1013089372565037\n",
      "793-th epoch train loss 0.9863739749975067\n",
      "793-th epoch val loss 1.101309055999502\n",
      "794-th epoch train loss 0.9863740947821201\n",
      "794-th epoch val loss 1.101309172882576\n",
      "795-th epoch train loss 0.986374212690462\n",
      "795-th epoch val loss 1.1013092879348463\n",
      "796-th epoch train loss 0.9863743287519107\n",
      "796-th epoch val loss 1.1013094011849789\n",
      "797-th epoch train loss 0.9863744429953836\n",
      "797-th epoch val loss 1.1013095126611905\n",
      "798-th epoch train loss 0.9863745554493472\n",
      "798-th epoch val loss 1.1013096223912562\n",
      "799-th epoch train loss 0.9863746661418223\n",
      "799-th epoch val loss 1.1013097304025172\n",
      "800-th epoch train loss 0.9863747751003898\n",
      "800-th epoch val loss 1.101309836721887\n",
      "801-th epoch train loss 0.9863748823522012\n",
      "801-th epoch val loss 1.101309941375857\n",
      "802-th epoch train loss 0.9863749879239817\n",
      "802-th epoch val loss 1.101310044390505\n",
      "803-th epoch train loss 0.9863750918420384\n",
      "803-th epoch val loss 1.1013101457915002\n",
      "804-th epoch train loss 0.9863751941322673\n",
      "804-th epoch val loss 1.1013102456041106\n",
      "805-th epoch train loss 0.986375294820159\n",
      "805-th epoch val loss 1.1013103438532086\n",
      "806-th epoch train loss 0.9863753939308054\n",
      "806-th epoch val loss 1.101310440563278\n",
      "807-th epoch train loss 0.9863754914889049\n",
      "807-th epoch val loss 1.101310535758418\n",
      "808-th epoch train loss 0.9863755875187702\n",
      "808-th epoch val loss 1.101310629462352\n",
      "809-th epoch train loss 0.9863756820443335\n",
      "809-th epoch val loss 1.101310721698432\n",
      "810-th epoch train loss 0.9863757750891518\n",
      "810-th epoch val loss 1.1013108124896434\n",
      "811-th epoch train loss 0.9863758666764141\n",
      "811-th epoch val loss 1.1013109018586136\n",
      "812-th epoch train loss 0.9863759568289465\n",
      "812-th epoch val loss 1.1013109898276148\n",
      "813-th epoch train loss 0.9863760455692172\n",
      "813-th epoch val loss 1.101311076418571\n",
      "814-th epoch train loss 0.9863761329193432\n",
      "814-th epoch val loss 1.1013111616530624\n",
      "815-th epoch train loss 0.9863762189010955\n",
      "815-th epoch val loss 1.1013112455523328\n",
      "816-th epoch train loss 0.9863763035359037\n",
      "816-th epoch val loss 1.1013113281372917\n",
      "817-th epoch train loss 0.9863763868448627\n",
      "817-th epoch val loss 1.1013114094285228\n",
      "818-th epoch train loss 0.9863764688487365\n",
      "818-th epoch val loss 1.1013114894462877\n",
      "819-th epoch train loss 0.9863765495679649\n",
      "819-th epoch val loss 1.1013115682105292\n",
      "820-th epoch train loss 0.9863766290226678\n",
      "820-th epoch val loss 1.10131164574088\n",
      "821-th epoch train loss 0.9863767072326487\n",
      "821-th epoch val loss 1.1013117220566646\n",
      "822-th epoch train loss 0.9863767842174029\n",
      "822-th epoch val loss 1.101311797176904\n",
      "823-th epoch train loss 0.9863768599961196\n",
      "823-th epoch val loss 1.1013118711203236\n",
      "824-th epoch train loss 0.9863769345876875\n",
      "824-th epoch val loss 1.1013119439053536\n",
      "825-th epoch train loss 0.9863770080106997\n",
      "825-th epoch val loss 1.1013120155501368\n",
      "826-th epoch train loss 0.9863770802834588\n",
      "826-th epoch val loss 1.101312086072532\n",
      "827-th epoch train loss 0.9863771514239806\n",
      "827-th epoch val loss 1.1013121554901184\n",
      "828-th epoch train loss 0.9863772214499984\n",
      "828-th epoch val loss 1.1013122238201993\n",
      "829-th epoch train loss 0.9863772903789686\n",
      "829-th epoch val loss 1.1013122910798083\n",
      "830-th epoch train loss 0.9863773582280735\n",
      "830-th epoch val loss 1.1013123572857115\n",
      "831-th epoch train loss 0.9863774250142278\n",
      "831-th epoch val loss 1.1013124224544126\n",
      "832-th epoch train loss 0.9863774907540789\n",
      "832-th epoch val loss 1.101312486602157\n",
      "833-th epoch train loss 0.9863775554640161\n",
      "833-th epoch val loss 1.1013125497449354\n",
      "834-th epoch train loss 0.9863776191601715\n",
      "834-th epoch val loss 1.1013126118984895\n",
      "835-th epoch train loss 0.9863776818584233\n",
      "835-th epoch val loss 1.1013126730783132\n",
      "836-th epoch train loss 0.9863777435744023\n",
      "836-th epoch val loss 1.1013127332996575\n",
      "837-th epoch train loss 0.9863778043234941\n",
      "837-th epoch val loss 1.1013127925775363\n",
      "838-th epoch train loss 0.9863778641208442\n",
      "838-th epoch val loss 1.1013128509267271\n",
      "839-th epoch train loss 0.9863779229813598\n",
      "839-th epoch val loss 1.1013129083617765\n",
      "840-th epoch train loss 0.986377980919716\n",
      "840-th epoch val loss 1.1013129648970037\n",
      "841-th epoch train loss 0.9863780379503566\n",
      "841-th epoch val loss 1.1013130205465018\n",
      "842-th epoch train loss 0.9863780940875002\n",
      "842-th epoch val loss 1.1013130753241465\n",
      "843-th epoch train loss 0.9863781493451429\n",
      "843-th epoch val loss 1.1013131292435931\n",
      "844-th epoch train loss 0.986378203737061\n",
      "844-th epoch val loss 1.1013131823182851\n",
      "845-th epoch train loss 0.9863782572768152\n",
      "845-th epoch val loss 1.101313234561454\n",
      "846-th epoch train loss 0.986378309977755\n",
      "846-th epoch val loss 1.1013132859861265\n",
      "847-th epoch train loss 0.986378361853019\n",
      "847-th epoch val loss 1.101313336605122\n",
      "848-th epoch train loss 0.9863784129155415\n",
      "848-th epoch val loss 1.1013133864310625\n",
      "849-th epoch train loss 0.9863784631780538\n",
      "849-th epoch val loss 1.1013134354763696\n",
      "850-th epoch train loss 0.9863785126530868\n",
      "850-th epoch val loss 1.1013134837532712\n",
      "851-th epoch train loss 0.9863785613529781\n",
      "851-th epoch val loss 1.1013135312738056\n",
      "852-th epoch train loss 0.9863786092898684\n",
      "852-th epoch val loss 1.1013135780498198\n",
      "853-th epoch train loss 0.9863786564757111\n",
      "853-th epoch val loss 1.101313624092978\n",
      "854-th epoch train loss 0.9863787029222708\n",
      "854-th epoch val loss 1.101313669414759\n",
      "855-th epoch train loss 0.9863787486411285\n",
      "855-th epoch val loss 1.1013137140264628\n",
      "856-th epoch train loss 0.9863787936436836\n",
      "856-th epoch val loss 1.1013137579392134\n",
      "857-th epoch train loss 0.9863788379411579\n",
      "857-th epoch val loss 1.1013138011639603\n",
      "858-th epoch train loss 0.986378881544596\n",
      "858-th epoch val loss 1.1013138437114807\n",
      "859-th epoch train loss 0.98637892446487\n",
      "859-th epoch val loss 1.1013138855923834\n",
      "860-th epoch train loss 0.9863789667126825\n",
      "860-th epoch val loss 1.1013139268171122\n",
      "861-th epoch train loss 0.9863790082985678\n",
      "861-th epoch val loss 1.1013139673959453\n",
      "862-th epoch train loss 0.9863790492328958\n",
      "862-th epoch val loss 1.101314007339001\n",
      "863-th epoch train loss 0.9863790895258727\n",
      "863-th epoch val loss 1.1013140466562394\n",
      "864-th epoch train loss 0.986379129187546\n",
      "864-th epoch val loss 1.1013140853574641\n",
      "865-th epoch train loss 0.9863791682278059\n",
      "865-th epoch val loss 1.1013141234523254\n",
      "866-th epoch train loss 0.9863792066563873\n",
      "866-th epoch val loss 1.101314160950323\n",
      "867-th epoch train loss 0.9863792444828724\n",
      "867-th epoch val loss 1.1013141978608063\n",
      "868-th epoch train loss 0.9863792817166939\n",
      "868-th epoch val loss 1.1013142341929798\n",
      "869-th epoch train loss 0.9863793183671359\n",
      "869-th epoch val loss 1.1013142699559029\n",
      "870-th epoch train loss 0.9863793544433382\n",
      "870-th epoch val loss 1.1013143051584937\n",
      "871-th epoch train loss 0.9863793899542972\n",
      "871-th epoch val loss 1.101314339809531\n",
      "872-th epoch train loss 0.9863794249088678\n",
      "872-th epoch val loss 1.1013143739176543\n",
      "873-th epoch train loss 0.9863794593157663\n",
      "873-th epoch val loss 1.1013144074913694\n",
      "874-th epoch train loss 0.9863794931835731\n",
      "874-th epoch val loss 1.1013144405390491\n",
      "875-th epoch train loss 0.9863795265207332\n",
      "875-th epoch val loss 1.1013144730689335\n",
      "876-th epoch train loss 0.986379559335561\n",
      "876-th epoch val loss 1.1013145050891353\n",
      "877-th epoch train loss 0.9863795916362385\n",
      "877-th epoch val loss 1.1013145366076387\n",
      "878-th epoch train loss 0.9863796234308215\n",
      "878-th epoch val loss 1.1013145676323046\n",
      "879-th epoch train loss 0.9863796547272377\n",
      "879-th epoch val loss 1.1013145981708683\n",
      "880-th epoch train loss 0.9863796855332931\n",
      "880-th epoch val loss 1.1013146282309465\n",
      "881-th epoch train loss 0.9863797158566691\n",
      "881-th epoch val loss 1.1013146578200348\n",
      "882-th epoch train loss 0.9863797457049281\n",
      "882-th epoch val loss 1.1013146869455126\n",
      "883-th epoch train loss 0.9863797750855138\n",
      "883-th epoch val loss 1.101314715614643\n",
      "884-th epoch train loss 0.9863798040057523\n",
      "884-th epoch val loss 1.1013147438345743\n",
      "885-th epoch train loss 0.986379832472857\n",
      "885-th epoch val loss 1.1013147716123461\n",
      "886-th epoch train loss 0.9863798604939262\n",
      "886-th epoch val loss 1.1013147989548844\n",
      "887-th epoch train loss 0.9863798880759482\n",
      "887-th epoch val loss 1.1013148258690075\n",
      "888-th epoch train loss 0.9863799152258019\n",
      "888-th epoch val loss 1.1013148523614287\n",
      "889-th epoch train loss 0.9863799419502576\n",
      "889-th epoch val loss 1.101314878438754\n",
      "890-th epoch train loss 0.9863799682559808\n",
      "890-th epoch val loss 1.1013149041074868\n",
      "891-th epoch train loss 0.9863799941495309\n",
      "891-th epoch val loss 1.1013149293740288\n",
      "892-th epoch train loss 0.9863800196373662\n",
      "892-th epoch val loss 1.1013149542446807\n",
      "893-th epoch train loss 0.9863800447258432\n",
      "893-th epoch val loss 1.1013149787256458\n",
      "894-th epoch train loss 0.9863800694212179\n",
      "894-th epoch val loss 1.1013150028230283\n",
      "895-th epoch train loss 0.9863800937296499\n",
      "895-th epoch val loss 1.1013150265428386\n",
      "896-th epoch train loss 0.9863801176572015\n",
      "896-th epoch val loss 1.1013150498909923\n",
      "897-th epoch train loss 0.9863801412098399\n",
      "897-th epoch val loss 1.1013150728733117\n",
      "898-th epoch train loss 0.9863801643934393\n",
      "898-th epoch val loss 1.1013150954955293\n",
      "899-th epoch train loss 0.9863801872137815\n",
      "899-th epoch val loss 1.1013151177632865\n",
      "900-th epoch train loss 0.9863802096765583\n",
      "900-th epoch val loss 1.1013151396821368\n",
      "901-th epoch train loss 0.9863802317873709\n",
      "901-th epoch val loss 1.101315161257547\n",
      "902-th epoch train loss 0.9863802535517343\n",
      "902-th epoch val loss 1.1013151824948972\n",
      "903-th epoch train loss 0.9863802749750765\n",
      "903-th epoch val loss 1.1013152033994846\n",
      "904-th epoch train loss 0.9863802960627407\n",
      "904-th epoch val loss 1.1013152239765231\n",
      "905-th epoch train loss 0.9863803168199857\n",
      "905-th epoch val loss 1.1013152442311438\n",
      "906-th epoch train loss 0.9863803372519891\n",
      "906-th epoch val loss 1.1013152641683988\n",
      "907-th epoch train loss 0.9863803573638462\n",
      "907-th epoch val loss 1.1013152837932605\n",
      "908-th epoch train loss 0.9863803771605729\n",
      "908-th epoch val loss 1.1013153031106226\n",
      "909-th epoch train loss 0.9863803966471072\n",
      "909-th epoch val loss 1.1013153221253043\n",
      "910-th epoch train loss 0.9863804158283087\n",
      "910-th epoch val loss 1.1013153408420469\n",
      "911-th epoch train loss 0.9863804347089614\n",
      "911-th epoch val loss 1.101315359265519\n",
      "912-th epoch train loss 0.986380453293774\n",
      "912-th epoch val loss 1.1013153774003146\n",
      "913-th epoch train loss 0.9863804715873827\n",
      "913-th epoch val loss 1.1013153952509585\n",
      "914-th epoch train loss 0.986380489594349\n",
      "914-th epoch val loss 1.1013154128219014\n",
      "915-th epoch train loss 0.9863805073191652\n",
      "915-th epoch val loss 1.1013154301175259\n",
      "916-th epoch train loss 0.9863805247662503\n",
      "916-th epoch val loss 1.1013154471421454\n",
      "917-th epoch train loss 0.9863805419399571\n",
      "917-th epoch val loss 1.101315463900006\n",
      "918-th epoch train loss 0.9863805588445688\n",
      "918-th epoch val loss 1.1013154803952887\n",
      "919-th epoch train loss 0.9863805754843015\n",
      "919-th epoch val loss 1.101315496632106\n",
      "920-th epoch train loss 0.9863805918633052\n",
      "920-th epoch val loss 1.101315512614508\n",
      "921-th epoch train loss 0.9863806079856654\n",
      "921-th epoch val loss 1.1013155283464813\n",
      "922-th epoch train loss 0.9863806238554027\n",
      "922-th epoch val loss 1.1013155438319489\n",
      "923-th epoch train loss 0.9863806394764757\n",
      "923-th epoch val loss 1.101315559074774\n",
      "924-th epoch train loss 0.9863806548527807\n",
      "924-th epoch val loss 1.1013155740787581\n",
      "925-th epoch train loss 0.9863806699881525\n",
      "925-th epoch val loss 1.101315588847643\n",
      "926-th epoch train loss 0.9863806848863662\n",
      "926-th epoch val loss 1.101315603385113\n",
      "927-th epoch train loss 0.9863806995511375\n",
      "927-th epoch val loss 1.1013156176947934\n",
      "928-th epoch train loss 0.9863807139861244\n",
      "928-th epoch val loss 1.1013156317802535\n",
      "929-th epoch train loss 0.9863807281949271\n",
      "929-th epoch val loss 1.1013156456450064\n",
      "930-th epoch train loss 0.986380742181089\n",
      "930-th epoch val loss 1.10131565929251\n",
      "931-th epoch train loss 0.9863807559480997\n",
      "931-th epoch val loss 1.1013156727261688\n",
      "932-th epoch train loss 0.9863807694993921\n",
      "932-th epoch val loss 1.1013156859493334\n",
      "933-th epoch train loss 0.9863807828383466\n",
      "933-th epoch val loss 1.1013156989653012\n",
      "934-th epoch train loss 0.9863807959682896\n",
      "934-th epoch val loss 1.1013157117773196\n",
      "935-th epoch train loss 0.9863808088924965\n",
      "935-th epoch val loss 1.1013157243885832\n",
      "936-th epoch train loss 0.986380821614191\n",
      "936-th epoch val loss 1.1013157368022382\n",
      "937-th epoch train loss 0.9863808341365459\n",
      "937-th epoch val loss 1.1013157490213805\n",
      "938-th epoch train loss 0.9863808464626849\n",
      "938-th epoch val loss 1.1013157610490583\n",
      "939-th epoch train loss 0.9863808585956816\n",
      "939-th epoch val loss 1.101315772888271\n",
      "940-th epoch train loss 0.9863808705385633\n",
      "940-th epoch val loss 1.101315784541972\n",
      "941-th epoch train loss 0.9863808822943086\n",
      "941-th epoch val loss 1.101315796013068\n",
      "942-th epoch train loss 0.986380893865849\n",
      "942-th epoch val loss 1.1013158073044196\n",
      "943-th epoch train loss 0.9863809052560718\n",
      "943-th epoch val loss 1.1013158184188439\n",
      "944-th epoch train loss 0.9863809164678171\n",
      "944-th epoch val loss 1.101315829359113\n",
      "945-th epoch train loss 0.9863809275038824\n",
      "945-th epoch val loss 1.1013158401279557\n",
      "946-th epoch train loss 0.9863809383670196\n",
      "946-th epoch val loss 1.1013158507280578\n",
      "947-th epoch train loss 0.9863809490599387\n",
      "947-th epoch val loss 1.1013158611620635\n",
      "948-th epoch train loss 0.9863809595853059\n",
      "948-th epoch val loss 1.1013158714325746\n",
      "949-th epoch train loss 0.9863809699457479\n",
      "949-th epoch val loss 1.1013158815421538\n",
      "950-th epoch train loss 0.9863809801438477\n",
      "950-th epoch val loss 1.1013158914933219\n",
      "951-th epoch train loss 0.9863809901821494\n",
      "951-th epoch val loss 1.1013159012885616\n",
      "952-th epoch train loss 0.9863810000631578\n",
      "952-th epoch val loss 1.101315910930316\n",
      "953-th epoch train loss 0.986381009789336\n",
      "953-th epoch val loss 1.10131592042099\n",
      "954-th epoch train loss 0.986381019363111\n",
      "954-th epoch val loss 1.1013159297629511\n",
      "955-th epoch train loss 0.9863810287868703\n",
      "955-th epoch val loss 1.1013159389585292\n",
      "956-th epoch train loss 0.9863810380629647\n",
      "956-th epoch val loss 1.1013159480100176\n",
      "957-th epoch train loss 0.986381047193708\n",
      "957-th epoch val loss 1.1013159569196747\n",
      "958-th epoch train loss 0.9863810561813778\n",
      "958-th epoch val loss 1.1013159656897225\n",
      "959-th epoch train loss 0.9863810650282154\n",
      "959-th epoch val loss 1.1013159743223484\n",
      "960-th epoch train loss 0.9863810737364284\n",
      "960-th epoch val loss 1.101315982819706\n",
      "961-th epoch train loss 0.9863810823081884\n",
      "961-th epoch val loss 1.101315991183915\n",
      "962-th epoch train loss 0.9863810907456331\n",
      "962-th epoch val loss 1.1013159994170607\n",
      "963-th epoch train loss 0.9863810990508675\n",
      "963-th epoch val loss 1.101316007521198\n",
      "964-th epoch train loss 0.9863811072259624\n",
      "964-th epoch val loss 1.1013160154983466\n",
      "965-th epoch train loss 0.9863811152729589\n",
      "965-th epoch val loss 1.101316023350499\n",
      "966-th epoch train loss 0.9863811231938626\n",
      "966-th epoch val loss 1.1013160310796113\n",
      "967-th epoch train loss 0.9863811309906494\n",
      "967-th epoch val loss 1.1013160386876124\n",
      "968-th epoch train loss 0.9863811386652643\n",
      "968-th epoch val loss 1.1013160461763998\n",
      "969-th epoch train loss 0.9863811462196225\n",
      "969-th epoch val loss 1.1013160535478421\n",
      "970-th epoch train loss 0.9863811536556066\n",
      "970-th epoch val loss 1.101316060803777\n",
      "971-th epoch train loss 0.9863811609750732\n",
      "971-th epoch val loss 1.1013160679460152\n",
      "972-th epoch train loss 0.9863811681798461\n",
      "972-th epoch val loss 1.1013160749763378\n",
      "973-th epoch train loss 0.9863811752717241\n",
      "973-th epoch val loss 1.1013160818964982\n",
      "974-th epoch train loss 0.9863811822524752\n",
      "974-th epoch val loss 1.1013160887082227\n",
      "975-th epoch train loss 0.9863811891238413\n",
      "975-th epoch val loss 1.1013160954132113\n",
      "976-th epoch train loss 0.986381195887536\n",
      "976-th epoch val loss 1.1013161020131352\n",
      "977-th epoch train loss 0.9863812025452471\n",
      "977-th epoch val loss 1.1013161085096417\n",
      "978-th epoch train loss 0.9863812090986341\n",
      "978-th epoch val loss 1.1013161149043509\n",
      "979-th epoch train loss 0.9863812155493328\n",
      "979-th epoch val loss 1.1013161211988571\n",
      "980-th epoch train loss 0.986381221898951\n",
      "980-th epoch val loss 1.1013161273947316\n",
      "981-th epoch train loss 0.9863812281490743\n",
      "981-th epoch val loss 1.101316133493519\n",
      "982-th epoch train loss 0.9863812343012602\n",
      "982-th epoch val loss 1.1013161394967415\n",
      "983-th epoch train loss 0.9863812403570439\n",
      "983-th epoch val loss 1.1013161454058955\n",
      "984-th epoch train loss 0.9863812463179358\n",
      "984-th epoch val loss 1.1013161512224559\n",
      "985-th epoch train loss 0.9863812521854229\n",
      "985-th epoch val loss 1.1013161569478729\n",
      "986-th epoch train loss 0.9863812579609686\n",
      "986-th epoch val loss 1.1013161625835746\n",
      "987-th epoch train loss 0.9863812636460138\n",
      "987-th epoch val loss 1.1013161681309669\n",
      "988-th epoch train loss 0.9863812692419763\n",
      "988-th epoch val loss 1.101316173591434\n",
      "989-th epoch train loss 0.986381274750253\n",
      "989-th epoch val loss 1.1013161789663384\n",
      "990-th epoch train loss 0.9863812801722159\n",
      "990-th epoch val loss 1.1013161842570192\n",
      "991-th epoch train loss 0.986381285509219\n",
      "991-th epoch val loss 1.101316189464797\n",
      "992-th epoch train loss 0.9863812907625936\n",
      "992-th epoch val loss 1.101316194590971\n",
      "993-th epoch train loss 0.9863812959336495\n",
      "993-th epoch val loss 1.1013161996368197\n",
      "994-th epoch train loss 0.9863813010236765\n",
      "994-th epoch val loss 1.1013162046036018\n",
      "995-th epoch train loss 0.9863813060339448\n",
      "995-th epoch val loss 1.1013162094925562\n",
      "996-th epoch train loss 0.9863813109657036\n",
      "996-th epoch val loss 1.1013162143049022\n",
      "997-th epoch train loss 0.9863813158201837\n",
      "997-th epoch val loss 1.1013162190418406\n",
      "998-th epoch train loss 0.9863813205985958\n",
      "998-th epoch val loss 1.101316223704553\n",
      "999-th epoch train loss 0.9863813253021321\n",
      "999-th epoch val loss 1.1013162282942022\n"
     ]
    }
   ],
   "source": [
    "slr = ScratchLinearRegression(num_iter=1000, lr=0.01, no_bias=True, verbose=True)\n",
    "slr.fit(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ecc6e9e7-ccb2-43e6-a08b-d560c677cb15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[12.80731167],\n",
       "       [10.61743588],\n",
       "       [12.06102741],\n",
       "       [10.15320822],\n",
       "       [12.1159035 ],\n",
       "       [13.18618092],\n",
       "       [11.41794831],\n",
       "       [13.35882172],\n",
       "       [13.44980011],\n",
       "       [13.10904681],\n",
       "       [ 9.91019967],\n",
       "       [13.11602999],\n",
       "       [12.94177422],\n",
       "       [13.19015351],\n",
       "       [12.05702241],\n",
       "       [12.20058157],\n",
       "       [13.30906714],\n",
       "       [ 8.54293476],\n",
       "       [11.54856288],\n",
       "       [12.21659653],\n",
       "       [13.46252535],\n",
       "       [13.86023323],\n",
       "       [ 9.97684153],\n",
       "       [13.38110964],\n",
       "       [11.6747064 ],\n",
       "       [13.42323059],\n",
       "       [13.39250157],\n",
       "       [10.62901473],\n",
       "       [10.48942562],\n",
       "       [11.60573681],\n",
       "       [11.30257363],\n",
       "       [11.82698833],\n",
       "       [10.99587149],\n",
       "       [11.66893445],\n",
       "       [13.29242656],\n",
       "       [10.90377408],\n",
       "       [ 9.72442735],\n",
       "       [10.10807667],\n",
       "       [ 9.78656827],\n",
       "       [12.9568596 ],\n",
       "       [11.87177845],\n",
       "       [12.35664663],\n",
       "       [11.09358382],\n",
       "       [12.72021367],\n",
       "       [10.07638565],\n",
       "       [11.56749624],\n",
       "       [11.37207158],\n",
       "       [12.37449591],\n",
       "       [11.00037244],\n",
       "       [13.44442937],\n",
       "       [13.52752236],\n",
       "       [10.88207936],\n",
       "       [10.26007695],\n",
       "       [13.29584343],\n",
       "       [11.26572868],\n",
       "       [13.3157712 ],\n",
       "       [11.36878186],\n",
       "       [ 9.8729834 ],\n",
       "       [12.1432805 ],\n",
       "       [10.79579138],\n",
       "       [13.48866123],\n",
       "       [11.61430507],\n",
       "       [11.65875375],\n",
       "       [13.86315162],\n",
       "       [13.22612866],\n",
       "       [13.44210164],\n",
       "       [10.15991227],\n",
       "       [11.02951892],\n",
       "       [13.42754461],\n",
       "       [12.34873135],\n",
       "       [13.49160952],\n",
       "       [13.02145287],\n",
       "       [11.47872349],\n",
       "       [12.11894651],\n",
       "       [10.13464618],\n",
       "       [13.32887029],\n",
       "       [10.6246384 ],\n",
       "       [10.77518075],\n",
       "       [13.34165784],\n",
       "       [11.63084839],\n",
       "       [11.02951892],\n",
       "       [13.23919533],\n",
       "       [13.0691938 ],\n",
       "       [13.36971771],\n",
       "       [11.90899471],\n",
       "       [ 9.54936156],\n",
       "       [13.13195528],\n",
       "       [11.78812721],\n",
       "       [12.0843371 ],\n",
       "       [13.31726155],\n",
       "       [13.59525084],\n",
       "       [12.71028221],\n",
       "       [13.08970971],\n",
       "       [11.38858248],\n",
       "       [11.21249746],\n",
       "       [13.19716911],\n",
       "       [11.64413441],\n",
       "       [ 9.75068785],\n",
       "       [11.71127222],\n",
       "       [13.66716618],\n",
       "       [10.55442516],\n",
       "       [11.95437297],\n",
       "       [12.31508646],\n",
       "       [12.34066156],\n",
       "       [12.41965735],\n",
       "       [11.81258581],\n",
       "       [12.23925577],\n",
       "       [11.79445741],\n",
       "       [12.60539978],\n",
       "       [13.2741786 ],\n",
       "       [11.71748033],\n",
       "       [ 9.87257966],\n",
       "       [10.61330878],\n",
       "       [12.14086057],\n",
       "       [13.1503927 ],\n",
       "       [13.67359365],\n",
       "       [13.48518459],\n",
       "       [13.05441743],\n",
       "       [11.93534741],\n",
       "       [11.66899676],\n",
       "       [13.02905661],\n",
       "       [10.63904092],\n",
       "       [ 9.88999277],\n",
       "       [13.45569668],\n",
       "       [11.32029829],\n",
       "       [10.96157362],\n",
       "       [11.81022566],\n",
       "       [13.77546549],\n",
       "       [12.85573294],\n",
       "       [10.99273628],\n",
       "       [13.12692598],\n",
       "       [13.30608644],\n",
       "       [13.2051467 ],\n",
       "       [11.47676709],\n",
       "       [10.5161547 ],\n",
       "       [13.3645339 ],\n",
       "       [13.39398939],\n",
       "       [13.49213788],\n",
       "       [13.73716261],\n",
       "       [11.84070546],\n",
       "       [13.47500389],\n",
       "       [12.35397494],\n",
       "       [11.45072594],\n",
       "       [11.78812721],\n",
       "       [13.81712039],\n",
       "       [10.08442555],\n",
       "       [12.26206951],\n",
       "       [13.28715308],\n",
       "       [12.22547381],\n",
       "       [13.45861507],\n",
       "       [12.00363414],\n",
       "       [ 8.43864046],\n",
       "       [12.09659629],\n",
       "       [13.30183473],\n",
       "       [13.29295492],\n",
       "       [11.47866118],\n",
       "       [10.93844833],\n",
       "       [10.87866503],\n",
       "       [ 9.77762869],\n",
       "       [13.05305169],\n",
       "       [13.34479305],\n",
       "       [ 9.2168052 ],\n",
       "       [10.23816288],\n",
       "       [13.25896859],\n",
       "       [ 9.65440357],\n",
       "       [13.86342822],\n",
       "       [11.56017163],\n",
       "       [11.67371452],\n",
       "       [13.63280854],\n",
       "       [11.44324428],\n",
       "       [12.71353951],\n",
       "       [13.33225221],\n",
       "       [13.27902098],\n",
       "       [ 8.09875949],\n",
       "       [11.43297139],\n",
       "       [ 9.50423254],\n",
       "       [11.80603373],\n",
       "       [12.24403584],\n",
       "       [13.52649806],\n",
       "       [10.15668486],\n",
       "       [11.03873762],\n",
       "       [11.32017367],\n",
       "       [ 8.9734228 ],\n",
       "       [ 9.67395748],\n",
       "       [13.26809257],\n",
       "       [12.72781994],\n",
       "       [13.10501191],\n",
       "       [12.94025397],\n",
       "       [11.0851726 ],\n",
       "       [11.50681325],\n",
       "       [13.24565268],\n",
       "       [11.36934011],\n",
       "       [10.45519006],\n",
       "       [11.6402216 ],\n",
       "       [12.32911512],\n",
       "       [11.15405   ],\n",
       "       [11.06775948],\n",
       "       [11.71785166],\n",
       "       [12.55340713],\n",
       "       [13.33358805],\n",
       "       [13.18968745],\n",
       "       [12.01989834],\n",
       "       [11.29770136],\n",
       "       [10.77126794],\n",
       "       [10.20237466],\n",
       "       [13.55229251],\n",
       "       [13.40653023],\n",
       "       [13.31018364],\n",
       "       [ 9.2231354 ],\n",
       "       [11.0384585 ],\n",
       "       [11.82711295],\n",
       "       [14.49315899],\n",
       "       [13.4244094 ],\n",
       "       [13.76003866],\n",
       "       [ 9.54179024],\n",
       "       [ 9.95452373],\n",
       "       [13.60816301],\n",
       "       [ 9.95402526],\n",
       "       [13.28367643],\n",
       "       [13.03020806],\n",
       "       [11.89440527],\n",
       "       [11.56637974],\n",
       "       [13.68129211],\n",
       "       [13.48742012],\n",
       "       [11.67458431],\n",
       "       [12.34240115],\n",
       "       [11.99329893],\n",
       "       [12.94428887],\n",
       "       [ 9.78995272],\n",
       "       [11.62969947],\n",
       "       [13.18776347],\n",
       "       [10.86584759],\n",
       "       [13.34889026],\n",
       "       [10.32103906],\n",
       "       [12.04997692],\n",
       "       [11.78964745],\n",
       "       [ 9.80435271],\n",
       "       [13.30375871],\n",
       "       [11.10885614],\n",
       "       [13.42229849],\n",
       "       [11.15405   ],\n",
       "       [12.06307348],\n",
       "       [11.07185669],\n",
       "       [11.87801645],\n",
       "       [11.10503553],\n",
       "       [11.7213283 ],\n",
       "       [11.75845237],\n",
       "       [13.35475694],\n",
       "       [13.26592188],\n",
       "       [13.32501979],\n",
       "       [11.32644409],\n",
       "       [13.32259987],\n",
       "       [11.58699038],\n",
       "       [ 9.87397528],\n",
       "       [11.85231673],\n",
       "       [11.82807494],\n",
       "       [13.5343485 ],\n",
       "       [13.3157712 ],\n",
       "       [11.63466647],\n",
       "       [13.25589569],\n",
       "       [13.36695382],\n",
       "       [13.37189093],\n",
       "       [ 9.34381851],\n",
       "       [13.43924555],\n",
       "       [13.2883942 ],\n",
       "       [13.64727337],\n",
       "       [11.66924599],\n",
       "       [10.25278222],\n",
       "       [13.36978002],\n",
       "       [11.66924599],\n",
       "       [ 9.38677937],\n",
       "       [13.20266193],\n",
       "       [11.97129014],\n",
       "       [13.36316816],\n",
       "       [13.07332089],\n",
       "       [13.43536517],\n",
       "       [13.01626906],\n",
       "       [13.66428274],\n",
       "       [11.21550805],\n",
       "       [11.91302961],\n",
       "       [12.96567457],\n",
       "       [13.46873346],\n",
       "       [11.69597001],\n",
       "       [12.11838826],\n",
       "       [11.03225038],\n",
       "       [11.11444369],\n",
       "       [11.09426669],\n",
       "       [13.14238269],\n",
       "       [ 9.91432676],\n",
       "       [11.82711295],\n",
       "       [13.48046683],\n",
       "       [11.74914146]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3466da63-2d30-4ed2-98c4-d3fa21b68f28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1f0bacca740>]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.plot(slr.loss)\n",
    "plt.plot(slr.val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6548b601-74fb-48b4-b397-901b96f9a9be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
