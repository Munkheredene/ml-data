{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2c27153-0d5d-4475-9c89-c75140479612",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42cbdb82-cf80-472e-b007-4199fd2ba4d7",
   "metadata": {},
   "source": [
    "Linear Regression Stratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23a43135-139a-4801-8281-9ebc0c47ab77",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScratchLinearRegression():\n",
    "    \n",
    "    \"\"\"\n",
    "    線形回帰のスクラッチ実装\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    num_iter : int\n",
    "      イテレーション数\n",
    "    lr : float\n",
    "      学習率\n",
    "    no_bias : bool\n",
    "      バイアス項を入れない場合はTrue\n",
    "    verbose : bool\n",
    "      学習過程を出力する場合はTrue\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    self.coef_ : 次の形のndarray, shape (n_features,)\n",
    "      パラメータ\n",
    "    self.loss : 次の形のndarray, shape (self.iter,)\n",
    "      訓練データに対する損失の記録\n",
    "    self.val_loss : 次の形のndarray, shape (self.iter,)\n",
    "      検証データに対する損失の記録\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_iter, lr, no_bias, verbose): \n",
    "        \n",
    "        self.iter = num_iter\n",
    "        self.lr = lr\n",
    "        self.bias = no_bias\n",
    "        self.verbose = verbose\n",
    "     \n",
    "        self.loss = np.zeros(self.iter)\n",
    "        self.val_loss = np.zeros(self.iter)\n",
    "      \n",
    "\n",
    "    #6\n",
    "    def fit(self, X, y, X_val=None, y_val=None):\n",
    "        \"\"\"\n",
    "        線形回帰を学習する。検証データが入力された場合はそれに対する損失と精度もイテレーションごとに計算する。\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (n_samples, n_features)\n",
    "            訓練データの特徴量\n",
    "        y : 次の形のndarray, shape (n_samples, )\n",
    "            訓練データの正解値\n",
    "        X_val : 次の形のndarray, shape (n_samples, n_features)\n",
    "            検証データの特徴量\n",
    "        y_val : 次の形のndarray, shape (n_samples, )\n",
    "            検証データの正解値\n",
    "        \"\"\"\n",
    "        \n",
    "        if self.bias == True:\n",
    "            bias = np.ones((X.shape[0], 1))\n",
    "            X = np.hstack((bias, X))\n",
    "            if X_val is not None:\n",
    "                bias = np.ones((X_val.shape[0], 1))\n",
    "                X_val = np.hstack((bias, X_val))\n",
    "            self.coef_ = np.random.rand(X.shape[1])\n",
    "            self.coef_ = self.coef_.reshape(X.shape[1], 1)\n",
    "    \n",
    "\n",
    "        for epoch in range(self.iter):\n",
    "            y_pred = self._linear_hypothesis(X)\n",
    "            self.loss[epoch] = np.mean((y-y_pred)**2)\n",
    "            \n",
    "            if X_val is not None:\n",
    "                pred_val = self._linear_hypothesis(X_val)\n",
    "                self.val_loss[epoch] = np.mean((y_val-pred_val)**2)\n",
    "                \n",
    "            self.coef_ = self._gradient_descent(X, (y_pred-y))\n",
    "           \n",
    "            if self.verbose == True:\n",
    "                print('{}-th epoch train loss {}'.format(epoch, self.loss[epoch]))\n",
    "                if X_val is not None:\n",
    "                    print('{}-th epoch val loss {}'.format(epoch, self.val_loss[epoch] ))\n",
    "\n",
    "\n",
    "    # 1\n",
    "    def _linear_hypothesis(self, X):\n",
    "        \"\"\"\n",
    "        仮定関数の出力を計算する\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (n_samples, n_features)\n",
    "          訓練データ\n",
    "        Returns\n",
    "        -------\n",
    "        次の形のndarray, shape (n_samples, 1)\n",
    "        線形の仮定関数による推定結果\n",
    "        \"\"\"\n",
    "        pred = X @ self.coef_\n",
    "        \n",
    "        return pred\n",
    "\n",
    "    # 2\n",
    "    def _gradient_descent(self, X, error):\n",
    "\n",
    "        for i in range(X.shape[1]):\n",
    "            gradient = error*X[:, i]\n",
    "            self.coef_[i, :] = self.coef_[i, :] - self.lr * np.mean(gradient)\n",
    "\n",
    "        return self.coef_\n",
    "        \n",
    "\n",
    "    # 3\n",
    "    def predict(self, X):\n",
    "        if self.bias == True:\n",
    "            bias = np.ones(X.shape[0]).reshape(X.shape[0], 1)\n",
    "            X = np.hstack([bias, X])\n",
    "        pred_y = self._linear_hypothesis(X)\n",
    "        return pred_y\n",
    "\n",
    "    # 4\n",
    "    def _mse(self, y_pred, y):\n",
    "        \"\"\"\n",
    "        平均二乗誤差の計算\n",
    "        \"\"\"\n",
    "        mse = np.mean((y-y_pred)**2)\n",
    "        \n",
    "        return mse\n",
    "\n",
    "    # 5\n",
    "    def _loss_func(self, pred, y):\n",
    "        \"\"\"\n",
    "        損失関数\n",
    "        \"\"\"\n",
    "        loss = self._mse(pred, y)/2\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e8a88fe-f171-4769-a4f1-444dacd9077b",
   "metadata": {},
   "source": [
    "Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4eb794e4-71d5-4218-aa68-abcba66db835",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "dataset = pd.read_csv(\"../data/train.csv\")\n",
    "X = dataset.loc[:, ['GrLivArea', 'YearBuilt']]\n",
    "y = dataset.loc[:, ['SalePrice']]\n",
    "X = X.values\n",
    "X = MinMaxScaler().fit_transform(X)\n",
    "y = np.log(y.values)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21e11039-408c-4c8e-834c-3cba80a00bd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0-th epoch train loss 117.03335915186287\n",
      "0-th epoch val loss 117.31735398243754\n",
      "1-th epoch train loss 113.40056215833891\n",
      "1-th epoch val loss 113.66778784670672\n",
      "2-th epoch train loss 109.88117362044744\n",
      "2-th epoch val loss 110.13226054237487\n",
      "3-th epoch train loss 106.4716623927383\n",
      "3-th epoch val loss 106.70721963241385\n",
      "4-th epoch train loss 103.16860713362148\n",
      "4-th epoch val loss 103.38922317170552\n",
      "5-th epoch train loss 99.96869289316965\n",
      "5-th epoch val loss 100.17493627305686\n",
      "6-th epoch train loss 96.86870780692136\n",
      "6-th epoch val loss 97.06112777989898\n",
      "7-th epoch train loss 93.86553989239185\n",
      "7-th epoch val loss 94.04466704235578\n",
      "8-th epoch train loss 90.9561739451017\n",
      "8-th epoch val loss 91.1225207934715\n",
      "9-th epoch train loss 88.1376885310324\n",
      "9-th epoch val loss 88.2917501224863\n",
      "10-th epoch train loss 85.40725307251364\n",
      "10-th epoch val loss 85.54950754214552\n",
      "11-th epoch train loss 82.76212502464014\n",
      "11-th epoch val loss 82.89303414712144\n",
      "12-th epoch train loss 80.19964713940654\n",
      "12-th epoch val loss 80.31965686071781\n",
      "13-th epoch train loss 77.71724481483542\n",
      "13-th epoch val loss 77.82678576711494\n",
      "14-th epoch train loss 75.31242352645882\n",
      "14-th epoch val loss 75.41191152649799\n",
      "15-th epoch train loss 72.98276633859491\n",
      "15-th epoch val loss 73.07260287049448\n",
      "16-th epoch train loss 70.72593149294188\n",
      "16-th epoch val loss 70.80650417542591\n",
      "17-th epoch train loss 68.53965007208684\n",
      "17-th epoch val loss 68.611333110957\n",
      "18-th epoch train loss 66.42172373560346\n",
      "18-th epoch val loss 66.48487836179994\n",
      "19-th epoch train loss 64.3700225264832\n",
      "19-th epoch val loss 64.42499742020513\n",
      "20-th epoch train loss 62.382482745715606\n",
      "20-th epoch val loss 62.429614447038844\n",
      "21-th epoch train loss 60.45710489290108\n",
      "21-th epoch val loss 60.496718199317925\n",
      "22-th epoch train loss 58.591951670844765\n",
      "22-th epoch val loss 58.6243600221369\n",
      "23-th epoch train loss 56.785146052144405\n",
      "23-th epoch val loss 56.810651902987495\n",
      "24-th epoch train loss 55.03486940584647\n",
      "24-th epoch val loss 55.053764586532225\n",
      "25-th epoch train loss 53.3393596823048\n",
      "25-th epoch val loss 53.351925747954496\n",
      "26-th epoch train loss 51.69690965443369\n",
      "26-th epoch val loss 51.70341822306529\n",
      "27-th epoch train loss 50.10586521360406\n",
      "27-th epoch val loss 50.1065782934038\n",
      "28-th epoch train loss 48.564623718484974\n",
      "28-th epoch val loss 48.55979402462334\n",
      "29-th epoch train loss 47.07163239518622\n",
      "29-th epoch val loss 47.06150365650761\n",
      "30-th epoch train loss 45.62538678710835\n",
      "30-th epoch val loss 45.610194043013266\n",
      "31-th epoch train loss 44.22442925295597\n",
      "31-th epoch val loss 44.20439914078491\n",
      "32-th epoch train loss 42.86734751141855\n",
      "32-th epoch val loss 42.842698544636825\n",
      "33-th epoch train loss 41.55277323106877\n",
      "33-th epoch val loss 41.52371606854222\n",
      "34-th epoch train loss 40.27938066407413\n",
      "34-th epoch val loss 40.246118370716715\n",
      "35-th epoch train loss 39.04588532236065\n",
      "35-th epoch val loss 39.00861362142588\n",
      "36-th epoch train loss 37.85104269491009\n",
      "36-th epoch val loss 37.80995021219005\n",
      "37-th epoch train loss 36.69364700491292\n",
      "37-th epoch val loss 36.648915505100085\n",
      "38-th epoch train loss 35.57253000553892\n",
      "38-th epoch val loss 35.52433462099824\n",
      "39-th epoch train loss 34.48655981312599\n",
      "39-th epoch val loss 34.43506926531671\n",
      "40-th epoch train loss 33.43463977662463\n",
      "40-th epoch val loss 33.38001659040409\n",
      "41-th epoch train loss 32.41570738217194\n",
      "41-th epoch val loss 32.358108093206134\n",
      "42-th epoch train loss 31.428733191704005\n",
      "42-th epoch val loss 31.3683085472026\n",
      "43-th epoch train loss 30.472719814549023\n",
      "43-th epoch val loss 30.409614967535973\n",
      "44-th epoch train loss 29.546700910976963\n",
      "44-th epoch val loss 29.481055608300988\n",
      "45-th epoch train loss 28.649740226712797\n",
      "45-th epoch val loss 28.58168899099567\n",
      "46-th epoch train loss 27.780930657451602\n",
      "46-th epoch val loss 27.71060296316605\n",
      "47-th epoch train loss 26.93939334244347\n",
      "47-th epoch val loss 26.86691378630645\n",
      "48-th epoch train loss 26.12427678624515\n",
      "48-th epoch val loss 26.049765252106376\n",
      "49-th epoch train loss 25.3347560077636\n",
      "49-th epoch val loss 25.258327826163587\n",
      "50-th epoch train loss 24.57003171574331\n",
      "50-th epoch val loss 24.491797818309838\n",
      "51-th epoch train loss 23.82932950987623\n",
      "51-th epoch val loss 23.7493965787227\n",
      "52-th epoch train loss 23.11189910673818\n",
      "52-th epoch val loss 23.030369719022236\n",
      "53-th epoch train loss 22.417013589780435\n",
      "53-th epoch val loss 22.33398635757637\n",
      "54-th epoch train loss 21.743968682629376\n",
      "54-th epoch val loss 21.65953838826282\n",
      "55-th epoch train loss 21.09208204496991\n",
      "55-th epoch val loss 21.00633977195883\n",
      "56-th epoch train loss 20.460692590311254\n",
      "56-th epoch val loss 20.373725850052708\n",
      "57-th epoch train loss 19.84915982495517\n",
      "57-th epoch val loss 19.76105267929278\n",
      "58-th epoch train loss 19.25686320750799\n",
      "58-th epoch val loss 19.16769638731107\n",
      "59-th epoch train loss 18.683201528298206\n",
      "59-th epoch val loss 18.59305254817917\n",
      "60-th epoch train loss 18.12759230808127\n",
      "60-th epoch val loss 18.036535577374064\n",
      "61-th epoch train loss 17.589471215432305\n",
      "61-th epoch val loss 17.497578145550793\n",
      "62-th epoch train loss 17.06829150224628\n",
      "62-th epoch val loss 16.975630610537607\n",
      "63-th epoch train loss 16.56352345678296\n",
      "63-th epoch val loss 16.470160466987547\n",
      "64-th epoch train loss 16.07465387371165\n",
      "64-th epoch val loss 15.980651813137753\n",
      "65-th epoch train loss 15.601185540627569\n",
      "65-th epoch val loss 15.506604834145012\n",
      "66-th epoch train loss 15.142636740528088\n",
      "66-th epoch val loss 15.047535301482581\n",
      "67-th epoch train loss 14.698540769753\n",
      "67-th epoch val loss 14.602974087899103\n",
      "68-th epoch train loss 14.268445470908397\n",
      "68-th epoch val loss 14.172466697456239\n",
      "69-th epoch train loss 13.851912780308616\n",
      "69-th epoch val loss 13.755572810176384\n",
      "70-th epoch train loss 13.448518289485166\n",
      "70-th epoch val loss 13.351865840846605\n",
      "71-th epoch train loss 13.057850820325656\n",
      "71-th epoch val loss 12.960932511538877\n",
      "72-th epoch train loss 12.679512013419185\n",
      "72-th epoch val loss 12.582372437420451\n",
      "73-th epoch train loss 12.313115929197924\n",
      "73-th epoch val loss 12.21579772544144\n",
      "74-th epoch train loss 11.958288661477267\n",
      "74-th epoch val loss 11.860832585499384\n",
      "75-th epoch train loss 11.614667963009348\n",
      "75-th epoch val loss 11.517112953693223\n",
      "76-th epoch train loss 11.281902882676695\n",
      "76-th epoch val loss 11.18428612729097\n",
      "77-th epoch train loss 10.959653413964311\n",
      "77-th epoch val loss 10.862010411047068\n",
      "78-th epoch train loss 10.64759015435976\n",
      "78-th epoch val loss 10.549954774516822\n",
      "79-th epoch train loss 10.345393975341754\n",
      "79-th epoch val loss 10.247798520026144\n",
      "80-th epoch train loss 10.052755702628176\n",
      "80-th epoch val loss 9.955230960965473\n",
      "81-th epoch train loss 9.769375806364815\n",
      "81-th epoch val loss 9.671951110087152\n",
      "82-th epoch train loss 9.494964100945943\n",
      "82-th epoch val loss 9.39766737749529\n",
      "83-th epoch train loss 9.22923945416738\n",
      "83-th epoch val loss 9.132097278027002\n",
      "84-th epoch train loss 8.971929505422159\n",
      "84-th epoch val loss 8.874967147733106\n",
      "85-th epoch train loss 8.722770392657722\n",
      "85-th epoch val loss 8.626011869175585\n",
      "86-th epoch train loss 8.481506487822513\n",
      "86-th epoch val loss 8.384974605267765\n",
      "87-th epoch train loss 8.247890140538022\n",
      "87-th epoch val loss 8.151606541391754\n",
      "88-th epoch train loss 8.021681429740854\n",
      "88-th epoch val loss 7.925666635535903\n",
      "89-th epoch train loss 7.802647923047024\n",
      "89-th epoch val loss 7.70692137620304\n",
      "90-th epoch train loss 7.590564443598589\n",
      "90-th epoch val loss 7.495144547848015\n",
      "91-th epoch train loss 7.385212844160132\n",
      "91-th epoch val loss 7.29011700361053\n",
      "92-th epoch train loss 7.186381788239692\n",
      "92-th epoch val loss 7.091626445116516\n",
      "93-th epoch train loss 6.993866538016003\n",
      "93-th epoch val loss 6.899467209128403\n",
      "94-th epoch train loss 6.807468748860442\n",
      "94-th epoch val loss 6.713440060831439\n",
      "95-th epoch train loss 6.626996270248756\n",
      "95-th epoch val loss 6.5333519935497675\n",
      "96-th epoch train loss 6.452262952864029\n",
      "96-th epoch val loss 6.359016034692453\n",
      "97-th epoch train loss 6.2830884616984815\n",
      "97-th epoch val loss 6.190251057735837\n",
      "98-th epoch train loss 6.119298094967648\n",
      "98-th epoch val loss 6.026881600054582\n",
      "99-th epoch train loss 5.960722608656335\n",
      "99-th epoch val loss 5.8687376864196485\n",
      "100-th epoch train loss 5.807198046521337\n",
      "100-th epoch val loss 5.71565465798706\n",
      "101-th epoch train loss 5.6585655753812825\n",
      "101-th epoch val loss 5.567473006606759\n",
      "102-th epoch train loss 5.514671325529355\n",
      "102-th epoch val loss 5.424038214286253\n",
      "103-th epoch train loss 5.375366236109647\n",
      "103-th epoch val loss 5.285200597648788\n",
      "104-th epoch train loss 5.240505905302864\n",
      "104-th epoch val loss 5.150815157230778\n",
      "105-th epoch train loss 5.109950445171955\n",
      "105-th epoch val loss 5.020741431468142\n",
      "106-th epoch train loss 4.983564341022799\n",
      "106-th epoch val loss 4.894843355225731\n",
      "107-th epoch train loss 4.8612163151396235\n",
      "107-th epoch val loss 4.772989122728633\n",
      "108-th epoch train loss 4.742779194759219\n",
      "108-th epoch val loss 4.655051054758572\n",
      "109-th epoch train loss 4.628129784152163\n",
      "109-th epoch val loss 4.54090546998276\n",
      "110-th epoch train loss 4.517148740683424\n",
      "110-th epoch val loss 4.43043256028677\n",
      "111-th epoch train loss 4.4097204547286495\n",
      "111-th epoch val loss 4.323516269986947\n",
      "112-th epoch train loss 4.305732933326313\n",
      "112-th epoch val loss 4.220044178801774\n",
      "113-th epoch train loss 4.205077687449568\n",
      "113-th epoch val loss 4.1199073884652995\n",
      "114-th epoch train loss 4.10764962278532\n",
      "114-th epoch val loss 4.023000412869428\n",
      "115-th epoch train loss 4.013346933911511\n",
      "115-th epoch val loss 3.929221071625379\n",
      "116-th epoch train loss 3.922071001766952\n",
      "116-th epoch val loss 3.838470386937968\n",
      "117-th epoch train loss 3.8337262943113624\n",
      "117-th epoch val loss 3.7506524836897563\n",
      "118-th epoch train loss 3.7482202702765086\n",
      "118-th epoch val loss 3.6656744926352647\n",
      "119-th epoch train loss 3.6654632859122462\n",
      "119-th epoch val loss 3.5834464566085384\n",
      "120-th epoch train loss 3.585368504634468\n",
      "120-th epoch val loss 3.503881239650399\n",
      "121-th epoch train loss 3.5078518094846833\n",
      "121-th epoch val loss 3.426894438964577\n",
      "122-th epoch train loss 3.43283171831387\n",
      "122-th epoch val loss 3.352404299614809\n",
      "123-th epoch train loss 3.360229301605839\n",
      "123-th epoch val loss 3.280331631877605\n",
      "124-th epoch train loss 3.289968102858157\n",
      "124-th epoch val loss 3.2105997311681986\n",
      "125-th epoch train loss 3.2219740614410126\n",
      "125-th epoch val loss 3.143134300459591\n",
      "126-th epoch train loss 3.1561754378570583\n",
      "126-th epoch val loss 3.0778633751172046\n",
      "127-th epoch train loss 3.092502741327552\n",
      "127-th epoch val loss 3.0147172500740296\n",
      "128-th epoch train loss 3.0308886596324904\n",
      "128-th epoch val loss 2.953628409273474\n",
      "129-th epoch train loss 2.9712679911346256\n",
      "129-th epoch val loss 2.8945314573093883\n",
      "130-th epoch train loss 2.913577578919516\n",
      "130-th epoch val loss 2.83736305319497\n",
      "131-th epoch train loss 2.857756246985758\n",
      "131-th epoch val loss 2.7820618461943\n",
      "132-th epoch train loss 2.8037447384217145\n",
      "132-th epoch val loss 2.7285684136524053\n",
      "133-th epoch train loss 2.751485655506922\n",
      "133-th epoch val loss 2.6768252007616646\n",
      "134-th epoch train loss 2.7009234016783674\n",
      "134-th epoch val loss 2.62677646220435\n",
      "135-th epoch train loss 2.6520041253036255\n",
      "135-th epoch val loss 2.578368205612946\n",
      "136-th epoch train loss 2.6046756652046867\n",
      "136-th epoch val loss 2.5315481367917236\n",
      "137-th epoch train loss 2.5588874978780276\n",
      "137-th epoch val loss 2.4862656066447677\n",
      "138-th epoch train loss 2.514590686358181\n",
      "138-th epoch val loss 2.4424715597574016\n",
      "139-th epoch train loss 2.4717378306737023\n",
      "139-th epoch val loss 2.4001184845795676\n",
      "140-th epoch train loss 2.4302830198460006\n",
      "140-th epoch val loss 2.359160365161341\n",
      "141-th epoch train loss 2.39018178538306\n",
      "141-th epoch val loss 2.3195526343922883\n",
      "142-th epoch train loss 2.3513910562215745\n",
      "142-th epoch val loss 2.2812521286979073\n",
      "143-th epoch train loss 2.3138691150724227\n",
      "143-th epoch val loss 2.244217044147797\n",
      "144-th epoch train loss 2.277575556125874\n",
      "144-th epoch val loss 2.20840689393167\n",
      "145-th epoch train loss 2.2424712440741916\n",
      "145-th epoch val loss 2.173782467160621\n",
      "146-th epoch train loss 2.2085182744107192\n",
      "146-th epoch val loss 2.1403057889524626\n",
      "147-th epoch train loss 2.175679934965678\n",
      "147-th epoch val loss 2.1079400817611385\n",
      "148-th epoch train loss 2.14392066864028\n",
      "148-th epoch val loss 2.0766497279115406\n",
      "149-th epoch train loss 2.113206037301822\n",
      "149-th epoch val loss 2.046400233302205\n",
      "150-th epoch train loss 2.0835026868036928\n",
      "150-th epoch val loss 2.017158192239561\n",
      "151-th epoch train loss 2.0547783130952855\n",
      "151-th epoch val loss 1.9888912533685255\n",
      "152-th epoch train loss 2.0270016293878985\n",
      "152-th epoch val loss 1.9615680866653193\n",
      "153-th epoch train loss 2.000142334343811\n",
      "153-th epoch val loss 1.9351583514594644\n",
      "154-th epoch train loss 1.9741710812566597\n",
      "154-th epoch val loss 1.9096326654529228\n",
      "155-th epoch train loss 1.94905944819232\n",
      "155-th epoch val loss 1.884962574705368\n",
      "156-th epoch train loss 1.9247799090603812\n",
      "156-th epoch val loss 1.8611205245554976\n",
      "157-th epoch train loss 1.9013058055872782\n",
      "157-th epoch val loss 1.8380798314492774\n",
      "158-th epoch train loss 1.8786113201630197\n",
      "158-th epoch val loss 1.8158146556468775\n",
      "159-th epoch train loss 1.8566714495343455\n",
      "159-th epoch val loss 1.794299974780966\n",
      "160-th epoch train loss 1.8354619793179607\n",
      "160-th epoch val loss 1.773511558239849\n",
      "161-th epoch train loss 1.8149594593083433\n",
      "161-th epoch val loss 1.7534259423497953\n",
      "162-th epoch train loss 1.7951411795553998\n",
      "162-th epoch val loss 1.7340204063316642\n",
      "163-th epoch train loss 1.7759851471880004\n",
      "163-th epoch val loss 1.7152729490077259\n",
      "164-th epoch train loss 1.757470063960212\n",
      "164-th epoch val loss 1.697162266235348\n",
      "165-th epoch train loss 1.73957530449771\n",
      "165-th epoch val loss 1.6796677290448905\n",
      "166-th epoch train loss 1.7222808952225959\n",
      "166-th epoch val loss 1.6627693624599047\n",
      "167-th epoch train loss 1.705567493935505\n",
      "167-th epoch val loss 1.6464478249783905\n",
      "168-th epoch train loss 1.689416370034552\n",
      "168-th epoch val loss 1.630684388694531\n",
      "169-th epoch train loss 1.673809385351286\n",
      "169-th epoch val loss 1.615460920040962\n",
      "170-th epoch train loss 1.6587289755844563\n",
      "170-th epoch val loss 1.6007598611322527\n",
      "171-th epoch train loss 1.6441581323129872\n",
      "171-th epoch val loss 1.5865642116908907\n",
      "172-th epoch train loss 1.6300803855701234\n",
      "172-th epoch val loss 1.572857511537614\n",
      "173-th epoch train loss 1.6164797869612837\n",
      "173-th epoch val loss 1.5596238236285285\n",
      "174-th epoch train loss 1.6033408933087012\n",
      "174-th epoch val loss 1.5468477176219841\n",
      "175-th epoch train loss 1.5906487508064477\n",
      "175-th epoch val loss 1.534514253958705\n",
      "176-th epoch train loss 1.578388879669949\n",
      "176-th epoch val loss 1.5226089684391892\n",
      "177-th epoch train loss 1.5665472592646086\n",
      "177-th epoch val loss 1.5111178572828985\n",
      "178-th epoch train loss 1.5551103136986135\n",
      "178-th epoch val loss 1.5000273626542227\n",
      "179-th epoch train loss 1.5440648978654787\n",
      "179-th epoch val loss 1.489324358640685\n",
      "180-th epoch train loss 1.533398283922314\n",
      "180-th epoch val loss 1.4789961376692964\n",
      "181-th epoch train loss 1.523098148190263\n",
      "181-th epoch val loss 1.4690303973474164\n",
      "182-th epoch train loss 1.5131525584639542\n",
      "182-th epoch val loss 1.459415227714885\n",
      "183-th epoch train loss 1.5035499617172423\n",
      "183-th epoch val loss 1.45013909889463\n",
      "184-th epoch train loss 1.49427917219288\n",
      "184-th epoch val loss 1.441190849129315\n",
      "185-th epoch train loss 1.4853293598641832\n",
      "185-th epoch val loss 1.4325596731920134\n",
      "186-th epoch train loss 1.476690039257091\n",
      "186-th epoch val loss 1.424235111159251\n",
      "187-th epoch train loss 1.4683510586214024\n",
      "187-th epoch val loss 1.4162070375351208\n",
      "188-th epoch train loss 1.460302589440313\n",
      "188-th epoch val loss 1.4084656507155366\n",
      "189-th epoch train loss 1.4525351162677116\n",
      "189-th epoch val loss 1.4010014627820189\n",
      "190-th epoch train loss 1.4450394268830296\n",
      "190-th epoch val loss 1.3938052896147444\n",
      "191-th epoch train loss 1.4378066027537537\n",
      "191-th epoch val loss 1.3868682413149156\n",
      "192-th epoch train loss 1.4308280097960113\n",
      "192-th epoch val loss 1.3801817129267941\n",
      "193-th epoch train loss 1.4240952894239465\n",
      "193-th epoch val loss 1.373737375450072\n",
      "194-th epoch train loss 1.417600349878891\n",
      "194-th epoch val loss 1.3675271671335205\n",
      "195-th epoch train loss 1.4113353578296055\n",
      "195-th epoch val loss 1.361543285041151\n",
      "196-th epoch train loss 1.4052927302351517\n",
      "196-th epoch val loss 1.3557781768823898\n",
      "197-th epoch train loss 1.39946512646221\n",
      "197-th epoch val loss 1.350224533098038\n",
      "198-th epoch train loss 1.3938454406489107\n",
      "198-th epoch val loss 1.3448752791940382\n",
      "199-th epoch train loss 1.388426794307501\n",
      "199-th epoch val loss 1.3397235683153206\n",
      "200-th epoch train loss 1.3832025291583985\n",
      "200-th epoch val loss 1.334762774052245\n",
      "201-th epoch train loss 1.3781662001884254\n",
      "201-th epoch val loss 1.329986483472377\n",
      "202-th epoch train loss 1.3733115689262338\n",
      "202-th epoch val loss 1.3253884903705797\n",
      "203-th epoch train loss 1.3686325969281485\n",
      "203-th epoch val loss 1.3209627887306046\n",
      "204-th epoch train loss 1.3641234394678723\n",
      "204-th epoch val loss 1.3167035663915845\n",
      "205-th epoch train loss 1.3597784394236994\n",
      "205-th epoch val loss 1.3126051989130427\n",
      "206-th epoch train loss 1.3555921213570699\n",
      "206-th epoch val loss 1.3086622436322128\n",
      "207-th epoch train loss 1.3515591857765177\n",
      "207-th epoch val loss 1.3048694339076856\n",
      "208-th epoch train loss 1.347674503581207\n",
      "208-th epoch val loss 1.3012216735435485\n",
      "209-th epoch train loss 1.3439331106784813\n",
      "209-th epoch val loss 1.2977140313884081\n",
      "210-th epoch train loss 1.3403302027699804\n",
      "210-th epoch val loss 1.2943417361038165\n",
      "211-th epoch train loss 1.336861130301077\n",
      "211-th epoch val loss 1.2911001710968293\n",
      "212-th epoch train loss 1.3335213935685388\n",
      "212-th epoch val loss 1.287984869611564\n",
      "213-th epoch train loss 1.3303066379814779\n",
      "213-th epoch val loss 1.2849915099748026\n",
      "214-th epoch train loss 1.3272126494708087\n",
      "214-th epoch val loss 1.28211591099082\n",
      "215-th epoch train loss 1.324235350042577\n",
      "215-th epoch val loss 1.279354027480787\n",
      "216-th epoch train loss 1.3213707934706784\n",
      "216-th epoch val loss 1.276701945962232\n",
      "217-th epoch train loss 1.3186151611246122\n",
      "217-th epoch val loss 1.2741558804641835\n",
      "218-th epoch train loss 1.3159647579280596\n",
      "218-th epoch val loss 1.2717121684737658\n",
      "219-th epoch train loss 1.3134160084442008\n",
      "219-th epoch val loss 1.26936726701013\n",
      "220-th epoch train loss 1.3109654530838224\n",
      "220-th epoch val loss 1.267117748821753\n",
      "221-th epoch train loss 1.308609744432372\n",
      "221-th epoch val loss 1.2649602987032478\n",
      "222-th epoch train loss 1.3063456436922607\n",
      "222-th epoch val loss 1.2628917099279466\n",
      "223-th epoch train loss 1.3041700172368038\n",
      "223-th epoch val loss 1.260908880792649\n",
      "224-th epoch train loss 1.302079833272324\n",
      "224-th epoch val loss 1.259008811271023\n",
      "225-th epoch train loss 1.3000721586050346\n",
      "225-th epoch val loss 1.2571885997722652\n",
      "226-th epoch train loss 1.2981441555094317\n",
      "226-th epoch val loss 1.2554454400017314\n",
      "227-th epoch train loss 1.2962930786950302\n",
      "227-th epoch val loss 1.253776617920353\n",
      "228-th epoch train loss 1.2945162723683656\n",
      "228-th epoch val loss 1.252179508799743\n",
      "229-th epoch train loss 1.2928111673872928\n",
      "229-th epoch val loss 1.2506515743700097\n",
      "230-th epoch train loss 1.2911752785046977\n",
      "230-th epoch val loss 1.2491903600573773\n",
      "231-th epoch train loss 1.2896062016988206\n",
      "231-th epoch val loss 1.2477934923087945\n",
      "232-th epoch train loss 1.2881016115875012\n",
      "232-th epoch val loss 1.246458676000831\n",
      "233-th epoch train loss 1.286659258923708\n",
      "233-th epoch val loss 1.245183691930204\n",
      "234-th epoch train loss 1.2852769681698224\n",
      "234-th epoch val loss 1.2439663943833987\n",
      "235-th epoch train loss 1.2839526351482116\n",
      "235-th epoch val loss 1.2428047087828975\n",
      "236-th epoch train loss 1.2826842247657075\n",
      "236-th epoch val loss 1.2416966294076262\n",
      "237-th epoch train loss 1.2814697688096819\n",
      "237-th epoch val loss 1.2406402171852935\n",
      "238-th epoch train loss 1.2803073638134803\n",
      "238-th epoch val loss 1.2396335975543729\n",
      "239-th epoch train loss 1.2791951689890446\n",
      "239-th epoch val loss 1.2386749583935495\n",
      "240-th epoch train loss 1.2781314042246261\n",
      "240-th epoch val loss 1.2377625480165184\n",
      "241-th epoch train loss 1.2771143481455525\n",
      "241-th epoch val loss 1.2368946732300894\n",
      "242-th epoch train loss 1.276142336236075\n",
      "242-th epoch val loss 1.2360696974536136\n",
      "243-th epoch train loss 1.275213759020392\n",
      "243-th epoch val loss 1.2352860388978177\n",
      "244-th epoch train loss 1.274327060300988\n",
      "244-th epoch val loss 1.2345421688011768\n",
      "245-th epoch train loss 1.273480735452504\n",
      "245-th epoch val loss 1.233836609722029\n",
      "246-th epoch train loss 1.2726733297693995\n",
      "246-th epoch val loss 1.2331679338846884\n",
      "247-th epoch train loss 1.2719034368657183\n",
      "247-th epoch val loss 1.232534761577854\n",
      "248-th epoch train loss 1.2711696971253357\n",
      "248-th epoch val loss 1.231935759603687\n",
      "249-th epoch train loss 1.2704707962010984\n",
      "249-th epoch val loss 1.2313696397759641\n",
      "250-th epoch train loss 1.2698054635613363\n",
      "250-th epoch val loss 1.2308351574657654\n",
      "251-th epoch train loss 1.2691724710822523\n",
      "251-th epoch val loss 1.2303311101932108\n",
      "252-th epoch train loss 1.2685706316847638\n",
      "252-th epoch val loss 1.2298563362638006\n",
      "253-th epoch train loss 1.267998798014399\n",
      "253-th epoch val loss 1.229409713447961\n",
      "254-th epoch train loss 1.267455861162897\n",
      "254-th epoch val loss 1.2289901577024362\n",
      "255-th epoch train loss 1.2669407494302143\n",
      "255-th epoch val loss 1.2285966219322215\n",
      "256-th epoch train loss 1.2664524271256608\n",
      "256-th epoch val loss 1.2282280947917574\n",
      "257-th epoch train loss 1.2659898934069485\n",
      "257-th epoch val loss 1.2278835995241577\n",
      "258-th epoch train loss 1.2655521811559594\n",
      "258-th epoch val loss 1.2275621928372789\n",
      "259-th epoch train loss 1.2651383558900855\n",
      "259-th epoch val loss 1.2272629638154673\n",
      "260-th epoch train loss 1.2647475147080223\n",
      "260-th epoch val loss 1.2269850328658747\n",
      "261-th epoch train loss 1.2643787852689383\n",
      "261-th epoch val loss 1.2267275506982445\n",
      "262-th epoch train loss 1.2640313248039752\n",
      "262-th epoch val loss 1.2264896973371284\n",
      "263-th epoch train loss 1.2637043191590593\n",
      "263-th epoch val loss 1.2262706811655073\n",
      "264-th epoch train loss 1.2633969818680506\n",
      "264-th epoch val loss 1.2260697379988343\n",
      "265-th epoch train loss 1.263108553255268\n",
      "265-th epoch val loss 1.2258861301885404\n",
      "266-th epoch train loss 1.2628382995664782\n",
      "266-th epoch val loss 1.225719145754077\n",
      "267-th epoch train loss 1.2625855121274512\n",
      "267-th epoch val loss 1.225568097542604\n",
      "268-th epoch train loss 1.2623495065292134\n",
      "268-th epoch val loss 1.2254323224154446\n",
      "269-th epoch train loss 1.262129621839169\n",
      "269-th epoch val loss 1.2253111804604744\n",
      "270-th epoch train loss 1.2619252198372704\n",
      "270-th epoch val loss 1.2252040542296194\n",
      "271-th epoch train loss 1.2617356842764504\n",
      "271-th epoch val loss 1.2251103480006837\n",
      "272-th epoch train loss 1.261560420166563\n",
      "272-th epoch val loss 1.2250294870627323\n",
      "273-th epoch train loss 1.261398853081086\n",
      "273-th epoch val loss 1.2249609170242917\n",
      "274-th epoch train loss 1.2612504284858712\n",
      "274-th epoch val loss 1.2249041031436496\n",
      "275-th epoch train loss 1.2611146110892517\n",
      "275-th epoch val loss 1.2248585296805552\n",
      "276-th epoch train loss 1.2609908842128366\n",
      "276-th epoch val loss 1.2248236992686525\n",
      "277-th epoch train loss 1.260878749182339\n",
      "277-th epoch val loss 1.2247991323079859\n",
      "278-th epoch train loss 1.2607777247378094\n",
      "278-th epoch val loss 1.224784366376949\n",
      "279-th epoch train loss 1.2606873464626613\n",
      "279-th epoch val loss 1.2247789556630637\n",
      "280-th epoch train loss 1.2606071662309062\n",
      "280-th epoch val loss 1.2247824704119958\n",
      "281-th epoch train loss 1.2605367516720125\n",
      "281-th epoch val loss 1.2247944963942363\n",
      "282-th epoch train loss 1.2604756856528467\n",
      "282-th epoch val loss 1.2248146343888768\n",
      "283-th epoch train loss 1.2604235657761498\n",
      "283-th epoch val loss 1.2248424996839602\n",
      "284-th epoch train loss 1.2603800038950312\n",
      "284-th epoch val loss 1.224877721592868\n",
      "285-th epoch train loss 1.2603446256429793\n",
      "285-th epoch val loss 1.224919942986243\n",
      "286-th epoch train loss 1.260317069978894\n",
      "286-th epoch val loss 1.224968819838958\n",
      "287-th epoch train loss 1.2602969887466728\n",
      "287-th epoch val loss 1.225024020791656\n",
      "288-th epoch train loss 1.2602840462488891\n",
      "288-th epoch val loss 1.2250852267263965\n",
      "289-th epoch train loss 1.2602779188341204\n",
      "289-th epoch val loss 1.225152130355967\n",
      "290-th epoch train loss 1.2602782944974944\n",
      "290-th epoch val loss 1.2252244358264266\n",
      "291-th epoch train loss 1.2602848724940376\n",
      "291-th epoch val loss 1.225301858332462\n",
      "292-th epoch train loss 1.2602973629644207\n",
      "292-th epoch val loss 1.2253841237451526\n",
      "293-th epoch train loss 1.2603154865727144\n",
      "293-th epoch val loss 1.22547096825175\n",
      "294-th epoch train loss 1.2603389741557687\n",
      "294-th epoch val loss 1.2255621380070925\n",
      "295-th epoch train loss 1.260367566383857\n",
      "295-th epoch val loss 1.2256573887962885\n",
      "296-th epoch train loss 1.260401013432223\n",
      "296-th epoch val loss 1.2257564857083065\n",
      "297-th epoch train loss 1.2604390746631922\n",
      "297-th epoch val loss 1.225859202820133\n",
      "298-th epoch train loss 1.2604815183185059\n",
      "298-th epoch val loss 1.225965322891157\n",
      "299-th epoch train loss 1.2605281212215667\n",
      "299-th epoch val loss 1.2260746370674636\n",
      "300-th epoch train loss 1.2605786684892657\n",
      "300-th epoch val loss 1.2261869445957136\n",
      "301-th epoch train loss 1.2606329532531069\n",
      "301-th epoch val loss 1.2263020525463153\n",
      "302-th epoch train loss 1.2606907763893207\n",
      "302-th epoch val loss 1.2264197755455868\n",
      "303-th epoch train loss 1.2607519462576884\n",
      "303-th epoch val loss 1.2265399355166227\n",
      "304-th epoch train loss 1.260816278448801\n",
      "304-th epoch val loss 1.2266623614285952\n",
      "305-th epoch train loss 1.2608835955394848\n",
      "305-th epoch val loss 1.226786889054216\n",
      "306-th epoch train loss 1.2609537268561373\n",
      "306-th epoch val loss 1.2269133607350993\n",
      "307-th epoch train loss 1.2610265082457168\n",
      "307-th epoch val loss 1.2270416251547824\n",
      "308-th epoch train loss 1.2611017818541521\n",
      "308-th epoch val loss 1.2271715371191492\n",
      "309-th epoch train loss 1.261179395911931\n",
      "309-th epoch val loss 1.2273029573440288\n",
      "310-th epoch train loss 1.2612592045266404\n",
      "310-th epoch val loss 1.2274357522497392\n",
      "311-th epoch train loss 1.2613410674822385\n",
      "311-th epoch val loss 1.2275697937623546\n",
      "312-th epoch train loss 1.261424850044848\n",
      "312-th epoch val loss 1.2277049591214821\n",
      "313-th epoch train loss 1.2615104227748593\n",
      "313-th epoch val loss 1.227841130694338\n",
      "314-th epoch train loss 1.261597661345144\n",
      "314-th epoch val loss 1.2279781957959275\n",
      "315-th epoch train loss 1.2616864463651867\n",
      "315-th epoch val loss 1.2281160465151315\n",
      "316-th epoch train loss 1.2617766632109477\n",
      "316-th epoch val loss 1.228254579546509\n",
      "317-th epoch train loss 1.261868201860269\n",
      "317-th epoch val loss 1.2283936960276363\n",
      "318-th epoch train loss 1.2619609567336536\n",
      "318-th epoch val loss 1.2285333013818043\n",
      "319-th epoch train loss 1.2620548265402456\n",
      "319-th epoch val loss 1.2286733051659045\n",
      "320-th epoch train loss 1.26214971412884\n",
      "320-th epoch val loss 1.2288136209233338\n",
      "321-th epoch train loss 1.2622455263437722\n",
      "321-th epoch val loss 1.2289541660417636\n",
      "322-th epoch train loss 1.2623421738855236\n",
      "322-th epoch val loss 1.229094861615613\n",
      "323-th epoch train loss 1.2624395711758944\n",
      "323-th epoch val loss 1.2292356323130755\n",
      "324-th epoch train loss 1.2625376362276028\n",
      "324-th epoch val loss 1.2293764062475563\n",
      "325-th epoch train loss 1.2626362905181616\n",
      "325-th epoch val loss 1.2295171148533763\n",
      "326-th epoch train loss 1.2627354588679058\n",
      "326-th epoch val loss 1.2296576927656087\n",
      "327-th epoch train loss 1.2628350693220258\n",
      "327-th epoch val loss 1.2297980777039104\n",
      "328-th epoch train loss 1.2629350530364931\n",
      "328-th epoch val loss 1.2299382103602268\n",
      "329-th epoch train loss 1.2630353441677413\n",
      "329-th epoch val loss 1.2300780342902415\n",
      "330-th epoch train loss 1.2631358797659884\n",
      "330-th epoch val loss 1.2302174958084484\n",
      "331-th epoch train loss 1.2632365996720858\n",
      "331-th epoch val loss 1.230356543886736\n",
      "332-th epoch train loss 1.2633374464177751\n",
      "332-th epoch val loss 1.2304951300563673\n",
      "333-th epoch train loss 1.2634383651292502\n",
      "333-th epoch val loss 1.2306332083132427\n",
      "334-th epoch train loss 1.2635393034339137\n",
      "334-th epoch val loss 1.230770735026349\n",
      "335-th epoch train loss 1.2636402113702274\n",
      "335-th epoch val loss 1.230907668849281\n",
      "336-th epoch train loss 1.2637410413005592\n",
      "336-th epoch val loss 1.231043970634747\n",
      "337-th epoch train loss 1.263841747826926\n",
      "337-th epoch val loss 1.2311796033519533\n",
      "338-th epoch train loss 1.263942287709546\n",
      "338-th epoch val loss 1.2313145320067798\n",
      "339-th epoch train loss 1.2640426197881023\n",
      "339-th epoch val loss 1.2314487235646558\n",
      "340-th epoch train loss 1.2641427049056388\n",
      "340-th epoch val loss 1.231582146876047\n",
      "341-th epoch train loss 1.264242505834997\n",
      "341-th epoch val loss 1.231714772604471\n",
      "342-th epoch train loss 1.2643419872077164\n",
      "342-th epoch val loss 1.2318465731569581\n",
      "343-th epoch train loss 1.2644411154453177\n",
      "343-th epoch val loss 1.231977522616878\n",
      "344-th epoch train loss 1.2645398586928913\n",
      "344-th epoch val loss 1.2321075966790596\n",
      "345-th epoch train loss 1.2646381867549183\n",
      "345-th epoch val loss 1.2322367725871217\n",
      "346-th epoch train loss 1.264736071033251\n",
      "346-th epoch val loss 1.2323650290729529\n",
      "347-th epoch train loss 1.2648334844671827\n",
      "347-th epoch val loss 1.2324923462982569\n",
      "348-th epoch train loss 1.2649304014755425\n",
      "348-th epoch val loss 1.2326187057981173\n",
      "349-th epoch train loss 1.2650267979007441\n",
      "349-th epoch val loss 1.23274409042649\n",
      "350-th epoch train loss 1.265122650954732\n",
      "350-th epoch val loss 1.2328684843035829\n",
      "351-th epoch train loss 1.2652179391667582\n",
      "351-th epoch val loss 1.232991872765048\n",
      "352-th epoch train loss 1.2653126423329328\n",
      "352-th epoch val loss 1.2331142423129335\n",
      "353-th epoch train loss 1.2654067414674912\n",
      "353-th epoch val loss 1.2332355805683335\n",
      "354-th epoch train loss 1.265500218755721\n",
      "354-th epoch val loss 1.2333558762256842\n",
      "355-th epoch train loss 1.2655930575084953\n",
      "355-th epoch val loss 1.233475119008649\n",
      "356-th epoch train loss 1.2656852421183618\n",
      "356-th epoch val loss 1.233593299627546\n",
      "357-th epoch train loss 1.265776758017135\n",
      "357-th epoch val loss 1.2337104097382592\n",
      "358-th epoch train loss 1.2658675916349404\n",
      "358-th epoch val loss 1.2338264419025906\n",
      "359-th epoch train loss 1.2659577303606722\n",
      "359-th epoch val loss 1.2339413895500067\n",
      "360-th epoch train loss 1.2660471625038068\n",
      "360-th epoch val loss 1.2340552469407282\n",
      "361-th epoch train loss 1.2661358772575382\n",
      "361-th epoch val loss 1.2341680091301246\n",
      "362-th epoch train loss 1.266223864663186\n",
      "362-th epoch val loss 1.2342796719343658\n",
      "363-th epoch train loss 1.2663111155758349\n",
      "363-th epoch val loss 1.2343902318972921\n",
      "364-th epoch train loss 1.266397621631169\n",
      "364-th epoch val loss 1.2344996862584636\n",
      "365-th epoch train loss 1.2664833752134599\n",
      "365-th epoch val loss 1.2346080329223454\n",
      "366-th epoch train loss 1.2665683694246666\n",
      "366-th epoch val loss 1.2347152704285949\n",
      "367-th epoch train loss 1.2666525980546202\n",
      "367-th epoch val loss 1.2348213979234146\n",
      "368-th epoch train loss 1.2667360555522476\n",
      "368-th epoch val loss 1.2349264151319341\n",
      "369-th epoch train loss 1.2668187369978088\n",
      "369-th epoch val loss 1.2350303223315846\n",
      "370-th epoch train loss 1.2669006380761072\n",
      "370-th epoch val loss 1.2351331203264395\n",
      "371-th epoch train loss 1.2669817550506464\n",
      "371-th epoch val loss 1.2352348104224822\n",
      "372-th epoch train loss 1.2670620847386984\n",
      "372-th epoch val loss 1.2353353944037722\n",
      "373-th epoch train loss 1.2671416244872582\n",
      "373-th epoch val loss 1.2354348745094863\n",
      "374-th epoch train loss 1.267220372149848\n",
      "374-th epoch val loss 1.2355332534117913\n",
      "375-th epoch train loss 1.2672983260641506\n",
      "375-th epoch val loss 1.2356305341945353\n",
      "376-th epoch train loss 1.2673754850304402\n",
      "376-th epoch val loss 1.2357267203327211\n",
      "377-th epoch train loss 1.2674518482907842\n",
      "377-th epoch val loss 1.235821815672739\n",
      "378-th epoch train loss 1.2675274155089944\n",
      "378-th epoch val loss 1.235915824413333\n",
      "379-th epoch train loss 1.2676021867512954\n",
      "379-th epoch val loss 1.2360087510872733\n",
      "380-th epoch train loss 1.2676761624676958\n",
      "380-th epoch val loss 1.2361006005437163\n",
      "381-th epoch train loss 1.2677493434740308\n",
      "381-th epoch val loss 1.2361913779312266\n",
      "382-th epoch train loss 1.2678217309346573\n",
      "382-th epoch val loss 1.2362810886814326\n",
      "383-th epoch train loss 1.2678933263457801\n",
      "383-th epoch val loss 1.2363697384933081\n",
      "384-th epoch train loss 1.267964131519388\n",
      "384-th epoch val loss 1.236457333318044\n",
      "385-th epoch train loss 1.2680341485677764\n",
      "385-th epoch val loss 1.2365438793444987\n",
      "386-th epoch train loss 1.2681033798886434\n",
      "386-th epoch val loss 1.2366293829852073\n",
      "387-th epoch train loss 1.2681718281507321\n",
      "387-th epoch val loss 1.2367138508629274\n",
      "388-th epoch train loss 1.2682394962800057\n",
      "388-th epoch val loss 1.2367972897977\n",
      "389-th epoch train loss 1.268306387446339\n",
      "389-th epoch val loss 1.2368797067944233\n",
      "390-th epoch train loss 1.2683725050507026\n",
      "390-th epoch val loss 1.2369611090308983\n",
      "391-th epoch train loss 1.268437852712832\n",
      "391-th epoch val loss 1.237041503846355\n",
      "392-th epoch train loss 1.2685024342593532\n",
      "392-th epoch val loss 1.2371208987304183\n",
      "393-th epoch train loss 1.2685662537123648\n",
      "393-th epoch val loss 1.2371993013125229\n",
      "394-th epoch train loss 1.2686293152784462\n",
      "394-th epoch val loss 1.2372767193517384\n",
      "395-th epoch train loss 1.2686916233380896\n",
      "395-th epoch val loss 1.2373531607270118\n",
      "396-th epoch train loss 1.268753182435529\n",
      "396-th epoch val loss 1.237428633427794\n",
      "397-th epoch train loss 1.2688139972689674\n",
      "397-th epoch val loss 1.237503145545051\n",
      "398-th epoch train loss 1.268874072681176\n",
      "398-th epoch val loss 1.2375767052626403\n",
      "399-th epoch train loss 1.2689334136504584\n",
      "399-th epoch val loss 1.2376493208490396\n",
      "400-th epoch train loss 1.2689920252819675\n",
      "400-th epoch val loss 1.2377210006494168\n",
      "401-th epoch train loss 1.2690499127993622\n",
      "401-th epoch val loss 1.2377917530780316\n",
      "402-th epoch train loss 1.2691070815367895\n",
      "402-th epoch val loss 1.2378615866109501\n",
      "403-th epoch train loss 1.269163536931189\n",
      "403-th epoch val loss 1.2379305097790723\n",
      "404-th epoch train loss 1.2692192845148966\n",
      "404-th epoch val loss 1.2379985311614483\n",
      "405-th epoch train loss 1.2692743299085474\n",
      "405-th epoch val loss 1.2380656593788852\n",
      "406-th epoch train loss 1.2693286788142628\n",
      "406-th epoch val loss 1.2381319030878273\n",
      "407-th epoch train loss 1.2693823370091155\n",
      "407-th epoch val loss 1.2381972709745028\n",
      "408-th epoch train loss 1.2694353103388545\n",
      "408-th epoch val loss 1.2382617717493238\n",
      "409-th epoch train loss 1.2694876047118915\n",
      "409-th epoch val loss 1.2383254141415365\n",
      "410-th epoch train loss 1.2695392260935332\n",
      "410-th epoch val loss 1.2383882068941092\n",
      "411-th epoch train loss 1.2695901805004517\n",
      "411-th epoch val loss 1.2384501587588461\n",
      "412-th epoch train loss 1.2696404739953882\n",
      "412-th epoch val loss 1.238511278491729\n",
      "413-th epoch train loss 1.2696901126820752\n",
      "413-th epoch val loss 1.2385715748484663\n",
      "414-th epoch train loss 1.2697391027003773\n",
      "414-th epoch val loss 1.2386310565802514\n",
      "415-th epoch train loss 1.2697874502216357\n",
      "415-th epoch val loss 1.2386897324297195\n",
      "416-th epoch train loss 1.269835161444215\n",
      "416-th epoch val loss 1.23874761112709\n",
      "417-th epoch train loss 1.269882242589242\n",
      "417-th epoch val loss 1.2388047013865042\n",
      "418-th epoch train loss 1.2699286998965291\n",
      "418-th epoch val loss 1.2388610119025263\n",
      "419-th epoch train loss 1.26997453962068\n",
      "419-th epoch val loss 1.2389165513468263\n",
      "420-th epoch train loss 1.270019768027364\n",
      "420-th epoch val loss 1.2389713283650254\n",
      "421-th epoch train loss 1.2700643913897625\n",
      "421-th epoch val loss 1.2390253515736942\n",
      "422-th epoch train loss 1.2701084159851688\n",
      "422-th epoch val loss 1.2390786295575098\n",
      "423-th epoch train loss 1.2701518480917515\n",
      "423-th epoch val loss 1.2391311708665582\n",
      "424-th epoch train loss 1.2701946939854611\n",
      "424-th epoch val loss 1.2391829840137765\n",
      "425-th epoch train loss 1.270236959937081\n",
      "425-th epoch val loss 1.2392340774725328\n",
      "426-th epoch train loss 1.270278652209422\n",
      "426-th epoch val loss 1.2392844596743384\n",
      "427-th epoch train loss 1.270319777054643\n",
      "427-th epoch val loss 1.2393341390066814\n",
      "428-th epoch train loss 1.2703603407117088\n",
      "428-th epoch val loss 1.2393831238109898\n",
      "429-th epoch train loss 1.2704003494039646\n",
      "429-th epoch val loss 1.2394314223807015\n",
      "430-th epoch train loss 1.2704398093368359\n",
      "430-th epoch val loss 1.2394790429594553\n",
      "431-th epoch train loss 1.2704787266956388\n",
      "431-th epoch val loss 1.2395259937393848\n",
      "432-th epoch train loss 1.2705171076435038\n",
      "432-th epoch val loss 1.2395722828595166\n",
      "433-th epoch train loss 1.2705549583194076\n",
      "433-th epoch val loss 1.2396179184042708\n",
      "434-th epoch train loss 1.2705922848363038\n",
      "434-th epoch val loss 1.2396629084020556\n",
      "435-th epoch train loss 1.270629093279356\n",
      "435-th epoch val loss 1.2397072608239539\n",
      "436-th epoch train loss 1.2706653897042637\n",
      "436-th epoch val loss 1.2397509835824974\n",
      "437-th epoch train loss 1.270701180135683\n",
      "437-th epoch val loss 1.2397940845305306\n",
      "438-th epoch train loss 1.2707364705657311\n",
      "438-th epoch val loss 1.23983657146015\n",
      "439-th epoch train loss 1.2707712669525797\n",
      "439-th epoch val loss 1.239878452101726\n",
      "440-th epoch train loss 1.270805575219127\n",
      "440-th epoch val loss 1.2399197341229995\n",
      "441-th epoch train loss 1.2708394012517497\n",
      "441-th epoch val loss 1.2399604251282472\n",
      "442-th epoch train loss 1.2708727508991327\n",
      "442-th epoch val loss 1.2400005326575239\n",
      "443-th epoch train loss 1.270905629971165\n",
      "443-th epoch val loss 1.2400400641859617\n",
      "444-th epoch train loss 1.27093804423791\n",
      "444-th epoch val loss 1.2400790271231406\n",
      "445-th epoch train loss 1.2709699994286443\n",
      "445-th epoch val loss 1.2401174288125156\n",
      "446-th epoch train loss 1.271001501230956\n",
      "446-th epoch val loss 1.2401552765309043\n",
      "447-th epoch train loss 1.271032555289908\n",
      "447-th epoch val loss 1.240192577488031\n",
      "448-th epoch train loss 1.27106316720726\n",
      "448-th epoch val loss 1.240229338826123\n",
      "449-th epoch train loss 1.271093342540748\n",
      "449-th epoch val loss 1.2402655676195598\n",
      "450-th epoch train loss 1.271123086803417\n",
      "450-th epoch val loss 1.240301270874569\n",
      "451-th epoch train loss 1.2711524054630077\n",
      "451-th epoch val loss 1.240336455528974\n",
      "452-th epoch train loss 1.2711813039413928\n",
      "452-th epoch val loss 1.2403711284519807\n",
      "453-th epoch train loss 1.2712097876140633\n",
      "453-th epoch val loss 1.240405296444014\n",
      "454-th epoch train loss 1.271237861809657\n",
      "454-th epoch val loss 1.2404389662365871\n",
      "455-th epoch train loss 1.2712655318095365\n",
      "455-th epoch val loss 1.2404721444922193\n",
      "456-th epoch train loss 1.2712928028474069\n",
      "456-th epoch val loss 1.240504837804385\n",
      "457-th epoch train loss 1.271319680108975\n",
      "457-th epoch val loss 1.2405370526975001\n",
      "458-th epoch train loss 1.2713461687316463\n",
      "458-th epoch val loss 1.240568795626943\n",
      "459-th epoch train loss 1.2713722738042625\n",
      "459-th epoch val loss 1.2406000729791102\n",
      "460-th epoch train loss 1.271398000366871\n",
      "460-th epoch val loss 1.2406308910714963\n",
      "461-th epoch train loss 1.271423353410533\n",
      "461-th epoch val loss 1.2406612561528132\n",
      "462-th epoch train loss 1.2714483378771582\n",
      "462-th epoch val loss 1.2406911744031284\n",
      "463-th epoch train loss 1.2714729586593794\n",
      "463-th epoch val loss 1.2407206519340375\n",
      "464-th epoch train loss 1.271497220600448\n",
      "464-th epoch val loss 1.2407496947888548\n",
      "465-th epoch train loss 1.2715211284941654\n",
      "465-th epoch val loss 1.2407783089428355\n",
      "466-th epoch train loss 1.2715446870848377\n",
      "466-th epoch val loss 1.2408065003034163\n",
      "467-th epoch train loss 1.2715679010672565\n",
      "467-th epoch val loss 1.2408342747104775\n",
      "468-th epoch train loss 1.271590775086707\n",
      "468-th epoch val loss 1.2408616379366282\n",
      "469-th epoch train loss 1.2716133137389989\n",
      "469-th epoch val loss 1.2408885956875109\n",
      "470-th epoch train loss 1.2716355215705182\n",
      "470-th epoch val loss 1.2409151536021226\n",
      "471-th epoch train loss 1.271657403078303\n",
      "471-th epoch val loss 1.2409413172531532\n",
      "472-th epoch train loss 1.2716789627101381\n",
      "472-th epoch val loss 1.240967092147346\n",
      "473-th epoch train loss 1.2717002048646702\n",
      "473-th epoch val loss 1.240992483725864\n",
      "474-th epoch train loss 1.2717211338915402\n",
      "474-th epoch val loss 1.2410174973646801\n",
      "475-th epoch train loss 1.2717417540915366\n",
      "475-th epoch val loss 1.2410421383749772\n",
      "476-th epoch train loss 1.2717620697167604\n",
      "476-th epoch val loss 1.2410664120035604\n",
      "477-th epoch train loss 1.2717820849708112\n",
      "477-th epoch val loss 1.2410903234332815\n",
      "478-th epoch train loss 1.2718018040089831\n",
      "478-th epoch val loss 1.241113877783478\n",
      "479-th epoch train loss 1.2718212309384822\n",
      "479-th epoch val loss 1.2411370801104207\n",
      "480-th epoch train loss 1.2718403698186478\n",
      "480-th epoch val loss 1.2411599354077691\n",
      "481-th epoch train loss 1.2718592246611966\n",
      "481-th epoch val loss 1.2411824486070404\n",
      "482-th epoch train loss 1.271877799430471\n",
      "482-th epoch val loss 1.2412046245780837\n",
      "483-th epoch train loss 1.271896098043703\n",
      "483-th epoch val loss 1.2412264681295642\n",
      "484-th epoch train loss 1.2719141243712881\n",
      "484-th epoch val loss 1.2412479840094532\n",
      "485-th epoch train loss 1.2719318822370687\n",
      "485-th epoch val loss 1.2412691769055266\n",
      "486-th epoch train loss 1.2719493754186266\n",
      "486-th epoch val loss 1.241290051445868\n",
      "487-th epoch train loss 1.271966607647587\n",
      "487-th epoch val loss 1.2413106121993787\n",
      "488-th epoch train loss 1.2719835826099275\n",
      "488-th epoch val loss 1.2413308636762934\n",
      "489-th epoch train loss 1.2720003039462968\n",
      "489-th epoch val loss 1.241350810328698\n",
      "490-th epoch train loss 1.2720167752523401\n",
      "490-th epoch val loss 1.241370456551055\n",
      "491-th epoch train loss 1.2720330000790316\n",
      "491-th epoch val loss 1.2413898066807296\n",
      "492-th epoch train loss 1.2720489819330163\n",
      "492-th epoch val loss 1.241408864998522\n",
      "493-th epoch train loss 1.2720647242769483\n",
      "493-th epoch val loss 1.241427635729201\n",
      "494-th epoch train loss 1.2720802305298473\n",
      "494-th epoch val loss 1.2414461230420388\n",
      "495-th epoch train loss 1.2720955040674509\n",
      "495-th epoch val loss 1.2414643310513533\n",
      "496-th epoch train loss 1.272110548222574\n",
      "496-th epoch val loss 1.2414822638170449\n",
      "497-th epoch train loss 1.2721253662854743\n",
      "497-th epoch val loss 1.241499925345139\n",
      "498-th epoch train loss 1.27213996150422\n",
      "498-th epoch val loss 1.2415173195883331\n",
      "499-th epoch train loss 1.2721543370850583\n",
      "499-th epoch val loss 1.241534450446533\n",
      "500-th epoch train loss 1.2721684961927957\n",
      "500-th epoch val loss 1.2415513217674057\n",
      "501-th epoch train loss 1.2721824419511694\n",
      "501-th epoch val loss 1.2415679373469173\n",
      "502-th epoch train loss 1.272196177443232\n",
      "502-th epoch val loss 1.241584300929882\n",
      "503-th epoch train loss 1.272209705711729\n",
      "503-th epoch val loss 1.2416004162105025\n",
      "504-th epoch train loss 1.2722230297594872\n",
      "504-th epoch val loss 1.2416162868329184\n",
      "505-th epoch train loss 1.2722361525497987\n",
      "505-th epoch val loss 1.2416319163917446\n",
      "506-th epoch train loss 1.2722490770068062\n",
      "506-th epoch val loss 1.2416473084326163\n",
      "507-th epoch train loss 1.2722618060158941\n",
      "507-th epoch val loss 1.2416624664527276\n",
      "508-th epoch train loss 1.2722743424240759\n",
      "508-th epoch val loss 1.2416773939013717\n",
      "509-th epoch train loss 1.2722866890403841\n",
      "509-th epoch val loss 1.241692094180479\n",
      "510-th epoch train loss 1.2722988486362614\n",
      "510-th epoch val loss 1.2417065706451518\n",
      "511-th epoch train loss 1.2723108239459504\n",
      "511-th epoch val loss 1.2417208266041975\n",
      "512-th epoch train loss 1.2723226176668836\n",
      "512-th epoch val loss 1.2417348653206632\n",
      "513-th epoch train loss 1.2723342324600748\n",
      "513-th epoch val loss 1.24174869001236\n",
      "514-th epoch train loss 1.2723456709505077\n",
      "514-th epoch val loss 1.2417623038523944\n",
      "515-th epoch train loss 1.2723569357275282\n",
      "515-th epoch val loss 1.2417757099696902\n",
      "516-th epoch train loss 1.272368029345229\n",
      "516-th epoch val loss 1.2417889114495102\n",
      "517-th epoch train loss 1.2723789543228416\n",
      "517-th epoch val loss 1.2418019113339749\n",
      "518-th epoch train loss 1.2723897131451205\n",
      "518-th epoch val loss 1.241814712622577\n",
      "519-th epoch train loss 1.2724003082627298\n",
      "519-th epoch val loss 1.2418273182726958\n",
      "520-th epoch train loss 1.2724107420926292\n",
      "520-th epoch val loss 1.2418397312001048\n",
      "521-th epoch train loss 1.2724210170184547\n",
      "521-th epoch val loss 1.2418519542794777\n",
      "522-th epoch train loss 1.2724311353909028\n",
      "522-th epoch val loss 1.2418639903448925\n",
      "523-th epoch train loss 1.2724410995281077\n",
      "523-th epoch val loss 1.2418758421903273\n",
      "524-th epoch train loss 1.2724509117160223\n",
      "524-th epoch val loss 1.241887512570158\n",
      "525-th epoch train loss 1.2724605742087935\n",
      "525-th epoch val loss 1.241899004199651\n",
      "526-th epoch train loss 1.2724700892291365\n",
      "526-th epoch val loss 1.2419103197554482\n",
      "527-th epoch train loss 1.2724794589687074\n",
      "527-th epoch val loss 1.2419214618760546\n",
      "528-th epoch train loss 1.2724886855884738\n",
      "528-th epoch val loss 1.2419324331623145\n",
      "529-th epoch train loss 1.2724977712190826\n",
      "529-th epoch val loss 1.2419432361778926\n",
      "530-th epoch train loss 1.2725067179612268\n",
      "530-th epoch val loss 1.2419538734497442\n",
      "531-th epoch train loss 1.272515527886008\n",
      "531-th epoch val loss 1.2419643474685829\n",
      "532-th epoch train loss 1.2725242030352961\n",
      "532-th epoch val loss 1.2419746606893465\n",
      "533-th epoch train loss 1.2725327454220927\n",
      "533-th epoch val loss 1.2419848155316582\n",
      "534-th epoch train loss 1.272541157030882\n",
      "534-th epoch val loss 1.2419948143802801\n",
      "535-th epoch train loss 1.2725494398179866\n",
      "535-th epoch val loss 1.242004659585568\n",
      "536-th epoch train loss 1.272557595711919\n",
      "536-th epoch val loss 1.2420143534639185\n",
      "537-th epoch train loss 1.272565626613728\n",
      "537-th epoch val loss 1.2420238982982121\n",
      "538-th epoch train loss 1.2725735343973454\n",
      "538-th epoch val loss 1.2420332963382548\n",
      "539-th epoch train loss 1.2725813209099277\n",
      "539-th epoch val loss 1.2420425498012093\n",
      "540-th epoch train loss 1.2725889879721966\n",
      "540-th epoch val loss 1.2420516608720318\n",
      "541-th epoch train loss 1.2725965373787755\n",
      "541-th epoch val loss 1.2420606317038938\n",
      "542-th epoch train loss 1.272603970898523\n",
      "542-th epoch val loss 1.2420694644186059\n",
      "543-th epoch train loss 1.272611290274866\n",
      "543-th epoch val loss 1.2420781611070386\n",
      "544-th epoch train loss 1.2726184972261254\n",
      "544-th epoch val loss 1.242086723829534\n",
      "545-th epoch train loss 1.2726255934458435\n",
      "545-th epoch val loss 1.2420951546163137\n",
      "546-th epoch train loss 1.2726325806031038\n",
      "546-th epoch val loss 1.2421034554678891\n",
      "547-th epoch train loss 1.2726394603428524\n",
      "547-th epoch val loss 1.2421116283554572\n",
      "548-th epoch train loss 1.2726462342862144\n",
      "548-th epoch val loss 1.242119675221301\n",
      "549-th epoch train loss 1.2726529040308028\n",
      "549-th epoch val loss 1.24212759797918\n",
      "550-th epoch train loss 1.2726594711510357\n",
      "550-th epoch val loss 1.242135398514719\n",
      "551-th epoch train loss 1.2726659371984366\n",
      "551-th epoch val loss 1.2421430786857923\n",
      "552-th epoch train loss 1.2726723037019425\n",
      "552-th epoch val loss 1.2421506403229037\n",
      "553-th epoch train loss 1.2726785721682032\n",
      "553-th epoch val loss 1.24215808522956\n",
      "554-th epoch train loss 1.2726847440818798\n",
      "554-th epoch val loss 1.2421654151826451\n",
      "555-th epoch train loss 1.2726908209059389\n",
      "555-th epoch val loss 1.2421726319327842\n",
      "556-th epoch train loss 1.2726968040819446\n",
      "556-th epoch val loss 1.242179737204708\n",
      "557-th epoch train loss 1.2727026950303468\n",
      "557-th epoch val loss 1.2421867326976126\n",
      "558-th epoch train loss 1.2727084951507672\n",
      "558-th epoch val loss 1.2421936200855106\n",
      "559-th epoch train loss 1.2727142058222807\n",
      "559-th epoch val loss 1.2422004010175836\n",
      "560-th epoch train loss 1.2727198284036976\n",
      "560-th epoch val loss 1.242207077118529\n",
      "561-th epoch train loss 1.272725364233835\n",
      "561-th epoch val loss 1.2422136499888992\n",
      "562-th epoch train loss 1.2727308146317955\n",
      "562-th epoch val loss 1.2422201212054442\n",
      "563-th epoch train loss 1.2727361808972333\n",
      "563-th epoch val loss 1.2422264923214403\n",
      "564-th epoch train loss 1.272741464310623\n",
      "564-th epoch val loss 1.2422327648670235\n",
      "565-th epoch train loss 1.272746666133525\n",
      "565-th epoch val loss 1.2422389403495149\n",
      "566-th epoch train loss 1.2727517876088426\n",
      "566-th epoch val loss 1.2422450202537405\n",
      "567-th epoch train loss 1.272756829961086\n",
      "567-th epoch val loss 1.2422510060423533\n",
      "568-th epoch train loss 1.2727617943966218\n",
      "568-th epoch val loss 1.2422568991561422\n",
      "569-th epoch train loss 1.2727666821039292\n",
      "569-th epoch val loss 1.2422627010143472\n",
      "570-th epoch train loss 1.2727714942538466\n",
      "570-th epoch val loss 1.2422684130149626\n",
      "571-th epoch train loss 1.272776231999819\n",
      "571-th epoch val loss 1.2422740365350393\n",
      "572-th epoch train loss 1.2727808964781417\n",
      "572-th epoch val loss 1.2422795729309855\n",
      "573-th epoch train loss 1.2727854888082\n",
      "573-th epoch val loss 1.242285023538861\n",
      "574-th epoch train loss 1.2727900100927065\n",
      "574-th epoch val loss 1.242290389674666\n",
      "575-th epoch train loss 1.2727944614179363\n",
      "575-th epoch val loss 1.2422956726346328\n",
      "576-th epoch train loss 1.27279884385396\n",
      "576-th epoch val loss 1.242300873695506\n",
      "577-th epoch train loss 1.272803158454869\n",
      "577-th epoch val loss 1.242305994114824\n",
      "578-th epoch train loss 1.2728074062590058\n",
      "578-th epoch val loss 1.2423110351311957\n",
      "579-th epoch train loss 1.2728115882891848\n",
      "579-th epoch val loss 1.242315997964573\n",
      "580-th epoch train loss 1.272815705552914\n",
      "580-th epoch val loss 1.2423208838165205\n",
      "581-th epoch train loss 1.272819759042612\n",
      "581-th epoch val loss 1.2423256938704816\n",
      "582-th epoch train loss 1.2728237497358235\n",
      "582-th epoch val loss 1.2423304292920399\n",
      "583-th epoch train loss 1.2728276785954316\n",
      "583-th epoch val loss 1.2423350912291786\n",
      "584-th epoch train loss 1.2728315465698679\n",
      "584-th epoch val loss 1.242339680812537\n",
      "585-th epoch train loss 1.2728353545933186\n",
      "585-th epoch val loss 1.2423441991556619\n",
      "586-th epoch train loss 1.272839103585929\n",
      "586-th epoch val loss 1.2423486473552556\n",
      "587-th epoch train loss 1.2728427944540075\n",
      "587-th epoch val loss 1.2423530264914233\n",
      "588-th epoch train loss 1.2728464280902207\n",
      "588-th epoch val loss 1.2423573376279125\n",
      "589-th epoch train loss 1.2728500053737928\n",
      "589-th epoch val loss 1.242361581812355\n",
      "590-th epoch train loss 1.2728535271706998\n",
      "590-th epoch val loss 1.2423657600765017\n",
      "591-th epoch train loss 1.272856994333859\n",
      "591-th epoch val loss 1.242369873436454\n",
      "592-th epoch train loss 1.2728604077033197\n",
      "592-th epoch val loss 1.2423739228928938\n",
      "593-th epoch train loss 1.272863768106449\n",
      "593-th epoch val loss 1.242377909431311\n",
      "594-th epoch train loss 1.272867076358116\n",
      "594-th epoch val loss 1.2423818340222241\n",
      "595-th epoch train loss 1.2728703332608728\n",
      "595-th epoch val loss 1.2423856976214045\n",
      "596-th epoch train loss 1.2728735396051358\n",
      "596-th epoch val loss 1.2423895011700898\n",
      "597-th epoch train loss 1.2728766961693592\n",
      "597-th epoch val loss 1.2423932455951991\n",
      "598-th epoch train loss 1.2728798037202118\n",
      "598-th epoch val loss 1.242396931809546\n",
      "599-th epoch train loss 1.2728828630127498\n",
      "599-th epoch val loss 1.2424005607120447\n",
      "600-th epoch train loss 1.272885874790584\n",
      "600-th epoch val loss 1.2424041331879176\n",
      "601-th epoch train loss 1.2728888397860496\n",
      "601-th epoch val loss 1.242407650108896\n",
      "602-th epoch train loss 1.2728917587203696\n",
      "602-th epoch val loss 1.2424111123334203\n",
      "603-th epoch train loss 1.2728946323038202\n",
      "603-th epoch val loss 1.2424145207068384\n",
      "604-th epoch train loss 1.272897461235889\n",
      "604-th epoch val loss 1.2424178760615978\n",
      "605-th epoch train loss 1.2729002462054346\n",
      "605-th epoch val loss 1.24242117921744\n",
      "606-th epoch train loss 1.272902987890846\n",
      "606-th epoch val loss 1.2424244309815886\n",
      "607-th epoch train loss 1.272905686960191\n",
      "607-th epoch val loss 1.2424276321489325\n",
      "608-th epoch train loss 1.272908344071373\n",
      "608-th epoch val loss 1.2424307835022137\n",
      "609-th epoch train loss 1.272910959872281\n",
      "609-th epoch val loss 1.2424338858122066\n",
      "610-th epoch train loss 1.272913535000933\n",
      "610-th epoch val loss 1.2424369398378954\n",
      "611-th epoch train loss 1.2729160700856295\n",
      "611-th epoch val loss 1.2424399463266529\n",
      "612-th epoch train loss 1.272918565745091\n",
      "612-th epoch val loss 1.2424429060144122\n",
      "613-th epoch train loss 1.2729210225886023\n",
      "613-th epoch val loss 1.2424458196258372\n",
      "614-th epoch train loss 1.272923441216152\n",
      "614-th epoch val loss 1.2424486878744911\n",
      "615-th epoch train loss 1.2729258222185733\n",
      "615-th epoch val loss 1.2424515114630066\n",
      "616-th epoch train loss 1.2729281661776766\n",
      "616-th epoch val loss 1.2424542910832435\n",
      "617-th epoch train loss 1.2729304736663833\n",
      "617-th epoch val loss 1.2424570274164557\n",
      "618-th epoch train loss 1.2729327452488624\n",
      "618-th epoch val loss 1.2424597211334458\n",
      "619-th epoch train loss 1.272934981480657\n",
      "619-th epoch val loss 1.2424623728947268\n",
      "620-th epoch train loss 1.272937182908814\n",
      "620-th epoch val loss 1.242464983350673\n",
      "621-th epoch train loss 1.2729393500720123\n",
      "621-th epoch val loss 1.2424675531416731\n",
      "622-th epoch train loss 1.2729414835006854\n",
      "622-th epoch val loss 1.2424700828982826\n",
      "623-th epoch train loss 1.2729435837171468\n",
      "623-th epoch val loss 1.2424725732413695\n",
      "624-th epoch train loss 1.2729456512357098\n",
      "624-th epoch val loss 1.2424750247822611\n",
      "625-th epoch train loss 1.2729476865628093\n",
      "625-th epoch val loss 1.242477438122889\n",
      "626-th epoch train loss 1.2729496901971171\n",
      "626-th epoch val loss 1.2424798138559274\n",
      "627-th epoch train loss 1.2729516626296609\n",
      "627-th epoch val loss 1.242482152564939\n",
      "628-th epoch train loss 1.2729536043439378\n",
      "628-th epoch val loss 1.2424844548245049\n",
      "629-th epoch train loss 1.2729555158160268\n",
      "629-th epoch val loss 1.242486721200367\n",
      "630-th epoch train loss 1.2729573975147017\n",
      "630-th epoch val loss 1.2424889522495572\n",
      "631-th epoch train loss 1.2729592499015396\n",
      "631-th epoch val loss 1.2424911485205326\n",
      "632-th epoch train loss 1.2729610734310302\n",
      "632-th epoch val loss 1.2424933105533023\n",
      "633-th epoch train loss 1.272962868550681\n",
      "633-th epoch val loss 1.2424954388795573\n",
      "634-th epoch train loss 1.272964635701125\n",
      "634-th epoch val loss 1.2424975340227957\n",
      "635-th epoch train loss 1.2729663753162221\n",
      "635-th epoch val loss 1.2424995964984475\n",
      "636-th epoch train loss 1.2729680878231624\n",
      "636-th epoch val loss 1.2425016268139975\n",
      "637-th epoch train loss 1.272969773642565\n",
      "637-th epoch val loss 1.2425036254691035\n",
      "638-th epoch train loss 1.272971433188581\n",
      "638-th epoch val loss 1.2425055929557178\n",
      "639-th epoch train loss 1.2729730668689878\n",
      "639-th epoch val loss 1.2425075297582038\n",
      "640-th epoch train loss 1.272974675085286\n",
      "640-th epoch val loss 1.2425094363534497\n",
      "641-th epoch train loss 1.2729762582327966\n",
      "641-th epoch val loss 1.2425113132109837\n",
      "642-th epoch train loss 1.2729778167007513\n",
      "642-th epoch val loss 1.242513160793087\n",
      "643-th epoch train loss 1.2729793508723868\n",
      "643-th epoch val loss 1.2425149795549\n",
      "644-th epoch train loss 1.272980861125036\n",
      "644-th epoch val loss 1.2425167699445354\n",
      "645-th epoch train loss 1.2729823478302158\n",
      "645-th epoch val loss 1.242518532403185\n",
      "646-th epoch train loss 1.2729838113537164\n",
      "646-th epoch val loss 1.2425202673652216\n",
      "647-th epoch train loss 1.2729852520556877\n",
      "647-th epoch val loss 1.2425219752583054\n",
      "648-th epoch train loss 1.272986670290726\n",
      "648-th epoch val loss 1.2425236565034885\n",
      "649-th epoch train loss 1.2729880664079574\n",
      "649-th epoch val loss 1.2425253115153112\n",
      "650-th epoch train loss 1.2729894407511204\n",
      "650-th epoch val loss 1.2425269407019044\n",
      "651-th epoch train loss 1.27299079365865\n",
      "651-th epoch val loss 1.2425285444650873\n",
      "652-th epoch train loss 1.2729921254637555\n",
      "652-th epoch val loss 1.2425301232004633\n",
      "653-th epoch train loss 1.272993436494503\n",
      "653-th epoch val loss 1.2425316772975152\n",
      "654-th epoch train loss 1.2729947270738908\n",
      "654-th epoch val loss 1.2425332071396982\n",
      "655-th epoch train loss 1.2729959975199288\n",
      "655-th epoch val loss 1.2425347131045323\n",
      "656-th epoch train loss 1.2729972481457144\n",
      "656-th epoch val loss 1.2425361955636953\n",
      "657-th epoch train loss 1.272998479259505\n",
      "657-th epoch val loss 1.2425376548831089\n",
      "658-th epoch train loss 1.2729996911647958\n",
      "658-th epoch val loss 1.2425390914230279\n",
      "659-th epoch train loss 1.2730008841603877\n",
      "659-th epoch val loss 1.242540505538129\n",
      "660-th epoch train loss 1.2730020585404638\n",
      "660-th epoch val loss 1.242541897577593\n",
      "661-th epoch train loss 1.2730032145946568\n",
      "661-th epoch val loss 1.2425432678851926\n",
      "662-th epoch train loss 1.273004352608119\n",
      "662-th epoch val loss 1.2425446167993726\n",
      "663-th epoch train loss 1.273005472861592\n",
      "663-th epoch val loss 1.2425459446533322\n",
      "664-th epoch train loss 1.2730065756314723\n",
      "664-th epoch val loss 1.242547251775107\n",
      "665-th epoch train loss 1.27300766118988\n",
      "665-th epoch val loss 1.2425485384876465\n",
      "666-th epoch train loss 1.273008729804722\n",
      "666-th epoch val loss 1.242549805108892\n",
      "667-th epoch train loss 1.2730097817397579\n",
      "667-th epoch val loss 1.2425510519518566\n",
      "668-th epoch train loss 1.2730108172546633\n",
      "668-th epoch val loss 1.2425522793246961\n",
      "669-th epoch train loss 1.2730118366050913\n",
      "669-th epoch val loss 1.2425534875307884\n",
      "670-th epoch train loss 1.273012840042736\n",
      "670-th epoch val loss 1.2425546768688027\n",
      "671-th epoch train loss 1.2730138278153926\n",
      "671-th epoch val loss 1.242555847632777\n",
      "672-th epoch train loss 1.2730148001670147\n",
      "672-th epoch val loss 1.2425570001121828\n",
      "673-th epoch train loss 1.2730157573377776\n",
      "673-th epoch val loss 1.2425581345920012\n",
      "674-th epoch train loss 1.2730166995641323\n",
      "674-th epoch val loss 1.242559251352789\n",
      "675-th epoch train loss 1.273017627078866\n",
      "675-th epoch val loss 1.2425603506707485\n",
      "676-th epoch train loss 1.273018540111155\n",
      "676-th epoch val loss 1.2425614328177916\n",
      "677-th epoch train loss 1.2730194388866225\n",
      "677-th epoch val loss 1.2425624980616095\n",
      "678-th epoch train loss 1.2730203236273936\n",
      "678-th epoch val loss 1.2425635466657357\n",
      "679-th epoch train loss 1.2730211945521464\n",
      "679-th epoch val loss 1.2425645788896102\n",
      "680-th epoch train loss 1.2730220518761672\n",
      "680-th epoch val loss 1.242565594988643\n",
      "681-th epoch train loss 1.2730228958114025\n",
      "681-th epoch val loss 1.2425665952142766\n",
      "682-th epoch train loss 1.2730237265665092\n",
      "682-th epoch val loss 1.2425675798140452\n",
      "683-th epoch train loss 1.2730245443469062\n",
      "683-th epoch val loss 1.2425685490316383\n",
      "684-th epoch train loss 1.273025349354824\n",
      "684-th epoch val loss 1.2425695031069577\n",
      "685-th epoch train loss 1.273026141789354\n",
      "685-th epoch val loss 1.2425704422761754\n",
      "686-th epoch train loss 1.2730269218464962\n",
      "686-th epoch val loss 1.242571366771793\n",
      "687-th epoch train loss 1.273027689719207\n",
      "687-th epoch val loss 1.242572276822698\n",
      "688-th epoch train loss 1.2730284455974465\n",
      "688-th epoch val loss 1.2425731726542188\n",
      "689-th epoch train loss 1.2730291896682249\n",
      "689-th epoch val loss 1.24257405448818\n",
      "690-th epoch train loss 1.273029922115647\n",
      "690-th epoch val loss 1.242574922542956\n",
      "691-th epoch train loss 1.273030643120957\n",
      "691-th epoch val loss 1.2425757770335264\n",
      "692-th epoch train loss 1.273031352862584\n",
      "692-th epoch val loss 1.2425766181715259\n",
      "693-th epoch train loss 1.2730320515161844\n",
      "693-th epoch val loss 1.2425774461652968\n",
      "694-th epoch train loss 1.2730327392546847\n",
      "694-th epoch val loss 1.2425782612199416\n",
      "695-th epoch train loss 1.2730334162483234\n",
      "695-th epoch val loss 1.2425790635373704\n",
      "696-th epoch train loss 1.2730340826646933\n",
      "696-th epoch val loss 1.2425798533163526\n",
      "697-th epoch train loss 1.2730347386687815\n",
      "697-th epoch val loss 1.2425806307525633\n",
      "698-th epoch train loss 1.2730353844230098\n",
      "698-th epoch val loss 1.2425813960386332\n",
      "699-th epoch train loss 1.273036020087276\n",
      "699-th epoch val loss 1.2425821493641955\n",
      "700-th epoch train loss 1.2730366458189895\n",
      "700-th epoch val loss 1.2425828909159298\n",
      "701-th epoch train loss 1.273037261773112\n",
      "701-th epoch val loss 1.2425836208776113\n",
      "702-th epoch train loss 1.2730378681021968\n",
      "702-th epoch val loss 1.2425843394301532\n",
      "703-th epoch train loss 1.2730384649564215\n",
      "703-th epoch val loss 1.2425850467516526\n",
      "704-th epoch train loss 1.2730390524836301\n",
      "704-th epoch val loss 1.2425857430174334\n",
      "705-th epoch train loss 1.2730396308293646\n",
      "705-th epoch val loss 1.2425864284000894\n",
      "706-th epoch train loss 1.2730402001369028\n",
      "706-th epoch val loss 1.2425871030695261\n",
      "707-th epoch train loss 1.273040760547293\n",
      "707-th epoch val loss 1.2425877671930028\n",
      "708-th epoch train loss 1.27304131219939\n",
      "708-th epoch val loss 1.2425884209351754\n",
      "709-th epoch train loss 1.2730418552298843\n",
      "709-th epoch val loss 1.2425890644581323\n",
      "710-th epoch train loss 1.273042389773342\n",
      "710-th epoch val loss 1.2425896979214388\n",
      "711-th epoch train loss 1.273042915962233\n",
      "711-th epoch val loss 1.2425903214821739\n",
      "712-th epoch train loss 1.2730434339269654\n",
      "712-th epoch val loss 1.2425909352949691\n",
      "713-th epoch train loss 1.2730439437959156\n",
      "713-th epoch val loss 1.2425915395120462\n",
      "714-th epoch train loss 1.273044445695463\n",
      "714-th epoch val loss 1.2425921342832567\n",
      "715-th epoch train loss 1.273044939750019\n",
      "715-th epoch val loss 1.2425927197561144\n",
      "716-th epoch train loss 1.2730454260820556\n",
      "716-th epoch val loss 1.2425932960758364\n",
      "717-th epoch train loss 1.2730459048121392\n",
      "717-th epoch val loss 1.2425938633853744\n",
      "718-th epoch train loss 1.2730463760589585\n",
      "718-th epoch val loss 1.2425944218254545\n",
      "719-th epoch train loss 1.2730468399393524\n",
      "719-th epoch val loss 1.2425949715346076\n",
      "720-th epoch train loss 1.2730472965683404\n",
      "720-th epoch val loss 1.2425955126492052\n",
      "721-th epoch train loss 1.2730477460591496\n",
      "721-th epoch val loss 1.2425960453034932\n",
      "722-th epoch train loss 1.273048188523244\n",
      "722-th epoch val loss 1.242596569629625\n",
      "723-th epoch train loss 1.2730486240703498\n",
      "723-th epoch val loss 1.2425970857576925\n",
      "724-th epoch train loss 1.2730490528084846\n",
      "724-th epoch val loss 1.2425975938157603\n",
      "725-th epoch train loss 1.2730494748439818\n",
      "725-th epoch val loss 1.2425980939298953\n",
      "726-th epoch train loss 1.2730498902815184\n",
      "726-th epoch val loss 1.2425985862241984\n",
      "727-th epoch train loss 1.2730502992241395\n",
      "727-th epoch val loss 1.2425990708208363\n",
      "728-th epoch train loss 1.2730507017732842\n",
      "728-th epoch val loss 1.2425995478400693\n",
      "729-th epoch train loss 1.2730510980288106\n",
      "729-th epoch val loss 1.2426000174002823\n",
      "730-th epoch train loss 1.27305148808902\n",
      "730-th epoch val loss 1.2426004796180137\n",
      "731-th epoch train loss 1.2730518720506818\n",
      "731-th epoch val loss 1.242600934607985\n",
      "732-th epoch train loss 1.2730522500090562\n",
      "732-th epoch val loss 1.2426013824831261\n",
      "733-th epoch train loss 1.273052622057919\n",
      "733-th epoch val loss 1.242601823354608\n",
      "734-th epoch train loss 1.2730529882895827\n",
      "734-th epoch val loss 1.2426022573318651\n",
      "735-th epoch train loss 1.2730533487949207\n",
      "735-th epoch val loss 1.2426026845226248\n",
      "736-th epoch train loss 1.2730537036633904\n",
      "736-th epoch val loss 1.2426031050329354\n",
      "737-th epoch train loss 1.2730540529830527\n",
      "737-th epoch val loss 1.2426035189671882\n",
      "738-th epoch train loss 1.2730543968405965\n",
      "738-th epoch val loss 1.2426039264281474\n",
      "739-th epoch train loss 1.2730547353213575\n",
      "739-th epoch val loss 1.242604327516972\n",
      "740-th epoch train loss 1.2730550685093414\n",
      "740-th epoch val loss 1.2426047223332444\n",
      "741-th epoch train loss 1.2730553964872426\n",
      "741-th epoch val loss 1.2426051109749918\n",
      "742-th epoch train loss 1.2730557193364662\n",
      "742-th epoch val loss 1.2426054935387114\n",
      "743-th epoch train loss 1.273056037137148\n",
      "743-th epoch val loss 1.242605870119395\n",
      "744-th epoch train loss 1.273056349968172\n",
      "744-th epoch val loss 1.242606240810551\n",
      "745-th epoch train loss 1.273056657907193\n",
      "745-th epoch val loss 1.2426066057042287\n",
      "746-th epoch train loss 1.2730569610306541\n",
      "746-th epoch val loss 1.242606964891041\n",
      "747-th epoch train loss 1.2730572594138043\n",
      "747-th epoch val loss 1.2426073184601851\n",
      "748-th epoch train loss 1.2730575531307202\n",
      "748-th epoch val loss 1.2426076664994674\n",
      "749-th epoch train loss 1.2730578422543217\n",
      "749-th epoch val loss 1.2426080090953229\n",
      "750-th epoch train loss 1.273058126856391\n",
      "750-th epoch val loss 1.2426083463328361\n",
      "751-th epoch train loss 1.2730584070075894\n",
      "751-th epoch val loss 1.2426086782957648\n",
      "752-th epoch train loss 1.2730586827774766\n",
      "752-th epoch val loss 1.2426090050665584\n",
      "753-th epoch train loss 1.2730589542345259\n",
      "753-th epoch val loss 1.2426093267263798\n",
      "754-th epoch train loss 1.2730592214461425\n",
      "754-th epoch val loss 1.242609643355123\n",
      "755-th epoch train loss 1.273059484478678\n",
      "755-th epoch val loss 1.2426099550314345\n",
      "756-th epoch train loss 1.273059743397451\n",
      "756-th epoch val loss 1.2426102618327357\n",
      "757-th epoch train loss 1.273059998266758\n",
      "757-th epoch val loss 1.2426105638352356\n",
      "758-th epoch train loss 1.2730602491498928\n",
      "758-th epoch val loss 1.2426108611139546\n",
      "759-th epoch train loss 1.2730604961091625\n",
      "759-th epoch val loss 1.242611153742742\n",
      "760-th epoch train loss 1.2730607392058981\n",
      "760-th epoch val loss 1.2426114417942922\n",
      "761-th epoch train loss 1.273060978500476\n",
      "761-th epoch val loss 1.2426117253401654\n",
      "762-th epoch train loss 1.2730612140523294\n",
      "762-th epoch val loss 1.2426120044508044\n",
      "763-th epoch train loss 1.2730614459199632\n",
      "763-th epoch val loss 1.2426122791955512\n",
      "764-th epoch train loss 1.2730616741609682\n",
      "764-th epoch val loss 1.2426125496426645\n",
      "765-th epoch train loss 1.2730618988320377\n",
      "765-th epoch val loss 1.242612815859338\n",
      "766-th epoch train loss 1.273062119988978\n",
      "766-th epoch val loss 1.2426130779117146\n",
      "767-th epoch train loss 1.2730623376867238\n",
      "767-th epoch val loss 1.2426133358649039\n",
      "768-th epoch train loss 1.2730625519793544\n",
      "768-th epoch val loss 1.2426135897829995\n",
      "769-th epoch train loss 1.2730627629201026\n",
      "769-th epoch val loss 1.2426138397290938\n",
      "770-th epoch train loss 1.2730629705613699\n",
      "770-th epoch val loss 1.242614085765292\n",
      "771-th epoch train loss 1.2730631749547394\n",
      "771-th epoch val loss 1.242614327952731\n",
      "772-th epoch train loss 1.273063376150989\n",
      "772-th epoch val loss 1.2426145663515906\n",
      "773-th epoch train loss 1.2730635742001042\n",
      "773-th epoch val loss 1.242614801021112\n",
      "774-th epoch train loss 1.2730637691512883\n",
      "774-th epoch val loss 1.24261503201961\n",
      "775-th epoch train loss 1.2730639610529766\n",
      "775-th epoch val loss 1.242615259404489\n",
      "776-th epoch train loss 1.2730641499528486\n",
      "776-th epoch val loss 1.2426154832322558\n",
      "777-th epoch train loss 1.2730643358978386\n",
      "777-th epoch val loss 1.2426157035585348\n",
      "778-th epoch train loss 1.2730645189341467\n",
      "778-th epoch val loss 1.2426159204380807\n",
      "779-th epoch train loss 1.2730646991072534\n",
      "779-th epoch val loss 1.242616133924793\n",
      "780-th epoch train loss 1.2730648764619263\n",
      "780-th epoch val loss 1.2426163440717288\n",
      "781-th epoch train loss 1.2730650510422354\n",
      "781-th epoch val loss 1.2426165509311158\n",
      "782-th epoch train loss 1.273065222891562\n",
      "782-th epoch val loss 1.2426167545543674\n",
      "783-th epoch train loss 1.2730653920526085\n",
      "783-th epoch val loss 1.24261695499209\n",
      "784-th epoch train loss 1.2730655585674113\n",
      "784-th epoch val loss 1.2426171522941027\n",
      "785-th epoch train loss 1.2730657224773496\n",
      "785-th epoch val loss 1.242617346509444\n",
      "786-th epoch train loss 1.2730658838231563\n",
      "786-th epoch val loss 1.2426175376863873\n",
      "787-th epoch train loss 1.2730660426449274\n",
      "787-th epoch val loss 1.2426177258724496\n",
      "788-th epoch train loss 1.2730661989821332\n",
      "788-th epoch val loss 1.2426179111144078\n",
      "789-th epoch train loss 1.273066352873626\n",
      "789-th epoch val loss 1.2426180934583055\n",
      "790-th epoch train loss 1.273066504357652\n",
      "790-th epoch val loss 1.242618272949468\n",
      "791-th epoch train loss 1.2730666534718595\n",
      "791-th epoch val loss 1.2426184496325112\n",
      "792-th epoch train loss 1.2730668002533085\n",
      "792-th epoch val loss 1.2426186235513552\n",
      "793-th epoch train loss 1.2730669447384804\n",
      "793-th epoch val loss 1.2426187947492322\n",
      "794-th epoch train loss 1.2730670869632852\n",
      "794-th epoch val loss 1.2426189632686995\n",
      "795-th epoch train loss 1.2730672269630723\n",
      "795-th epoch val loss 1.2426191291516473\n",
      "796-th epoch train loss 1.2730673647726394\n",
      "796-th epoch val loss 1.242619292439314\n",
      "797-th epoch train loss 1.27306750042624\n",
      "797-th epoch val loss 1.2426194531722916\n",
      "798-th epoch train loss 1.273067633957591\n",
      "798-th epoch val loss 1.242619611390537\n",
      "799-th epoch train loss 1.2730677653998843\n",
      "799-th epoch val loss 1.2426197671333843\n",
      "800-th epoch train loss 1.2730678947857912\n",
      "800-th epoch val loss 1.2426199204395507\n",
      "801-th epoch train loss 1.2730680221474742\n",
      "801-th epoch val loss 1.24262007134715\n",
      "802-th epoch train loss 1.273068147516592\n",
      "802-th epoch val loss 1.2426202198936989\n",
      "803-th epoch train loss 1.273068270924308\n",
      "803-th epoch val loss 1.2426203661161284\n",
      "804-th epoch train loss 1.273068392401299\n",
      "804-th epoch val loss 1.242620510050791\n",
      "805-th epoch train loss 1.2730685119777632\n",
      "805-th epoch val loss 1.242620651733472\n",
      "806-th epoch train loss 1.2730686296834255\n",
      "806-th epoch val loss 1.2426207911993965\n",
      "807-th epoch train loss 1.2730687455475465\n",
      "807-th epoch val loss 1.2426209284832401\n",
      "808-th epoch train loss 1.2730688595989303\n",
      "808-th epoch val loss 1.2426210636191353\n",
      "809-th epoch train loss 1.273068971865929\n",
      "809-th epoch val loss 1.24262119664068\n",
      "810-th epoch train loss 1.273069082376453\n",
      "810-th epoch val loss 1.242621327580949\n",
      "811-th epoch train loss 1.2730691911579761\n",
      "811-th epoch val loss 1.2426214564724984\n",
      "812-th epoch train loss 1.2730692982375416\n",
      "812-th epoch val loss 1.2426215833473753\n",
      "813-th epoch train loss 1.273069403641771\n",
      "813-th epoch val loss 1.2426217082371265\n",
      "814-th epoch train loss 1.2730695073968699\n",
      "814-th epoch val loss 1.242621831172805\n",
      "815-th epoch train loss 1.2730696095286327\n",
      "815-th epoch val loss 1.2426219521849788\n",
      "816-th epoch train loss 1.2730697100624524\n",
      "816-th epoch val loss 1.2426220713037366\n",
      "817-th epoch train loss 1.2730698090233235\n",
      "817-th epoch val loss 1.242622188558698\n",
      "818-th epoch train loss 1.2730699064358506\n",
      "818-th epoch val loss 1.242622303979019\n",
      "819-th epoch train loss 1.2730700023242536\n",
      "819-th epoch val loss 1.2426224175934\n",
      "820-th epoch train loss 1.2730700967123736\n",
      "820-th epoch val loss 1.2426225294300919\n",
      "821-th epoch train loss 1.273070189623678\n",
      "821-th epoch val loss 1.242622639516904\n",
      "822-th epoch train loss 1.2730702810812689\n",
      "822-th epoch val loss 1.2426227478812115\n",
      "823-th epoch train loss 1.2730703711078861\n",
      "823-th epoch val loss 1.2426228545499611\n",
      "824-th epoch train loss 1.273070459725914\n",
      "824-th epoch val loss 1.242622959549677\n",
      "825-th epoch train loss 1.2730705469573875\n",
      "825-th epoch val loss 1.2426230629064703\n",
      "826-th epoch train loss 1.2730706328239962\n",
      "826-th epoch val loss 1.2426231646460424\n",
      "827-th epoch train loss 1.2730707173470919\n",
      "827-th epoch val loss 1.242623264793695\n",
      "828-th epoch train loss 1.2730708005476898\n",
      "828-th epoch val loss 1.2426233633743302\n",
      "829-th epoch train loss 1.2730708824464798\n",
      "829-th epoch val loss 1.2426234604124649\n",
      "830-th epoch train loss 1.2730709630638266\n",
      "830-th epoch val loss 1.2426235559322305\n",
      "831-th epoch train loss 1.2730710424197755\n",
      "831-th epoch val loss 1.2426236499573795\n",
      "832-th epoch train loss 1.2730711205340604\n",
      "832-th epoch val loss 1.2426237425112963\n",
      "833-th epoch train loss 1.2730711974261055\n",
      "833-th epoch val loss 1.2426238336169975\n",
      "834-th epoch train loss 1.2730712731150309\n",
      "834-th epoch val loss 1.2426239232971397\n",
      "835-th epoch train loss 1.2730713476196585\n",
      "835-th epoch val loss 1.2426240115740252\n",
      "836-th epoch train loss 1.2730714209585157\n",
      "836-th epoch val loss 1.2426240984696093\n",
      "837-th epoch train loss 1.2730714931498397\n",
      "837-th epoch val loss 1.2426241840055008\n",
      "838-th epoch train loss 1.2730715642115824\n",
      "838-th epoch val loss 1.2426242682029731\n",
      "839-th epoch train loss 1.2730716341614166\n",
      "839-th epoch val loss 1.2426243510829662\n",
      "840-th epoch train loss 1.2730717030167362\n",
      "840-th epoch val loss 1.2426244326660918\n",
      "841-th epoch train loss 1.273071770794665\n",
      "841-th epoch val loss 1.24262451297264\n",
      "842-th epoch train loss 1.273071837512058\n",
      "842-th epoch val loss 1.2426245920225836\n",
      "843-th epoch train loss 1.2730719031855073\n",
      "843-th epoch val loss 1.2426246698355823\n",
      "844-th epoch train loss 1.2730719678313442\n",
      "844-th epoch val loss 1.2426247464309885\n",
      "845-th epoch train loss 1.273072031465647\n",
      "845-th epoch val loss 1.2426248218278526\n",
      "846-th epoch train loss 1.2730720941042397\n",
      "846-th epoch val loss 1.2426248960449264\n",
      "847-th epoch train loss 1.2730721557627012\n",
      "847-th epoch val loss 1.242624969100669\n",
      "848-th epoch train loss 1.2730722164563653\n",
      "848-th epoch val loss 1.2426250410132498\n",
      "849-th epoch train loss 1.2730722762003261\n",
      "849-th epoch val loss 1.2426251118005545\n",
      "850-th epoch train loss 1.2730723350094426\n",
      "850-th epoch val loss 1.2426251814801896\n",
      "851-th epoch train loss 1.2730723928983396\n",
      "851-th epoch val loss 1.2426252500694848\n",
      "852-th epoch train loss 1.273072449881415\n",
      "852-th epoch val loss 1.2426253175855\n",
      "853-th epoch train loss 1.2730725059728407\n",
      "853-th epoch val loss 1.242625384045028\n",
      "854-th epoch train loss 1.273072561186567\n",
      "854-th epoch val loss 1.2426254494645983\n",
      "855-th epoch train loss 1.2730726155363254\n",
      "855-th epoch val loss 1.2426255138604827\n",
      "856-th epoch train loss 1.2730726690356342\n",
      "856-th epoch val loss 1.2426255772486983\n",
      "857-th epoch train loss 1.273072721697799\n",
      "857-th epoch val loss 1.2426256396450108\n",
      "858-th epoch train loss 1.2730727735359173\n",
      "858-th epoch val loss 1.24262570106494\n",
      "859-th epoch train loss 1.2730728245628815\n",
      "859-th epoch val loss 1.2426257615237626\n",
      "860-th epoch train loss 1.273072874791384\n",
      "860-th epoch val loss 1.2426258210365175\n",
      "861-th epoch train loss 1.273072924233916\n",
      "861-th epoch val loss 1.242625879618006\n",
      "862-th epoch train loss 1.273072972902776\n",
      "862-th epoch val loss 1.2426259372828001\n",
      "863-th epoch train loss 1.2730730208100685\n",
      "863-th epoch val loss 1.2426259940452422\n",
      "864-th epoch train loss 1.273073067967709\n",
      "864-th epoch val loss 1.2426260499194521\n",
      "865-th epoch train loss 1.273073114387427\n",
      "865-th epoch val loss 1.242626104919327\n",
      "866-th epoch train loss 1.2730731600807677\n",
      "866-th epoch val loss 1.242626159058548\n",
      "867-th epoch train loss 1.2730732050590965\n",
      "867-th epoch val loss 1.242626212350581\n",
      "868-th epoch train loss 1.2730732493336008\n",
      "868-th epoch val loss 1.2426262648086823\n",
      "869-th epoch train loss 1.2730732929152926\n",
      "869-th epoch val loss 1.2426263164459002\n",
      "870-th epoch train loss 1.2730733358150124\n",
      "870-th epoch val loss 1.2426263672750788\n",
      "871-th epoch train loss 1.2730733780434305\n",
      "871-th epoch val loss 1.2426264173088613\n",
      "872-th epoch train loss 1.2730734196110498\n",
      "872-th epoch val loss 1.2426264665596942\n",
      "873-th epoch train loss 1.2730734605282108\n",
      "873-th epoch val loss 1.2426265150398275\n",
      "874-th epoch train loss 1.2730735008050902\n",
      "874-th epoch val loss 1.2426265627613213\n",
      "875-th epoch train loss 1.2730735404517066\n",
      "875-th epoch val loss 1.2426266097360454\n",
      "876-th epoch train loss 1.2730735794779209\n",
      "876-th epoch val loss 1.2426266559756842\n",
      "877-th epoch train loss 1.273073617893441\n",
      "877-th epoch val loss 1.242626701491741\n",
      "878-th epoch train loss 1.2730736557078222\n",
      "878-th epoch val loss 1.2426267462955372\n",
      "879-th epoch train loss 1.2730736929304705\n",
      "879-th epoch val loss 1.2426267903982176\n",
      "880-th epoch train loss 1.273073729570645\n",
      "880-th epoch val loss 1.2426268338107533\n",
      "881-th epoch train loss 1.2730737656374589\n",
      "881-th epoch val loss 1.242626876543943\n",
      "882-th epoch train loss 1.273073801139884\n",
      "882-th epoch val loss 1.2426269186084171\n",
      "883-th epoch train loss 1.2730738360867515\n",
      "883-th epoch val loss 1.2426269600146385\n",
      "884-th epoch train loss 1.2730738704867541\n",
      "884-th epoch val loss 1.242627000772908\n",
      "885-th epoch train loss 1.273073904348449\n",
      "885-th epoch val loss 1.2426270408933646\n",
      "886-th epoch train loss 1.273073937680259\n",
      "886-th epoch val loss 1.242627080385988\n",
      "887-th epoch train loss 1.2730739704904745\n",
      "887-th epoch val loss 1.242627119260602\n",
      "888-th epoch train loss 1.2730740027872587\n",
      "888-th epoch val loss 1.242627157526878\n",
      "889-th epoch train loss 1.2730740345786444\n",
      "889-th epoch val loss 1.2426271951943346\n",
      "890-th epoch train loss 1.2730740658725397\n",
      "890-th epoch val loss 1.2426272322723413\n",
      "891-th epoch train loss 1.2730740966767293\n",
      "891-th epoch val loss 1.242627268770123\n",
      "892-th epoch train loss 1.2730741269988755\n",
      "892-th epoch val loss 1.2426273046967575\n",
      "893-th epoch train loss 1.2730741568465216\n",
      "893-th epoch val loss 1.2426273400611834\n",
      "894-th epoch train loss 1.2730741862270918\n",
      "894-th epoch val loss 1.2426273748721965\n",
      "895-th epoch train loss 1.273074215147895\n",
      "895-th epoch val loss 1.242627409138458\n",
      "896-th epoch train loss 1.2730742436161246\n",
      "896-th epoch val loss 1.2426274428684907\n",
      "897-th epoch train loss 1.2730742716388643\n",
      "897-th epoch val loss 1.2426274760706875\n",
      "898-th epoch train loss 1.273074299223083\n",
      "898-th epoch val loss 1.2426275087533059\n",
      "899-th epoch train loss 1.2730743263756437\n",
      "899-th epoch val loss 1.2426275409244778\n",
      "900-th epoch train loss 1.2730743531032997\n",
      "900-th epoch val loss 1.242627572592205\n",
      "901-th epoch train loss 1.2730743794127013\n",
      "901-th epoch val loss 1.2426276037643664\n",
      "902-th epoch train loss 1.2730744053103922\n",
      "902-th epoch val loss 1.2426276344487164\n",
      "903-th epoch train loss 1.273074430802815\n",
      "903-th epoch val loss 1.242627664652888\n",
      "904-th epoch train loss 1.2730744558963107\n",
      "904-th epoch val loss 1.242627694384395\n",
      "905-th epoch train loss 1.2730744805971226\n",
      "905-th epoch val loss 1.2426277236506345\n",
      "906-th epoch train loss 1.2730745049113952\n",
      "906-th epoch val loss 1.242627752458886\n",
      "907-th epoch train loss 1.273074528845177\n",
      "907-th epoch val loss 1.242627780816318\n",
      "908-th epoch train loss 1.2730745524044216\n",
      "908-th epoch val loss 1.2426278087299834\n",
      "909-th epoch train loss 1.2730745755949902\n",
      "909-th epoch val loss 1.2426278362068266\n",
      "910-th epoch train loss 1.273074598422651\n",
      "910-th epoch val loss 1.2426278632536836\n",
      "911-th epoch train loss 1.2730746208930839\n",
      "911-th epoch val loss 1.2426278898772827\n",
      "912-th epoch train loss 1.273074643011878\n",
      "912-th epoch val loss 1.2426279160842466\n",
      "913-th epoch train loss 1.273074664784536\n",
      "913-th epoch val loss 1.2426279418810964\n",
      "914-th epoch train loss 1.2730746862164743\n",
      "914-th epoch val loss 1.2426279672742484\n",
      "915-th epoch train loss 1.2730747073130249\n",
      "915-th epoch val loss 1.2426279922700205\n",
      "916-th epoch train loss 1.2730747280794352\n",
      "916-th epoch val loss 1.2426280168746304\n",
      "917-th epoch train loss 1.2730747485208722\n",
      "917-th epoch val loss 1.2426280410942\n",
      "918-th epoch train loss 1.273074768642421\n",
      "918-th epoch val loss 1.242628064934755\n",
      "919-th epoch train loss 1.2730747884490867\n",
      "919-th epoch val loss 1.242628088402225\n",
      "920-th epoch train loss 1.2730748079457979\n",
      "920-th epoch val loss 1.242628111502449\n",
      "921-th epoch train loss 1.2730748271374028\n",
      "921-th epoch val loss 1.2426281342411736\n",
      "922-th epoch train loss 1.2730748460286776\n",
      "922-th epoch val loss 1.242628156624056\n",
      "923-th epoch train loss 1.2730748646243208\n",
      "923-th epoch val loss 1.242628178656664\n",
      "924-th epoch train loss 1.2730748829289598\n",
      "924-th epoch val loss 1.2426282003444797\n",
      "925-th epoch train loss 1.2730749009471474\n",
      "925-th epoch val loss 1.242628221692899\n",
      "926-th epoch train loss 1.2730749186833665\n",
      "926-th epoch val loss 1.2426282427072317\n",
      "927-th epoch train loss 1.2730749361420293\n",
      "927-th epoch val loss 1.2426282633927062\n",
      "928-th epoch train loss 1.273074953327479\n",
      "928-th epoch val loss 1.2426282837544695\n",
      "929-th epoch train loss 1.273074970243991\n",
      "929-th epoch val loss 1.242628303797586\n",
      "930-th epoch train loss 1.2730749868957743\n",
      "930-th epoch val loss 1.2426283235270426\n",
      "931-th epoch train loss 1.2730750032869702\n",
      "931-th epoch val loss 1.242628342947748\n",
      "932-th epoch train loss 1.2730750194216585\n",
      "932-th epoch val loss 1.2426283620645338\n",
      "933-th epoch train loss 1.2730750353038514\n",
      "933-th epoch val loss 1.2426283808821557\n",
      "934-th epoch train loss 1.2730750509375013\n",
      "934-th epoch val loss 1.242628399405295\n",
      "935-th epoch train loss 1.2730750663264967\n",
      "935-th epoch val loss 1.2426284176385605\n",
      "936-th epoch train loss 1.2730750814746663\n",
      "936-th epoch val loss 1.242628435586488\n",
      "937-th epoch train loss 1.273075096385779\n",
      "937-th epoch val loss 1.242628453253543\n",
      "938-th epoch train loss 1.2730751110635448\n",
      "938-th epoch val loss 1.2426284706441209\n",
      "939-th epoch train loss 1.2730751255116144\n",
      "939-th epoch val loss 1.2426284877625475\n",
      "940-th epoch train loss 1.2730751397335822\n",
      "940-th epoch val loss 1.2426285046130825\n",
      "941-th epoch train loss 1.2730751537329872\n",
      "941-th epoch val loss 1.242628521199918\n",
      "942-th epoch train loss 1.2730751675133118\n",
      "942-th epoch val loss 1.2426285375271802\n",
      "943-th epoch train loss 1.2730751810779841\n",
      "943-th epoch val loss 1.2426285535989312\n",
      "944-th epoch train loss 1.273075194430379\n",
      "944-th epoch val loss 1.2426285694191697\n",
      "945-th epoch train loss 1.2730752075738183\n",
      "945-th epoch val loss 1.2426285849918313\n",
      "946-th epoch train loss 1.2730752205115727\n",
      "946-th epoch val loss 1.2426286003207907\n",
      "947-th epoch train loss 1.2730752332468596\n",
      "947-th epoch val loss 1.2426286154098611\n",
      "948-th epoch train loss 1.273075245782848\n",
      "948-th epoch val loss 1.242628630262797\n",
      "949-th epoch train loss 1.273075258122657\n",
      "949-th epoch val loss 1.2426286448832937\n",
      "950-th epoch train loss 1.2730752702693564\n",
      "950-th epoch val loss 1.2426286592749884\n",
      "951-th epoch train loss 1.2730752822259674\n",
      "951-th epoch val loss 1.2426286734414613\n",
      "952-th epoch train loss 1.2730752939954657\n",
      "952-th epoch val loss 1.2426286873862369\n",
      "953-th epoch train loss 1.273075305580779\n",
      "953-th epoch val loss 1.2426287011127857\n",
      "954-th epoch train loss 1.2730753169847897\n",
      "954-th epoch val loss 1.2426287146245218\n",
      "955-th epoch train loss 1.2730753282103349\n",
      "955-th epoch val loss 1.2426287279248067\n",
      "956-th epoch train loss 1.273075339260207\n",
      "956-th epoch val loss 1.2426287410169494\n",
      "957-th epoch train loss 1.2730753501371554\n",
      "957-th epoch val loss 1.2426287539042076\n",
      "958-th epoch train loss 1.2730753608438863\n",
      "958-th epoch val loss 1.2426287665897873\n",
      "959-th epoch train loss 1.2730753713830636\n",
      "959-th epoch val loss 1.2426287790768444\n",
      "960-th epoch train loss 1.2730753817573093\n",
      "960-th epoch val loss 1.2426287913684864\n",
      "961-th epoch train loss 1.2730753919692037\n",
      "961-th epoch val loss 1.24262880346777\n",
      "962-th epoch train loss 1.2730754020212882\n",
      "962-th epoch val loss 1.2426288153777065\n",
      "963-th epoch train loss 1.2730754119160637\n",
      "963-th epoch val loss 1.242628827101259\n",
      "964-th epoch train loss 1.2730754216559919\n",
      "964-th epoch val loss 1.2426288386413447\n",
      "965-th epoch train loss 1.2730754312434953\n",
      "965-th epoch val loss 1.2426288500008331\n",
      "966-th epoch train loss 1.27307544068096\n",
      "966-th epoch val loss 1.2426288611825518\n",
      "967-th epoch train loss 1.2730754499707337\n",
      "967-th epoch val loss 1.242628872189282\n",
      "968-th epoch train loss 1.2730754591151272\n",
      "968-th epoch val loss 1.2426288830237624\n",
      "969-th epoch train loss 1.2730754681164163\n",
      "969-th epoch val loss 1.2426288936886885\n",
      "970-th epoch train loss 1.27307547697684\n",
      "970-th epoch val loss 1.2426289041867138\n",
      "971-th epoch train loss 1.273075485698603\n",
      "971-th epoch val loss 1.2426289145204503\n",
      "972-th epoch train loss 1.2730754942838751\n",
      "972-th epoch val loss 1.2426289246924687\n",
      "973-th epoch train loss 1.273075502734792\n",
      "973-th epoch val loss 1.2426289347053001\n",
      "974-th epoch train loss 1.2730755110534564\n",
      "974-th epoch val loss 1.2426289445614354\n",
      "975-th epoch train loss 1.2730755192419383\n",
      "975-th epoch val loss 1.2426289542633266\n",
      "976-th epoch train loss 1.2730755273022747\n",
      "976-th epoch val loss 1.2426289638133876\n",
      "977-th epoch train loss 1.2730755352364709\n",
      "977-th epoch val loss 1.242628973213995\n",
      "978-th epoch train loss 1.2730755430465006\n",
      "978-th epoch val loss 1.2426289824674865\n",
      "979-th epoch train loss 1.2730755507343072\n",
      "979-th epoch val loss 1.2426289915761657\n",
      "980-th epoch train loss 1.2730755583018039\n",
      "980-th epoch val loss 1.2426290005422982\n",
      "981-th epoch train loss 1.2730755657508723\n",
      "981-th epoch val loss 1.242629009368114\n",
      "982-th epoch train loss 1.273075573083367\n",
      "982-th epoch val loss 1.2426290180558104\n",
      "983-th epoch train loss 1.2730755803011116\n",
      "983-th epoch val loss 1.242629026607548\n",
      "984-th epoch train loss 1.2730755874059019\n",
      "984-th epoch val loss 1.2426290350254547\n",
      "985-th epoch train loss 1.2730755943995056\n",
      "985-th epoch val loss 1.2426290433116245\n",
      "986-th epoch train loss 1.2730756012836624\n",
      "986-th epoch val loss 1.2426290514681193\n",
      "987-th epoch train loss 1.2730756080600854\n",
      "987-th epoch val loss 1.2426290594969684\n",
      "988-th epoch train loss 1.2730756147304605\n",
      "988-th epoch val loss 1.2426290674001694\n",
      "989-th epoch train loss 1.2730756212964478\n",
      "989-th epoch val loss 1.242629075179689\n",
      "990-th epoch train loss 1.2730756277596798\n",
      "990-th epoch val loss 1.242629082837462\n",
      "991-th epoch train loss 1.273075634121765\n",
      "991-th epoch val loss 1.242629090375394\n",
      "992-th epoch train loss 1.2730756403842867\n",
      "992-th epoch val loss 1.2426290977953607\n",
      "993-th epoch train loss 1.2730756465488027\n",
      "993-th epoch val loss 1.2426291050992078\n",
      "994-th epoch train loss 1.2730756526168463\n",
      "994-th epoch val loss 1.242629112288752\n",
      "995-th epoch train loss 1.273075658589928\n",
      "995-th epoch val loss 1.2426291193657835\n",
      "996-th epoch train loss 1.2730756644695338\n",
      "996-th epoch val loss 1.2426291263320617\n",
      "997-th epoch train loss 1.2730756702571258\n",
      "997-th epoch val loss 1.242629133189321\n",
      "998-th epoch train loss 1.2730756759541448\n",
      "998-th epoch val loss 1.2426291399392662\n",
      "999-th epoch train loss 1.2730756815620075\n",
      "999-th epoch val loss 1.2426291465835777\n"
     ]
    }
   ],
   "source": [
    "slr = ScratchLinearRegression(num_iter=1000, lr=0.01, no_bias=True, verbose=True)\n",
    "slr.fit(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c76d73-1b7e-4f15-999a-805c5d231e6e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
